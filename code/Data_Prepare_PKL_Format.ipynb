{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Prepare_PKL_Format.ipynb","version":"0.3.2","provenance":[{"file_id":"1W6GHhtAv1zZ1Z2D6lLvC-uDSDWJD130O","timestamp":1567765261410}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ueQIwiNm42tG","colab_type":"code","outputId":"303b101a-9a49-4518-d9eb-38e2c617e70b","executionInfo":{"status":"ok","timestamp":1567550423515,"user_tz":-120,"elapsed":3945,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# !pip install tensorflow \n","# TensorFlow and tf.keras\n","# its just working with tensorflow 1.13.1, with others has problem\n","import tensorflow as tf\n","print(tf.__version__)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.14.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z_pjD2e15BFA","colab_type":"text"},"source":["**- goolge Colab Configuration**"]},{"cell_type":"code","metadata":{"id":"nZKvqNpW5HjM","colab_type":"code","outputId":"49848eb7-397d-4cb8-f830-2776374057d0","executionInfo":{"status":"ok","timestamp":1568234561318,"user_tz":-120,"elapsed":21597,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UTR5Csn45f2I","colab_type":"text"},"source":["**- Setting root address to project in Google Drive**"]},{"cell_type":"code","metadata":{"id":"ywAPM0ns5dX-","colab_type":"code","outputId":"bd2c08de-f4e7-4a6e-db3d-776202456830","executionInfo":{"status":"ok","timestamp":1568234564739,"user_tz":-120,"elapsed":5509,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":553}},"source":["!ls \"/content/gdrive/My Drive/NLPHW3\"\n","root_path = '/content/gdrive/My Drive/NLPHW3'"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" BackUpCodes\n","'Basic BDLSTM '\n"," BASIC_BDLSTM_Tagger.ipynb\n"," Data_Prepare_PKL_Format.ipynb\n"," Evaluation_Datasets\n"," LEX_BDLSTM_models_46.04966139954853\n"," LEX_BDLSTM_models_46.27539503386004\n"," LEX_BDLSTM_models_47.629796839729124\n"," LEX_BDLSTM_models_48.306997742663654\n"," LEX_BDLSTM_Tagger.ipynb\n"," Mask_LSTM_Model1.ipynb\n"," MFS_Tagger.ipynb\n"," POS_BDLSTM_models_20.118343195266274\n"," POS_BDLSTM_models_21.301775147928996\n"," POS_BDLSTM_models_23.668639053254438\n"," POS_BDLSTM_models_24.85207100591716\n"," POS_BDLSTM_models_25.443786982248522\n"," POS_BDLSTM_Tagger.ipynb\n"," POS_LEX_BDLSTM_Tagger.ipynb\n"," POS_LEX_WnDomain_BDLSTM_Tagger.ipynb\n"," POS_WnDomain_BDLSTM_Tagger.ipynb\n"," resources\n"," semcor\n"," semcor+omsti\n"," Test_Prediction_Write.ipynb\n"," Training_Corpora\n"," WnDomain_BDLSTM_models_46.72686230248307\n"," WnDomain_BDLSTM_models_47.17832957110609\n"," WnDomain_BDLSTM_models_48.53273137697517\n"," WnDomain_BDLSTM_models_48.75846501128668\n"," WnDomain_BDLSTM_models_50.112866817155755\n"," WnDomain_BDLSTM_Tagger.ipynb\n"," WSD_Evaluation_Framework\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"waoW11y6yLmL","colab_type":"code","outputId":"82b42041-77ba-40d4-a9ca-dd7c4f5702e7","executionInfo":{"status":"ok","timestamp":1568234565845,"user_tz":-120,"elapsed":5330,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# Imports\n","from collections import Counter\n","import codecs\n","import xml.etree.cElementTree as etree\n","import pickle\n","import nltk\n","nltk.download('wordnet')\n","import numpy as np\n","import copy\n","from nltk.corpus import wordnet as wn\n","\n","\n","# Added Chars to dictionaries\n","UNK = \"<UNK>\"\n","PAD = \"<PAD>\"\n","\n","\n","RESOURCE_PATH = root_path+'/resources/'\n","\n","MAP_WN2BN_FILE = RESOURCE_PATH + 'babelnet2wordnet.tsv'\n","MAP_BN2WNDOMAIN_FILE = RESOURCE_PATH + 'babelnet2wndomains.tsv'\n","MAP_BN2LEXNAMES_FILE = RESOURCE_PATH + 'babelnet2lexnames.tsv'\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_spZj5tuol7Y","colab_type":"code","outputId":"d6b09736-c35c-4591-e9ec-f0cf96e99170","executionInfo":{"status":"ok","timestamp":1568234617950,"user_tz":-120,"elapsed":37412,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":748}},"source":["## -----------------------------------------------------------------------------\n","# function for getting dictionary of gold keys ->instance id: senseKey\n","def getGoldKey(goldKeyFile):\n","    print('getGoldKey is started ....')\n","    goldAddr = []\n","    goldKeys = []\n","    with codecs.open(goldKeyFile,'rb') as f:        \n","        for line in f:            \n","            line_ = line.decode().strip().split(' ')  \n","            goldAddr.append(line_[0])\n","            goldKeys.append(line_[1]) \n","    goldKeysDic = dict(zip(goldAddr,goldKeys))\n","    \n","    print('getGoldKey is done ....')\n","\n","    return goldKeysDic\n","\n","# -----------------------------------------------------------------------------\n","def WN_to_BN_dic(mapping_file):\n","    print('WN_to_BN_dic is started ....')\n","    BN_ID = []\n","    WN_ID = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            WN_ID.append(line_synsets[1])    \n","    WN2BN_map_dic = dict(zip(WN_ID,BN_ID))\n","    print('WN_to_BN_dic is done ....')\n","\n","    return WN2BN_map_dic\n","\n","\n","# -----------------------------------------------------------------------------\n","def BN_to_WNDOMAIN_dic(mapping_file):\n","    print('WN_to_BN_dic is started ....')\n","    BN_ID = []\n","    WN_DOMAIN = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            WN_DOMAIN.append(line_synsets[1])    \n","    BN2WnDomain_dic = dict(zip(BN_ID, WN_DOMAIN))\n","    print('BN_to_WNDOMAIN_dic is done ....')\n","    return BN2WnDomain_dic\n","\n","\n","\n","# -----------------------------------------------------------------------------\n","def BN_to_LexNames_dic(mapping_file):\n","    print('BN_to_LexNames is started ....')\n","    BN_ID = []\n","    LEXNAMES = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            LEXNAMES.append(line_synsets[1])    \n","    BN2LexNames_dic = dict(zip(BN_ID, LEXNAMES))\n","    print('BN_to_LexNames_dic is done ....')\n","\n","    return BN2LexNames_dic\n","\n","\n","\n","## ----------------------------------------------------------------------------\n","def getMFS_(word, WN2BN_map_dic):\n","    MFS_wnId = UNK\n","    all_synsets = wn.synsets(word)\n","\n","    # check if the word has synsets or not!\n","    if len(all_synsets) == 0 or all_synsets is None:\n","        return word\n","    \n","    synset = all_synsets[0]\n","    MFS_wnId = \"wn:\" + str(synset.offset()).zfill( 8) + synset.pos()\n","    if WN2BN_map_dic.get(MFS_wnId) is not None:\n","        MFS_bnId = WN2BN_map_dic.get(MFS_wnId)\n","        \n","    return MFS_bnId\n","\n","#mfs = getMFS_('how')\n","\n","\n","def getPOSTagOfCharPOS(pos):\n","    if pos == 'n':\n","        return 'noun'\n","    if pos == 'v':\n","        return 'verb'\n","    if pos == 'a':\n","        return 'adj'\n","    if pos == 'r':\n","        return 'adv'\n","\n","def get_WN_POSTagOfCharPOS(pos):\n","    if pos == 'noun':\n","        return wn.NOUN\n","    if pos == 'verb':\n","        return wn.VERB\n","    if pos == 'adj':\n","        return wn.ADJ\n","    if pos == 'adv':\n","        return wn.ADV\n","\n","## ----------------------------------------------------------------------    \n","# function for getting all possible synsets of one lemma with considering the POS of lemma from WordNets in BabelNet id format\n","def get_bnSenseCandydtsAndPOSOfLemma(lemma, pos_lemma, WN2BN_map_dic):\n","    synsets_ = []\n","    pos_ = []\n","    for synset in wn.synsets(lemma, pos = get_WN_POSTagOfCharPOS(pos_lemma)): # pos=wn.VERB\n","        synset_id = \"wn:\" + str(synset.offset()).zfill( 8) + synset.pos() # out: wn:05899087n                \n","        if WN2BN_map_dic.get(synset_id) is not None:\n","            bnId_ = WN2BN_map_dic.get(synset_id)\n","#            print(WN2BN_map_dic.get(synset_id))\n","            synsets_.append(bnId_)\n","            pos_.append(getPOSTagOfCharPOS(bnId_[-1:]))\n","\n","    return synsets_, pos_\n","\n","#synsets_ = get_bnSenseCandydtsOfLemma('long' ,)\n","\n","\n","## ----------------------------------------------------------------------    \n","# get LexName of bnId\n","def get_LexOfLemmaSense(bnIds, BN2LexNames_dic):\n","    lexs_ = []\n","    for bnId_ in bnIds:\n","        if BN2LexNames_dic.get(bnId_) is not None:\n","            lexs_.append(BN2LexNames_dic.get(bnId_))\n","        else:\n","            lexs_.append('other')\n","\n","    return lexs_\n","\n","#lexs_ = get_LexOfLemmaSense(synsets_)\n","    \n","\n","\n","## ----------------------------------------------------------------------    \n","# get WnDomain of bnId\n","def get_WnDomainsOfLemmaSense(bnIds, BN2WnDomain_dic):\n","    wnDomains_ = []\n","    for bnId_ in bnIds:\n","        if BN2WnDomain_dic.get(bnId_) is not None:\n","            wnDomains_.append(BN2WnDomain_dic.get(bnId_))\n","        else:\n","            wnDomains_.append('other')\n","\n","    return wnDomains_\n","\n","#wnDomains_ = get_WnDomainsOfLemmaSense(synsets_)\n","\n","\n","# this is code for parsing xml file for getting all all data like instance id, pos, Sense, All possible Sense and ....\n","def parse_xml(filename, goldKeysDic, WN2BN_map_dic, BN2LexNames_dic, BN2WnDomain_dic):  \n","    print('parse_xml is started ....')   \n","    \n","    x_IdInstance_Sent, x_Raw_Sent, y_POS_Sent = [],[],[]\n","    y_BNsense_Sent, y_Lex_Sent, y_WnDomain_Sent, y_MFS_Sent = [],[],[],[]\n","    y_allSenses_Sent, y_allPos_Sent, y_allLexs_Sent, y_allwnDomains_Sent = [],[],[],[]\n","    \n","    IdInstance_sent, Raw_sent, POS_sent, BN_sent, Lex_sent, WnDomain_sent, MFS_sent,\\\n","                    allSenses_sent, allpos_sent, allLexs_sent, allwnDomains_sent = [],[],[],[],[],[],[],[],[],[],[]\n","    \n","    context = etree.iterparse(filename, events=(\"start\", \"end\"))   \n","    \n","    id_sent = 0;\n","\n","    for event, element in context:            \n","        if event == \"start\":        \n","            if element.tag =='sentence':                \n","#                 if id_sent == 10:\n","#                     break\n","#                sentfile.write((str(id_sent)+'<M.J>').lower().encode())\n","                if id_sent % 5000 == 0:\n","                    print(id_sent)\n","                id_sent = id_sent + 1\n","                \n","            if element.tag == \"wf\":  \n","                id_ = 'None'\n","                lemma_ = (str(element.attrib['lemma'])).lower()\n","                pos_ = (str(element.attrib['pos'])).lower()                \n","                \n","                IdInstance_sent.append(id_)\n","                Raw_sent.append(lemma_)            \n","                POS_sent.append(pos_)\n","                BN_sent.append(lemma_)\n","                Lex_sent.append('other')\n","                WnDomain_sent.append('other')            \n","                MFS_sent.append(getMFS_(lemma_, WN2BN_map_dic))\n","                allSenses_sent.append([lemma_])\n","                allpos_sent.append([pos_])\n","                allLexs_sent.append(['other'])\n","                allwnDomains_sent.append(['other'])\n","                \n","            if element.tag == \"instance\": \n","                instanceId_ = (str(element.attrib['id'])).lower()\n","                lemma_ = (str(element.attrib['lemma'])).lower()\n","                pos_ = (str(element.attrib['pos'])).lower()\n","                    \n","                IdInstance_sent.append(instanceId_)\n","                Raw_sent.append(lemma_)            \n","                POS_sent.append(pos_)\n","                \n","                SenseKey_ = goldKeysDic.get(instanceId_) # sense for specific instance from gold file\n","#                 print(SenseKey_)\n","                \n","                synset = wn.lemma_from_key(SenseKey_).synset() # out: Synset('long.a.01')\n","                synset_id = \"wn:\" + str(synset.offset()).zfill( 8) + synset.pos() # out: wn:05899087n                \n","                \n","                bnId_ = UNK\n","                if WN2BN_map_dic.get(synset_id) is not None:\n","                    bnId_ = WN2BN_map_dic.get(synset_id)                    \n","                else:\n","                    print(\"wnId:  \",synset_id, \" is not in wn2bnMapping file.\")\n","                \n","                BN_sent.append(bnId_)\n","            \n","                if BN2LexNames_dic.get(bnId_) is not None:\n","                    Lex_sent.append(BN2LexNames_dic.get(bnId_))\n","                else:\n","                    Lex_sent.append('other')\n","#                    print(\"bnId:  \", bnId_ , \" is not in bn2lexMapping file.\")\n","\n","                if BN2WnDomain_dic.get(bnId_) is not None:\n","                    WnDomain_sent.append(BN2WnDomain_dic.get(bnId_))                    \n","                else:\n","                    WnDomain_sent.append('other')                    \n","#                    print(\"bnId:  \", bnId_ , \" is not in bn2wnDomainMapping file.\")\n","                                \n","                candSense, CandPos = get_bnSenseCandydtsAndPOSOfLemma(lemma_, pos_, WN2BN_map_dic)\n","                            \n","                MFS_sent.append(getMFS_(lemma_, WN2BN_map_dic))\n","                allSenses_sent.append(candSense)\n","                allpos_sent.append(CandPos)\n","                allLexs_sent.append(get_LexOfLemmaSense(candSense, BN2LexNames_dic))\n","                allwnDomains_sent.append(get_WnDomainsOfLemmaSense(candSense, BN2WnDomain_dic))\n","                                                                                    \n","        if event == \"end\":\n","            if element.tag == \"sentence\" :\n","                x_IdInstance_Sent.append(IdInstance_sent)\n","                x_Raw_Sent.append(Raw_sent)            \n","                y_POS_Sent.append(POS_sent)\n","                y_BNsense_Sent.append(BN_sent)\n","                y_Lex_Sent.append(Lex_sent)\n","                y_WnDomain_Sent.append(WnDomain_sent)\n","                y_MFS_Sent.append(MFS_sent)\n","                y_allSenses_Sent.append(allSenses_sent)\n","                y_allPos_Sent.append(allpos_sent)\n","                y_allLexs_Sent.append(allLexs_sent)\n","                y_allwnDomains_Sent.append(allwnDomains_sent) \n","                \n","                IdInstance_sent, Raw_sent, POS_sent, BN_sent, Lex_sent, WnDomain_sent, MFS_sent,\\\n","                    allSenses_sent, allpos_sent, allLexs_sent, allwnDomains_sent = [],[],[],[],[],[],[],[],[],[],[]\n","                \n","        element.clear()\n","        \n","    return x_IdInstance_Sent, x_Raw_Sent, y_POS_Sent,\\\n","             y_BNsense_Sent, y_Lex_Sent, y_WnDomain_Sent, y_MFS_Sent,\\\n","                 y_allSenses_Sent, y_allPos_Sent, y_allLexs_Sent, y_allwnDomains_Sent\n","\n","\n","# function for writing files as pkl file in disk\n","def get_sent_PKL_file(DATASET_FILE, save_path, goldKeysDic, WN2BN_map_dic, BN2LexNames_dic, BN2WnDomain_dic):                    \n","    # function for parsing large xml file and getting needed data from that.\n","\n","    x_IdInstance_Sent, x_Raw_Sent, y_POS_Sent, y_BNsense_Sent, y_Lex_Sent, y_WnDomain_Sent,\\\n","        y_MFS_Sent, y_allSenses_Sent, y_allPos_Sent, y_allLexs_Sent, y_allwnDomains_Sent\\\n","    = parse_xml(DATASET_FILE, goldKeysDic, WN2BN_map_dic, BN2LexNames_dic, BN2WnDomain_dic)\n","    \n","    print('parse xml is ok .... ')\n","    \n","    with open(save_path + 'x_IdInstance_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(x_IdInstance_Sent, outfile)\n","        \n","    with open(save_path + 'x_Raw_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(x_Raw_Sent, outfile)\n","        \n","    with open(save_path + 'y_POS_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_POS_Sent, outfile)\n","        \n","    with open(save_path + 'y_BNsense_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_BNsense_Sent, outfile)\n","        \n","    with open(save_path + 'y_Lex_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_Lex_Sent, outfile)\n","        \n","    with open(save_path + 'y_WnDomain_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_WnDomain_Sent, outfile)\n","        \n","    with open(save_path + 'y_MFS_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_MFS_Sent, outfile)\n","        \n","    with open(save_path + 'y_allSenses_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_allSenses_Sent, outfile)\n","        \n","    with open(save_path + 'y_allPos_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_allPos_Sent, outfile)\n","        \n","    with open(save_path + 'y_allLex_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_allLexs_Sent, outfile)\n","        \n","    with open(save_path + 'y_allWnDomain_Sent.pkl', \"wb\") as outfile:\n","        pickle.dump(y_allwnDomains_Sent, outfile)\n","                \n","        \n","    print('PKL file saved successfully ......')\n","    \n","\n","\n","WN2BN_map_dic = WN_to_BN_dic(MAP_WN2BN_FILE)\n","print(len(WN2BN_map_dic))\n","\n","BN2WnDomain_dic = BN_to_WNDOMAIN_dic(MAP_BN2WNDOMAIN_FILE)\n","print(len(BN2WnDomain_dic))\n","\n","BN2LexNames_dic = BN_to_LexNames_dic(MAP_BN2LEXNAMES_FILE)\n","print(len(BN2LexNames_dic))\n","\n","\n","# DATASET_FILE = root_path + '/Training_Corpora/SemCor/semcor.data.xml'\n","# GOLD_KEY_FILE = root_path + '/Training_Corpora/SemCor/semcor.gold.key.txt'  \n","# PATH_TRAIN = root_path + '/semcor/semcor/'\n","# goldKeysDic = getGoldKey(GOLD_KEY_FILE)\n","\n","# get_sent_PKL_file(DATASET_FILE, PATH_TRAIN, goldKeysDic, WN2BN_map_dic, BN2LexNames_dic, BN2WnDomain_dic)\n","\n","file_names = ['semeval2007', 'semeval2013', 'semeval2015', 'senseval2', 'senseval3', 'ALL']\n","\n","\n","for file_name in file_names:\n","    DATASET_FILE = root_path + '/Evaluation_Datasets/{}/{}.data.xml'.format(file_name, file_name)\n","    GOLD_KEY_FILE = root_path + '/Evaluation_Datasets/{}/{}.gold.key.txt'.format(file_name, file_name) \n","    PATH = root_path + '/semcor+omsti/{}/'.format(file_name)\n","\n","    goldKeysDic = getGoldKey(GOLD_KEY_FILE)\n","\n","    get_sent_PKL_file(DATASET_FILE, PATH, goldKeysDic, WN2BN_map_dic, BN2LexNames_dic, BN2WnDomain_dic)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WN_to_BN_dic is started ....\n","WN_to_BN_dic is done ....\n","117659\n","WN_to_BN_dic is started ....\n","BN_to_WNDOMAIN_dic is done ....\n","92601\n","BN_to_LexNames is started ....\n","BN_to_LexNames_dic is done ....\n","117653\n","getGoldKey is started ....\n","getGoldKey is done ....\n","parse_xml is started ....\n","0\n","parse xml is ok .... \n","PKL file saved successfully ......\n","getGoldKey is started ....\n","getGoldKey is done ....\n","parse_xml is started ....\n","0\n","parse xml is ok .... \n","PKL file saved successfully ......\n","getGoldKey is started ....\n","getGoldKey is done ....\n","parse_xml is started ....\n","0\n","parse xml is ok .... \n","PKL file saved successfully ......\n","getGoldKey is started ....\n","getGoldKey is done ....\n","parse_xml is started ....\n","0\n","parse xml is ok .... \n","PKL file saved successfully ......\n","getGoldKey is started ....\n","getGoldKey is done ....\n","parse_xml is started ....\n","0\n","parse xml is ok .... \n","PKL file saved successfully ......\n","getGoldKey is started ....\n","getGoldKey is done ....\n","parse_xml is started ....\n","0\n","parse xml is ok .... \n","PKL file saved successfully ......\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jsqm-czx3DGU","colab_type":"code","colab":{}},"source":["# this 5 function is for creating dictionaries of lemma words, Senses, POS, LexNames and WnDomains\n","## -----------------------------------------------------------------------------\n","def get_RawWords2id_Dic(x_Raw_Sent_pkl):\n","    print('get_RawWords2id_Dic is started ....')\n","        \n","    x_Raw_Sent = pickle.load(open(x_Raw_Sent_pkl, \"rb\"))\n","\n","    RawWords2id = { PAD : 0, UNK: 1} \n","    \n","    raw_words = []\n","    # get words from dataset\n","    for i in range(len(x_Raw_Sent)):\n","        for j in range(len(x_Raw_Sent[i])):\n","            raw_words.append(x_Raw_Sent[i][j])                        \n","                \n","    counter = Counter(raw_words)    \n","    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n","    words, _ = list(zip(*count_pairs))\n","    RawWords2id.update(dict(zip(words, range(2, len(words) + 2))))\n","\n","    print('get_RawWords2id is done ....')    \n","    return RawWords2id\n","\n","\n","## -----------------------------------------------------------------------------\n","def get_POS2id_Dic(y_POS_Sent_pkl, y_allPos_Sent_pkl):\n","    print('get_POS2id_Dic is started ....')\n","    \n","    y_POS_Sent = pickle.load(open(y_POS_Sent_pkl, \"rb\"))\n","    y_allPos_Sent = pickle.load(open(y_allPos_Sent_pkl, \"rb\"))    \n","    \n","    pos2id = { 'verb' : 0, 'noun': 1, 'adj' : 2, 'adv': 3}\n","    pos = []\n","    # get words from dataset\n","    for i in range(len(y_POS_Sent)):\n","        for j in range(len(y_POS_Sent[i])):                        \n","            pos.append(y_POS_Sent[i][j]) \n","            \n","            for k in range(len(y_allPos_Sent[i][j])):\n","                pos.append(y_allPos_Sent[i][j][k])                            \n","                  \n","        \n","    counter = Counter(pos)    \n","    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n","    words, _ = list(zip(*count_pairs))\n","    \n","    pos_words = []\n","    for p in range(len(words)):\n","        if words[p] != 'verb':\n","            if words[p] != 'noun':\n","                if words[p] != 'adj':\n","                    if words[p] != 'adv':\n","                        pos_words.append(words[p])    \n","    \n","    pos2id.update(dict(zip(pos_words, range(4, len(pos_words) + 4))))\n","\n","\n","    print('get_POS2id_Dic is done ....')    \n","    return pos2id\n","\n","\n","\n","## -----------------------------------------------------------------------------\n","def get_Senses2id_Dic(y_allSenses_Sent_pkl, y_BNsense_Sent_pkl):\n","    print('get_Senses2id_Dic is started ....')\n","    \n","    y_allSenses_Sent = pickle.load(open(y_allSenses_Sent_pkl, \"rb\"))\n","    y_BNsense_Sent = pickle.load(open(y_BNsense_Sent_pkl, \"rb\"))\n","    \n","    SenseWords2id = {UNK: 0} \n","    sense_words = []\n","    # get words from dataset\n","    for i in range(len(y_allSenses_Sent)):\n","        for j in range(len(y_allSenses_Sent[i])):\n","            sense_words.append(y_BNsense_Sent[i][j])\n","            \n","            for k in range(len(y_allSenses_Sent[i][j])):\n","                sense_words.append(y_allSenses_Sent[i][j][k])\n","                                \n","    \n","    counter = Counter(sense_words)    \n","    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n","    all_senseANDwords, _ = list(zip(*count_pairs))\n","    SenseWords2id.update(dict(zip(all_senseANDwords, range(1, len(all_senseANDwords) + 1))))                \n","\n","    print('get_Senses2id_Dic is done ....')    \n","    return SenseWords2id\n","\n","\n","## -----------------------------------------------------------------------------\n","def get_LEX2id_Dic(y_allLexs_Sent_pkl):\n","    print('get_LEX2id_Dic is started ....')\n","    \n","    y_allLexs_Sent = pickle.load(open(y_allLexs_Sent_pkl, \"rb\"))    \n","    \n","    lex = []\n","    # get words from dataset\n","    for i in range(len(y_allLexs_Sent)):\n","        for j in range(len(y_allLexs_Sent[i])):            \n","            for k in range(len(y_allLexs_Sent[i][j])):\n","                lex.append(y_allLexs_Sent[i][j][k])                                        \n","    \n","    counter = Counter(lex)    \n","    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n","    words, _ = list(zip(*count_pairs))\n","    lex2id = dict(zip(words, range(len(words))))        \n","\n","    print('get_2idDictionaries is done ....')    \n","    return lex2id\n","\n","\n","\n","## -----------------------------------------------------------------------------\n","def get_2WnDomain2id_Dic(y_allwnDomains_Sent_pkl):\n","    print('get_2WnDomain2id_Dic is started ....')\n","    \n","    y_allwnDomains_Sent = pickle.load(open(y_allwnDomains_Sent_pkl, \"rb\")) \n","    \n","    wnDomain = []\n","    # get words from dataset\n","    for i in range(len(y_allwnDomains_Sent)):\n","        for j in range(len(y_allwnDomains_Sent[i])):            \n","            for k in range(len(y_allwnDomains_Sent[i][j])):\n","                wnDomain.append(y_allwnDomains_Sent[i][j][k])\n","                    \n","    counter = Counter(wnDomain)    \n","    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n","    words, _ = list(zip(*count_pairs))\n","    wnDomain2id = dict(zip(words, range(len(words))))\n","\n","    print('get_2idDictionaries is done ....')    \n","    return wnDomain2id\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_-JVAxL4xn7","colab_type":"code","outputId":"413247a8-0065-4086-ae9b-7a0aa6682577","executionInfo":{"status":"ok","timestamp":1568234630844,"user_tz":-120,"elapsed":34206,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":180}},"source":["dataset_file = 'semcor'\n","\n","RawWords2id = get_RawWords2id_Dic(root_path + '/{}/{}/x_Raw_Sent.pkl'.format(dataset_file,dataset_file))\n","\n","pos2id = get_POS2id_Dic(root_path + '/{}/{}/y_POS_Sent.pkl'.format(dataset_file,dataset_file), root_path + '/{}/{}/y_allPos_Sent.pkl'.format(dataset_file,dataset_file))\n","\n","SenseWords2id = get_Senses2id_Dic(root_path + '/{}/{}/y_allSenses_Sent.pkl'.format(dataset_file,dataset_file), root_path + '/{}/{}/y_BNsense_Sent.pkl'.format(dataset_file,dataset_file))\n","\n","lex2id = get_LEX2id_Dic(root_path + '/{}/{}/y_allLex_Sent.pkl'.format(dataset_file,dataset_file))\n","\n","wnDomain2id = get_2WnDomain2id_Dic(root_path + '/{}/{}/y_allWnDomain_Sent.pkl'.format(dataset_file,dataset_file))\n","\n","\n","all_dict_data = {}    \n","all_dict_data[\"RawWords2id\"] = RawWords2id\n","all_dict_data[\"SenseWords2id\"] = SenseWords2id\n","all_dict_data[\"pos2id\"] = pos2id\n","all_dict_data[\"lex2id\"] = lex2id\n","all_dict_data[\"wnDomain2id\"] = wnDomain2id\n","\n","with open(root_path + '/{}/{}/all_dict_data.pkl'.format(dataset_file,dataset_file), \"wb\") as outfile:\n","    pickle.dump(all_dict_data, outfile)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["get_RawWords2id_Dic is started ....\n","get_RawWords2id is done ....\n","get_POS2id_Dic is started ....\n","get_POS2id_Dic is done ....\n","get_Senses2id_Dic is started ....\n","get_Senses2id_Dic is done ....\n","get_LEX2id_Dic is started ....\n","get_2idDictionaries is done ....\n","get_2WnDomain2id_Dic is started ....\n","get_2idDictionaries is done ....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"epYctd54CoTK","colab_type":"code","outputId":"835e07d7-3507-4c97-a193-3a273a138993","executionInfo":{"status":"ok","timestamp":1568234679544,"user_tz":-120,"elapsed":707,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["dataset_file = 'semcor'\n","\n","all_dict_data = pickle.load(open(root_path+'/{}/{}/all_dict_data.pkl'.format(dataset_file,dataset_file), \"rb\"))\n","\n","RawWords2id = all_dict_data[\"RawWords2id\"]\n","SenseWords2id = all_dict_data[\"SenseWords2id\"] \n","pos2id = all_dict_data[\"pos2id\"] \n","lex2id = all_dict_data[\"lex2id\"] \n","wnDomain2id = all_dict_data[\"wnDomain2id\"]\n","\n","print(len(pos2id))\n","print(len(SenseWords2id))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12\n","60919\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wUrZ8mIS732j","colab_type":"code","colab":{}},"source":["# this 5 functions are for converting data to id\n","## -----------------------------------------------------------------------------\n","def get_Raw_data_2id(x_Raw_Sent_pkl, RawWords2id, dataset_name, file_name):   \n","    print('get_Raw_data_2id is started ....')\n","    \n","    x_Raw_Sent = pickle.load(open(x_Raw_Sent_pkl, \"rb\")) \n","    x_Raw_Sent_id = [[] for i in range(len(x_Raw_Sent))]        \n","    \n","    for i in range(len(x_Raw_Sent)):\n","        for j in range(len(x_Raw_Sent[i])):                        \n","            x_Raw_Sent_id[i].append(RawWords2id.get(x_Raw_Sent[i][j], RawWords2id[UNK]))            \n","            \n","    with open(root_path + '/{}/{}/x_Raw_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(x_Raw_Sent_id, outfile)\n","        \n","    print('get_Raw_data_2id is done ....')                \n","#     return x_Raw_Sent_id\n","\n","\n","\n","## -----------------------------------------------------------------------------\n","def get_BNSense_data_2id(y_BNsense_Sent_pkl, y_allSenses_Sent_pkl, SenseWords2id, dataset_name, file_name):   \n","    print('get_BNSense_data_2id is started ....')\n","\n","    y_BNsense_Sent = pickle.load(open(y_BNsense_Sent_pkl, \"rb\")) \n","    y_allSenses_Sent = pickle.load(open(y_allSenses_Sent_pkl, \"rb\")) \n","\n","    y_BNsense_Sent_id = [[] for i in range(len(y_BNsense_Sent))]    \n","    y_allSenses_Sent_id = [[] for i in range(len(y_allSenses_Sent))]    \n","    \n","    for i in range(len(y_BNsense_Sent)):\n","        for j in range(len(y_BNsense_Sent[i])):                                             \n","            y_BNsense_Sent_id[i].append(SenseWords2id.get(y_BNsense_Sent[i][j], SenseWords2id[UNK]))  \n","                                      \n","            y_allSenses_Sent_id[i].append([])\n","                        \n","            for k in range(len(y_allSenses_Sent[i][j])):                                            \n","                y_allSenses_Sent_id[i][j].append(SenseWords2id.get(y_allSenses_Sent[i][j][k], SenseWords2id[UNK]))  \n","                \n","    with open(root_path + '/{}/{}/y_BNsense_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(y_BNsense_Sent_id, outfile)\n","        \n","    with open(root_path + '/{}/{}/y_allSenses_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(y_allSenses_Sent_id, outfile)\n","        \n","    print('get_BNSense_data_2id is done ....')                \n","#     return y_BNsense_Sent_id, y_allSenses_Sent_id\n","\n","\n","## -----------------------------------------------------------------------------\n","def get_POS_data_2id(y_POS_Sent_pkl, y_allPos_Sent_pkl, pos2id, dataset_name, file_name):   \n","    print('get_POS_data_2id is started ....')\n","        \n","    y_POS_Sent = pickle.load(open(y_POS_Sent_pkl, \"rb\")) \n","    y_allPos_Sent = pickle.load(open(y_allPos_Sent_pkl, \"rb\"))\n","    \n","    y_POS_Sent_id = [[] for i in range(len(y_POS_Sent))]     \n","    y_allPos_Sent_id = [[] for i in range(len(y_allPos_Sent))]        \n","    \n","    for i in range(len(y_POS_Sent)):\n","        for j in range(len(y_POS_Sent[i])):                                    \n","            y_POS_Sent_id[i].append(pos2id.get(y_POS_Sent[i][j]))   \n","            \n","            y_allPos_Sent_id[i].append([])           \n","            \n","            for k in range(len(y_allPos_Sent[i][j])):                                                            \n","                y_allPos_Sent_id[i][j].append(pos2id.get(y_allPos_Sent[i][j][k]))                \n","\n","    with open(root_path + '/{}/{}/y_POS_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(y_POS_Sent_id, outfile)\n","        \n","    with open(root_path + '/{}/{}/y_allPos_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(y_allPos_Sent_id, outfile)\n","        \n","    print('get_POS_data_2id is done ....')                \n","#     return y_POS_Sent_id, y_allPos_Sent_id\n","\n","\n","\n","## -----------------------------------------------------------------------------\n","def get_LEX_data_2id(y_Lex_Sent_pkl, y_allLexs_Sent_pkl, lex2id, dataset_name, file_name):   \n","    print('get_LEX_data_2id is started ....')\n","\n","    y_Lex_Sent = pickle.load(open(y_Lex_Sent_pkl, \"rb\")) \n","    y_allLexs_Sent = pickle.load(open(y_allLexs_Sent_pkl, \"rb\"))\n","    \n","    y_Lex_Sent_id = [[] for i in range(len(y_Lex_Sent))]\n","    y_allLexs_Sent_id = [[] for i in range(len(y_allLexs_Sent))]\n","           \n","    for i in range(len(y_Lex_Sent)):\n","        for j in range(len(y_Lex_Sent[i])):                                    \n","            y_Lex_Sent_id[i].append(lex2id.get(y_Lex_Sent[i][j]))                                   \n","            \n","            y_allLexs_Sent_id[i].append([])\n","            \n","            for k in range(len(y_allLexs_Sent[i][j])):                                                            \n","                y_allLexs_Sent_id[i][j].append(lex2id.get(y_allLexs_Sent[i][j][k]))                \n","    \n","    with open(root_path + '/{}/{}/y_Lex_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(y_Lex_Sent_id, outfile)\n","        \n","    with open(root_path + '/{}/{}/y_allLex_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(y_allLexs_Sent_id, outfile)\n","        \n","    print('get_LEX_data_2id is done ....')                \n","#     return y_Lex_Sent_id, y_allLexs_Sent_id\n","\n","\n","## -----------------------------------------------------------------------------\n","def get_WnDomain_data_2id(y_WnDomain_Sent_pkl, y_allwnDomains_Sent_pkl, wnDomain2id, dataset_name, file_name):   \n","    print('get_WnDomain_data_2id is started ....')\n","        \n","    y_WnDomain_Sent = pickle.load(open(y_WnDomain_Sent_pkl, \"rb\")) \n","    y_allwnDomains_Sent = pickle.load(open(y_allwnDomains_Sent_pkl, \"rb\"))\n","    \n","    y_WnDomain_Sent_id = [[] for i in range(len(y_WnDomain_Sent))]               \n","    y_allwnDomains_Sent_id = [[] for i in range(len(y_allwnDomains_Sent))]\n","    \n","    \n","    for i in range(len(y_WnDomain_Sent)):\n","        for j in range(len(y_WnDomain_Sent[i])):                                    \n","            y_WnDomain_Sent_id[i].append(wnDomain2id.get(y_WnDomain_Sent[i][j]))              \n","            y_allwnDomains_Sent_id[i].append([])\n","            \n","            for k in range(len(y_allwnDomains_Sent[i][j])):                                                            \n","                y_allwnDomains_Sent_id[i][j].append(wnDomain2id.get(y_allwnDomains_Sent[i][j][k])) \n","    \n","    with open(root_path + '/{}/{}/y_WnDomain_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(y_WnDomain_Sent_id, outfile)\n","        \n","    with open(root_path + '/{}/{}/y_allWnDomain_Sent_id.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(y_allwnDomains_Sent_id, outfile)\n","        \n","    print('get_WnDomain_data_2id is done ....')                \n","#     return y_WnDomain_Sent_id, y_allwnDomains_Sent_id\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbI3t4FbEdqc","colab_type":"code","outputId":"c9b21ab1-e605-4198-91fe-998369388277","executionInfo":{"status":"ok","timestamp":1568234756810,"user_tz":-120,"elapsed":61810,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["file_names = ['semcor', 'ALL', 'senseval2', 'senseval3', 'semeval2007', 'semeval2013', 'semeval2015']\n","dataset_name = 'semcor'\n","for file_name in file_names:\n","\n","    get_Raw_data_2id(root_path + '/{}/{}/x_Raw_Sent.pkl'.format(dataset_name, file_name), RawWords2id,dataset_name, file_name)\n","\n","    get_BNSense_data_2id(root_path + '/{}/{}/y_BNsense_Sent.pkl'.format(dataset_name, file_name), root_path + '/{}/{}/y_allSenses_Sent.pkl'.format(dataset_name, file_name), SenseWords2id, dataset_name, file_name)\n","\n","    get_POS_data_2id(root_path + '/{}/{}/y_POS_Sent.pkl'.format(dataset_name, file_name), root_path + '/{}/{}/y_allPos_Sent.pkl'.format(dataset_name, file_name), pos2id, dataset_name, file_name)\n","\n","    get_LEX_data_2id(root_path + '/{}/{}/y_Lex_Sent.pkl'.format(dataset_name, file_name), root_path + '/{}/{}/y_allLex_Sent.pkl'.format(dataset_name, file_name), lex2id, dataset_name, file_name)\n","\n","    get_WnDomain_data_2id(root_path + '/{}/{}/y_WnDomain_Sent.pkl'.format(dataset_name, file_name), root_path + '/{}/{}/y_allWnDomain_Sent.pkl'.format(dataset_name, file_name), wnDomain2id, dataset_name, file_name)\n","    \n","    print('processing is ok for -----> ', file_name)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["get_Raw_data_2id is started ....\n","get_Raw_data_2id is done ....\n","get_BNSense_data_2id is started ....\n","get_BNSense_data_2id is done ....\n","get_POS_data_2id is started ....\n","get_POS_data_2id is done ....\n","get_LEX_data_2id is started ....\n","get_LEX_data_2id is done ....\n","get_WnDomain_data_2id is started ....\n","get_WnDomain_data_2id is done ....\n","processing is ok for ----->  semcor\n","get_Raw_data_2id is started ....\n","get_Raw_data_2id is done ....\n","get_BNSense_data_2id is started ....\n","get_BNSense_data_2id is done ....\n","get_POS_data_2id is started ....\n","get_POS_data_2id is done ....\n","get_LEX_data_2id is started ....\n","get_LEX_data_2id is done ....\n","get_WnDomain_data_2id is started ....\n","get_WnDomain_data_2id is done ....\n","processing is ok for ----->  ALL\n","get_Raw_data_2id is started ....\n","get_Raw_data_2id is done ....\n","get_BNSense_data_2id is started ....\n","get_BNSense_data_2id is done ....\n","get_POS_data_2id is started ....\n","get_POS_data_2id is done ....\n","get_LEX_data_2id is started ....\n","get_LEX_data_2id is done ....\n","get_WnDomain_data_2id is started ....\n","get_WnDomain_data_2id is done ....\n","processing is ok for ----->  senseval2\n","get_Raw_data_2id is started ....\n","get_Raw_data_2id is done ....\n","get_BNSense_data_2id is started ....\n","get_BNSense_data_2id is done ....\n","get_POS_data_2id is started ....\n","get_POS_data_2id is done ....\n","get_LEX_data_2id is started ....\n","get_LEX_data_2id is done ....\n","get_WnDomain_data_2id is started ....\n","get_WnDomain_data_2id is done ....\n","processing is ok for ----->  senseval3\n","get_Raw_data_2id is started ....\n","get_Raw_data_2id is done ....\n","get_BNSense_data_2id is started ....\n","get_BNSense_data_2id is done ....\n","get_POS_data_2id is started ....\n","get_POS_data_2id is done ....\n","get_LEX_data_2id is started ....\n","get_LEX_data_2id is done ....\n","get_WnDomain_data_2id is started ....\n","get_WnDomain_data_2id is done ....\n","processing is ok for ----->  semeval2007\n","get_Raw_data_2id is started ....\n","get_Raw_data_2id is done ....\n","get_BNSense_data_2id is started ....\n","get_BNSense_data_2id is done ....\n","get_POS_data_2id is started ....\n","get_POS_data_2id is done ....\n","get_LEX_data_2id is started ....\n","get_LEX_data_2id is done ....\n","get_WnDomain_data_2id is started ....\n","get_WnDomain_data_2id is done ....\n","processing is ok for ----->  semeval2013\n","get_Raw_data_2id is started ....\n","get_Raw_data_2id is done ....\n","get_BNSense_data_2id is started ....\n","get_BNSense_data_2id is done ....\n","get_POS_data_2id is started ....\n","get_POS_data_2id is done ....\n","get_LEX_data_2id is started ....\n","get_LEX_data_2id is done ....\n","get_WnDomain_data_2id is started ....\n","get_WnDomain_data_2id is done ....\n","processing is ok for ----->  semeval2015\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1gZssTsh6lnY","colab_type":"code","colab":{}},"source":["# this 3 function is for creating masks of pos, lex and WnDomain to sense vocabulary\n","# our method is creatin an 2D list with size of pos dic * size of sense dic, and then put 1 in places that [pos][sense], we do the same for othe masks too.\n","\n","## ----------------------------------------------------------------------------\n","def mask_PosToSenses(y_allPos_Sent_pkl, y_allSenses_Sent_id_pkl, pos2id, SenseWords2id, dataset_name, file_name):\n","    print('mask_PosToSenses is started ....')\n","    \n","    y_allPos_Sent = pickle.load(open(y_allPos_Sent_pkl, \"rb\"))\n","    y_allSenses_Sent_id = pickle.load(open(y_allSenses_Sent_id_pkl, \"rb\"))\n","    \n","    mask_pos2sense = [0]*np.ones(shape = (len(pos2id), len(SenseWords2id)))\n","    \n","    for i in range(len(y_allPos_Sent)):\n","        for j in range(len(y_allPos_Sent[i])):\n","            for k in range(len(y_allPos_Sent[i][j])):\n","                if pos2id.get(y_allPos_Sent[i][j][k]) is not None:\n","                    mask_pos2sense[pos2id[y_allPos_Sent[i][j][k]]][y_allSenses_Sent_id[i][j][k]] = 1                                    \n","    \n","    with open(root_path + '/{}/{}/mask_pos2sense.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(mask_pos2sense, outfile)\n","        \n","    print('mask_PosToSenses is done ....')\n","#     return mask_pos2sense   \n","\n","\n","## ----------------------------------------------------------------------------\n","def mask_LexToSenses(y_allLexs_Sent_pkl, y_allSenses_Sent_id_pkl, lex2id, SenseWords2id, dataset_name, file_name):\n","    print('mask_LexToSenses is started ....')\n","    \n","    y_allLexs_Sent = pickle.load(open(y_allLexs_Sent_pkl, \"rb\"))\n","    y_allSenses_Sent_id = pickle.load(open(y_allSenses_Sent_id_pkl, \"rb\"))\n","    \n","    mask_lex2sense = [0]*np.ones(shape = (len(lex2id), len(SenseWords2id)))\n","    \n","    for i in range(len(y_allLexs_Sent)):\n","        for j in range(len(y_allLexs_Sent[i])):\n","            for k in range(len(y_allLexs_Sent[i][j])):\n","                if lex2id.get(y_allLexs_Sent[i][j][k]) is not None:\n","                    mask_lex2sense[lex2id[y_allLexs_Sent[i][j][k]]][y_allSenses_Sent_id[i][j][k]] = 1    \n","    \n","    with open(root_path + '/{}/{}/mask_lex2sense.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(mask_lex2sense, outfile)\n","        \n","    print('mask_LexToSenses is done ....')                                \n","#     return mask_lex2sense   \n","\n","\n","\n","## ----------------------------------------------------------------------------\n","def mask_WnDomansToSenses(y_allwnDomains_Sent_pkl, y_allSenses_Sent_id_pkl, wnDomain2id, SenseWords2id, dataset_name, file_name):\n","    print('mask_WnDomansToSenses is started ....')\n","    \n","    y_allwnDomains_Sent = pickle.load(open(y_allwnDomains_Sent_pkl, \"rb\"))\n","    y_allSenses_Sent_id = pickle.load(open(y_allSenses_Sent_id_pkl, \"rb\"))    \n","    \n","    mask_wnDomain2sense = [0]*np.ones(shape = (len(wnDomain2id), len(SenseWords2id)))\n","    \n","    for i in range(len(y_allwnDomains_Sent)):\n","        for j in range(len(y_allwnDomains_Sent[i])):\n","            for k in range(len(y_allwnDomains_Sent[i][j])):\n","                if wnDomain2id.get(y_allwnDomains_Sent[i][j][k]) is not None:\n","                    mask_wnDomain2sense[wnDomain2id[y_allwnDomains_Sent[i][j][k]]][y_allSenses_Sent_id[i][j][k]] = 1    \n","    \n","    with open(root_path + '/{}/{}/mask_wnDomain2sense.pkl'.format(dataset_name, file_name), \"wb\") as outfile:\n","        pickle.dump(mask_wnDomain2sense, outfile)\n","    \n","    print('mask_WnDomansToSenses is done ....')    \n","#     return mask_wnDomain2sense   \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwMXy3voHuy-","colab_type":"code","outputId":"409e82a8-541b-4bc7-d9e9-2c70cb89cdce","executionInfo":{"status":"ok","timestamp":1568234801023,"user_tz":-120,"elapsed":32299,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":813}},"source":["file_names = ['semcor', 'ALL', 'senseval2', 'senseval3', 'semeval2007', 'semeval2013', 'semeval2015']\n","dataset_name = 'semcor'\n","\n","for file_name in file_names:\n","    mask_PosToSenses(root_path + '/{}/{}/y_allPos_Sent.pkl'.format(dataset_name, file_name), root_path + '/{}/{}/y_allSenses_Sent_id.pkl'.format(dataset_name, file_name), pos2id, SenseWords2id, dataset_name, file_name)\n","\n","    mask_LexToSenses(root_path + '/{}/{}/y_allLex_Sent.pkl'.format(dataset_name, file_name), root_path + '/{}/{}/y_allSenses_Sent_id.pkl'.format(dataset_name, file_name), lex2id, SenseWords2id, dataset_name, file_name)\n","\n","    mask_WnDomansToSenses(root_path + '/{}/{}/y_allWnDomain_Sent.pkl'.format(dataset_name, file_name), root_path + '/{}/{}/y_allSenses_Sent_id.pkl'.format(dataset_name, file_name), wnDomain2id, SenseWords2id, dataset_name, file_name)\n","    \n","    print('processing is ok for -----> ', file_name)\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["mask_PosToSenses is started ....\n","mask_PosToSenses is done ....\n","mask_LexToSenses is started ....\n","mask_LexToSenses is done ....\n","mask_WnDomansToSenses is started ....\n","mask_WnDomansToSenses is done ....\n","processing is ok for ----->  semcor\n","mask_PosToSenses is started ....\n","mask_PosToSenses is done ....\n","mask_LexToSenses is started ....\n","mask_LexToSenses is done ....\n","mask_WnDomansToSenses is started ....\n","mask_WnDomansToSenses is done ....\n","processing is ok for ----->  ALL\n","mask_PosToSenses is started ....\n","mask_PosToSenses is done ....\n","mask_LexToSenses is started ....\n","mask_LexToSenses is done ....\n","mask_WnDomansToSenses is started ....\n","mask_WnDomansToSenses is done ....\n","processing is ok for ----->  senseval2\n","mask_PosToSenses is started ....\n","mask_PosToSenses is done ....\n","mask_LexToSenses is started ....\n","mask_LexToSenses is done ....\n","mask_WnDomansToSenses is started ....\n","mask_WnDomansToSenses is done ....\n","processing is ok for ----->  senseval3\n","mask_PosToSenses is started ....\n","mask_PosToSenses is done ....\n","mask_LexToSenses is started ....\n","mask_LexToSenses is done ....\n","mask_WnDomansToSenses is started ....\n","mask_WnDomansToSenses is done ....\n","processing is ok for ----->  semeval2007\n","mask_PosToSenses is started ....\n","mask_PosToSenses is done ....\n","mask_LexToSenses is started ....\n","mask_LexToSenses is done ....\n","mask_WnDomansToSenses is started ....\n","mask_WnDomansToSenses is done ....\n","processing is ok for ----->  semeval2013\n","mask_PosToSenses is started ....\n","mask_PosToSenses is done ....\n","mask_LexToSenses is started ....\n","mask_LexToSenses is done ....\n","mask_WnDomansToSenses is started ....\n","mask_WnDomansToSenses is done ....\n","processing is ok for ----->  semeval2015\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OvdXIV0DwVzf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6W6FmyzfxY_b","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"gHDw_9aExZWx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}