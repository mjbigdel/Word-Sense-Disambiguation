{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BASIC_BDLSTM_Tagger.ipynb","version":"0.3.2","provenance":[{"file_id":"1W6GHhtAv1zZ1Z2D6lLvC-uDSDWJD130O","timestamp":1567765261410}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ueQIwiNm42tG","colab_type":"code","outputId":"303b101a-9a49-4518-d9eb-38e2c617e70b","executionInfo":{"status":"ok","timestamp":1567550423515,"user_tz":-120,"elapsed":3945,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# !pip install tensorflow \n","# TensorFlow and tf.keras\n","# its just working with tensorflow 1.13.1, with others has problem\n","import tensorflow as tf\n","print(tf.__version__)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.14.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z_pjD2e15BFA","colab_type":"text"},"source":["**- goolge Colab Configuration**"]},{"cell_type":"code","metadata":{"id":"nZKvqNpW5HjM","colab_type":"code","outputId":"74291a04-2e97-44bf-cef9-eec260a8ee6f","executionInfo":{"status":"ok","timestamp":1568303443548,"user_tz":-120,"elapsed":17420,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UTR5Csn45f2I","colab_type":"text"},"source":["**- Setting root address to project in Google Drive**"]},{"cell_type":"code","metadata":{"id":"ywAPM0ns5dX-","colab_type":"code","outputId":"89285423-18bd-4ac0-97e4-c74b006803aa","executionInfo":{"status":"ok","timestamp":1568303446036,"user_tz":-120,"elapsed":4890,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":472}},"source":["!ls \"/content/gdrive/My Drive/NLPHW3\"\n","root_path = '/content/gdrive/My Drive/NLPHW3'"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" allWordsSensesembeddings.vec\n"," BackUpCodes\n","'Basic BDLSTM '\n"," Basic_BDLSTM_models_46.72686230248307\n"," BASIC_BDLSTM_Tagger.ipynb\n"," Data_Prepare_PKL_Format.ipynb\n"," Evaluation_Datasets\n","'LEX BDLSTM'\n"," LEX_BDLSTM_Tagger.ipynb\n"," Mask_LSTM_Model1.ipynb\n"," MFS_Tagger.ipynb\n","'POS BDLSTM'\n"," POS_BDLSTM_Tagger.ipynb\n","'POS_LEX BDLSTM'\n"," POS_LEX_BDLSTM_models_48.75846501128668\n"," POS_LEX_BDLSTM_Tagger.ipynb\n","'POS_LEX_WNDOMAIN BDLSTM '\n"," POS_LEX_WnDomain_BDLSTM_Tagger.ipynb\n","'POS_WNDOMAIN BDLSTM'\n"," POS_WnDomain_BDLSTM_Tagger.ipynb\n"," resources\n"," semcor\n"," semcor+omsti\n"," Test_Prediction_Write.ipynb\n"," Training_Corpora\n","'WNDOMAIN BDLSTM'\n"," WnDomain_BDLSTM_Tagger.ipynb\n"," WSD_Evaluation_Framework\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"waoW11y6yLmL","colab_type":"code","outputId":"d7f3b153-5e7f-4c25-eef6-535dc58a5918","executionInfo":{"status":"ok","timestamp":1568303447615,"user_tz":-120,"elapsed":6007,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# Imports\n","from collections import Counter\n","import codecs\n","import xml.etree.cElementTree as etree\n","import pickle\n","import nltk\n","nltk.download('wordnet')\n","import numpy as np\n","import copy\n","from nltk.corpus import wordnet as wn\n","\n","\n","# Added Chars to dictionaries\n","UNK = \"<UNK>\"\n","PAD = \"<PAD>\"\n","\n","\n","RESOURCE_PATH = root_path+'/resources/'\n","\n","MAP_WN2BN_FILE = RESOURCE_PATH + 'babelnet2wordnet.tsv'\n","MAP_BN2WNDOMAIN_FILE = RESOURCE_PATH + 'babelnet2wndomains.tsv'\n","MAP_BN2LEXNAMES_FILE = RESOURCE_PATH + 'babelnet2lexnames.tsv'\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"epYctd54CoTK","colab_type":"code","outputId":"f4c6fa98-2707-4fff-c423-2ce6d1d70719","executionInfo":{"status":"ok","timestamp":1568303449742,"user_tz":-120,"elapsed":7507,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["all_dict_data = pickle.load(open(root_path+'/semcor/semcor/all_dict_data.pkl', \"rb\"))\n","\n","RawWords2id = all_dict_data[\"RawWords2id\"]\n","SenseWords2id = all_dict_data[\"SenseWords2id\"] \n","pos2id = all_dict_data[\"pos2id\"] \n","lex2id = all_dict_data[\"lex2id\"] \n","wnDomain2id = all_dict_data[\"wnDomain2id\"]\n","\n","print(len(pos2id))\n","print(len(SenseWords2id))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12\n","60919\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6W6FmyzfxY_b","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"pT5cObkBpThO","colab_type":"code","colab":{}},"source":["# ===-----------------------------------------------------------------------===\n","# Trainig Section\n","# ===-----------------------------------------------------------------------===\n","\n","# function for add padding for train_y in every batch based on maximum length of sentence in that batch\n","# this method boost the performance of running.\n","def padding(X,padding_word):\n","\tmax_len = 0\n","\tfor x in X:\n","\t\tif len(x) > max_len:\n","\t\t\tmax_len = len(x)\n","\tpadded_X = np.ones((len(X), max_len), dtype=np.int32) * padding_word\n","\tfor i in range(len(X)):\n","\t\tfor j in range(len(X[i])):\n","\t\t\tpadded_X[i, j] = X[i][j]\n","\treturn padded_X\n","\n","\n","# padding function for train_x\n","def padding3(X,padding_word):\n","\tmax_len = 0\n","\tfor x in X:\n","\t\tif len(x) > max_len:\n","\t\t\tmax_len = len(x)\n","\tpadded_X = np.ones((len(X), max_len), dtype=np.int32) * padding_word\n","\tfor i in range(len(X)):\n","\t\tfor j in range(len(X[i])):\n","\t\t\tpadded_X[i, j] = X[i][j]\n","\treturn padded_X\n","\n","# function for calculating umber of hits and number of all predictions.\n","def number_of_batch_hits(y_batch_pred,y_batch_true):\n","    num_hits = 0\n","    num_all_chars = 0    \n","    for i in range(len(y_batch_true)):\n","        for j in range(len(y_batch_true[i])):\n","            num_all_chars = num_all_chars+1\n","            if y_batch_pred[i][j] == y_batch_true[i][j]:\n","                num_hits = num_hits+1    \n","    return num_hits, num_all_chars\n","\n","# function for calculating umber of hits and number of all predictions.\n","def number_of_batch_sense_hits(y_batch_pred,y_batch_true):\n","    num_hits = 0\n","    num_all_chars = 0    \n","    for i in range(len(y_batch_true)):\n","        for j in range(len(y_batch_true[i])):\n","            if id2Sensewords[y_batch_true[i][j]].startswith('bn'):\n","                num_all_chars = num_all_chars+1\n","                if y_batch_pred[i][j] == y_batch_true[i][j]:\n","                    num_hits = num_hits+1    \n","    return num_hits, num_all_chars\n","\n","# ----------------- Add Summary Function ------------------------------------------\n","def add_summary(writer, name, value, global_step):\n","    summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n","    writer.add_summary(summary, global_step=global_step)                                 \n","    \n","\n","## ----------------------------------------------------------------------------\n","def mask_lemmaToSenses_batch(x_Raw_Sent_batch, y_allSenses_Sent_batch, y_allSenses_Sent_id_batch, SenseWords2id):\n","     \n","    max_len = 0    \n","    for inp in x_Raw_Sent_batch:\n","        if len(inp) > max_len:\n","            max_len = len(inp)\n","            \n","    mask_lemma2sense_batch = [0]*np.ones(shape = (len(x_Raw_Sent_batch), max_len, len(SenseWords2id)))\n","            \n","    for i in range(len(x_Raw_Sent_batch)):\n","        for j in range(len(x_Raw_Sent_batch[i])):\n","            for k in range(len(y_allSenses_Sent_batch[i][j])):            \n","#                 if SenseWords2id.get(y_allSenses_Sent_batch[i][j][k]) is not None:\n","                mask_lemma2sense_batch[i][j][y_allSenses_Sent_id_batch[i][j][k]] = 1    \n","#                    mask_lemma2sense_batch[rawWords_batch2id[x_Raw_Sent_batch[i][j]]][y_allSenses_Sent_id_batch[i][j][k]] = 1    \n","#                 else:\n","#                     print('not in dictionary sense is : ', y_allSenses_Sent_batch[i][j][k])\n","            \n","    return mask_lemma2sense_batch    \n","\n","#x_Raw_Sent_batch = x_Raw_Sent[:12]\n","#y_allSenses_Sent_batch = y_allSenses_Sent[:12]\n","#y_allSenses_Sent_id_batch = y_allSenses_Sent_id[:12]\n","#\n","#mask_lemma2sense_batch = mask_lemmaToSenses_batch(x_Raw_Sent_batch, y_allSenses_Sent_batch, y_allSenses_Sent_id_batch, SenseWords2id)    \n","\n","# use pretrained embeddings function, for chars we are using pretrain file and for bigram, trigram \n","# and fourgram we are using mean of embeddings of all unigrams of them\n","def reading_pretrained_sense_embeddings(filename, RawWords2id):\n","    # Reading Pretrained Embeddings from file\n","    pretrain_embeddigs = {}\n","    with codecs.open(filename, \"r\", \"utf-8\") as f:\n","        for line in f:\n","            pre_train = line.split()\n","            if len(pre_train) > 2:\n","                word = pre_train[0]\n","                if word in RawWords2id:\n","                    vec = pre_train[1:]\n","                    pretrain_embeddigs[word] = vec                \n","    \n","    print(\"pretrained embeddings files reading finished ...\")\n","    # making embeddings for all RawWords2id.\n","    embedding_dim = len(next(iter(pretrain_embeddigs.values())))\n","    out_of_vocab = 0\n","    out = np.ones((len(RawWords2id), embedding_dim))*0.001\n","    for word in RawWords2id.keys():\n","        if len(word) == 1:\n","            if word in pretrain_embeddigs.keys():        \n","                out[RawWords2id[word]]=np.array(pretrain_embeddigs[word])\n","            else:                \n","                out_of_vocab+=1\n","                np.random.uniform(-1.0, 1.0, embedding_dim)\n","        \n","    return out,out_of_vocab\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifW2h9YyD7oi","colab_type":"code","outputId":"16a6c8b8-e32c-4b51-f7f9-ebf02a223efd","executionInfo":{"status":"ok","timestamp":1568303457286,"user_tz":-120,"elapsed":10032,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["PAD_ID = RawWords2id[PAD]\n","id2Sensewords = dict(zip(SenseWords2id.values(), SenseWords2id.keys()))\n","\n","sense_embeddings, out_of_vocab = reading_pretrained_sense_embeddings(root_path + '/allWordsSensesembeddings.vec', RawWords2id)\n","print('sense_embeddings: ', len(sense_embeddings))\n","print('out_of_vocab: ', out_of_vocab)\n","\n","# we are using pretrained embedding - len(words2id)\n","VOCAB_SIZE =  len(RawWords2id)\n","WORD_EMBEDDING_DIM = 100\n","print('WORD_EMBEDDING_DIM: ', WORD_EMBEDDING_DIM)\n","print('VOCAB_SIZE: ', VOCAB_SIZE)\n","\n","# some Basic Hyper parameters\n","NUM_EPOCHS = 5\n","BATCH_SIZE = 4\n","HIDDEN_LAYER_DIM = 256\n","LEARNING_RATE = 1\n","NUM_CLASSES = len(SenseWords2id)\n","print('num sense classes: ', NUM_CLASSES)\n","L2_REGU_LAMBDA=0.0001\n","NUM_LAYERS = 2\n","CLIP=10\n","num_sense_pos = len(pos2id)\n","print('num_sense_pos: ', num_sense_pos)\n","num_sense_lex = len(lex2id)\n","print('num_sense_lex: ', num_sense_lex)\n","num_sense_wnDomain = len(wnDomain2id)\n","print('num_sense_wnDomain: ', num_sense_wnDomain)\n","\n","VERY_BIG_NUMBER = 1e30\n","summaries = []\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["pretrained embeddings files reading finished ...\n","sense_embeddings:  32893\n","out_of_vocab:  5\n","WORD_EMBEDDING_DIM:  100\n","VOCAB_SIZE:  32893\n","num sense classes:  60919\n","num_sense_pos:  12\n","num_sense_lex:  46\n","num_sense_wnDomain:  160\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c7VncpqopT0M","colab_type":"code","colab":{}},"source":["\n","# --------------------- Tensorflow part ---------------------------------------------------\n","import tensorflow as tf\n","from tensorflow.contrib import layers\n","from tensorflow.contrib import crf\n","\n","def create_tensorflow_model(vocab_size, embedding_dim, hidden_layer_dim, sense_embed_bool = False, total_loss = 0):\n","    print(\"Creating TENSORFLOW model\")\n","     \n","    # Inputs have (batch_size, timesteps) shape.\n","    input_ = tf.placeholder(dtype = tf.int32, shape=[None, None], name='input_x')   \n","    # Labels have (batch_size,) shape.\n","    labels = tf.placeholder(dtype = tf.int32, shape=[None, None], name='labels_BN')\n","           \n","    \n","    # dropout_keep_prob is a scalar.\n","    dropout_keep_prob = tf.placeholder(dtype = tf.float32, name='dropout_keep_prob')\n","    \n","    seq_length = tf.reduce_sum(tf.cast( tf.not_equal(input_[:,:], tf.ones_like(input_[:,:] ) * PAD_ID ), tf.int32), 1)    \n","    print('seq_length: ', seq_length)\n","    \n","    max_sent_size = tf.size(input_[0,:])    \n","    print('max_sent_size: ', max_sent_size)\n","    \n","    batch_size = tf.size(input_[:,0])    \n","    print('batch_size: ', batch_size)\n","\n","    mask_RawTosense_batch = tf.placeholder(dtype = tf.bool, shape=[None, None, None], name='mask_Raw2Senses')    \n","    print('mask_RawTosense_batch: ', mask_RawTosense_batch)       \n","\n","    \n","    x_mask = tf.not_equal(input_[:,:], tf.ones_like(input_[:,:] ) * PAD_ID )    \n","    print('x_mask: ', x_mask)\n","    \n","    attention_mask = (tf.cast(x_mask, 'float') -1) * VERY_BIG_NUMBER     \n","    print('attention_mask: ', attention_mask)\n","\n","    # initialize weights randomly from a Gaussian distribution\n","    # step 1: create the initializer for weights\n","    weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)  \n","    \n","    \n","    \n","## ---------------------------- embedding Block --------------------------------------------\n","## -----------------------------------------------------------------------------------------  \n","    with tf.variable_scope('embeddings', reuse=tf.AUTO_REUSE):\n","        if sense_embed_bool:\n","            embedding_matrix = tf.Variable(sense_embeddings, dtype=tf.float32, name='embedding')\n","        else:\n","            embedding_matrix = tf.get_variable(\"embeddings\", shape=[vocab_size, embedding_dim])\n","        \n","        embeddings = tf.nn.embedding_lookup(embedding_matrix, input_)        \n","        print('embeddings: ', embeddings)\n","        \n","        embeddings = tf.reshape(embeddings,[batch_size, max_sent_size, embedding_dim]) #         \n","        print('embeddings: ', embeddings)\n","\n","        embeddings = tf.nn.dropout(tf.cast(embeddings, tf.float32), dropout_keep_prob) #     embeddings shape (batch size, sentence length with padding, 100) \n","\n","        print ('embeddings is ok \\n')\n","                \n","\n","            \n","## --------------------------- SENSE Block ------------------------------------------------   \n","## ----------------------------------------------------------------------------------------   \n","    with tf.variable_scope('rnn_cell_sense', reuse=tf.AUTO_REUSE):\n","            print ('lstm_sense is ok')\n","            def lstm_cell():\n","                return tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_layer_dim), output_keep_prob=dropout_keep_prob)\n","        \n","            stacked_fw_lstm_Sense = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell() for _ in range(NUM_LAYERS)])\n","            \n","            stacked_bw_lstm_Sense = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell() for _ in range(NUM_LAYERS)])\n","            \n","    with tf.variable_scope('rnn_sense', reuse=tf.AUTO_REUSE):                        \n","            (forward_output_Sense, backword_output_Sense), _ = tf.nn.bidirectional_dynamic_rnn(\n","                cell_fw = stacked_fw_lstm_Sense,\n","                cell_bw = stacked_bw_lstm_Sense,\n","                inputs = embeddings,\n","                sequence_length = seq_length,\n","                dtype=tf.float32\n","            )\n","            \n","            outputBD_Sense = tf.concat([forward_output_Sense, backword_output_Sense], axis=2)            \n","            print('outputBD_Sense: ', outputBD_Sense) # shape is batch*max_len*(2*hidden_layer_size) \n","            \n","            print ('outputBD_Sense is ok \\n')   \n","\n","            \n","## --------------------------- attention_layer --------------------------------------------------\n","    with tf.variable_scope(\"attention_layer\", reuse=tf.AUTO_REUSE):\n","        W_attention_L = tf.get_variable(\"W_attention_L\", shape=[2*hidden_layer_dim, 1], initializer = weight_initer )\n","        flat_outputBD_Sense = tf.reshape(outputBD_Sense, [batch_size*max_sent_size, tf.shape(outputBD_Sense)[2]])        \n","        print('flat_outputBD_Sense: ', flat_outputBD_Sense) # shape is  shape is (batch*max_length*(2*hidden_layer_size)).(51(2*hidden_layer_size)2*1) = (batch*max_length)*1 ->2D\n","        \n","        flat_outputBD_Sense_tanh = tf.tanh(flat_outputBD_Sense)\n","        \n","        u_flat_outputBD_Sense_tanh = tf.matmul(flat_outputBD_Sense_tanh, W_attention_L) # shape is batch*max_length -> 2D\n","        print('u_flat: ', u_flat_outputBD_Sense_tanh)\n","        \n","        u = tf.reshape(u_flat_outputBD_Sense_tanh, [batch_size, max_sent_size]) + attention_mask #\n","        print('u: ', u)\n","        \n","        u_softmax = tf.nn.softmax(u, 1)\n","        \n","        a = tf.expand_dims(u_softmax, 2) # shape is expand to batch*max_len*1 -> 3D\n","        print('a: ', a)\n","        \n","        c = tf.reduce_sum(tf.multiply(outputBD_Sense, a), axis=1) # shape is batch*max_len*(2*hidden_layer_size) and then reduce_sum with axis 1 to: batch*(2*hidden_layer_size)\n","        print('c: ', c)\n","        \n","        tiled_c = tf.tile(tf.expand_dims(c, 1), [1, max_sent_size, 1]) # shape is batch*max_len*(2*hidden_layer_size)\n","        print('tiled_c: ', tiled_c)\n","        \n","        attention_output = tf.concat([tiled_c, outputBD_Sense], 2) # batch*max_len*(2*hidden_layer_size) \"concat with\" batch*max_len*(2*hidden_layer_size) -> batch*max_len*(4*hidden_layer_size)\n","        print('attention_output: ', attention_output)\n","        \n","        flat_attention_output = tf.reshape(attention_output, [batch_size*max_sent_size, tf.shape(attention_output)[2]]) # reshape to (batch*max_len)*(4*hidden_layer_size) -> 2D\n","        print('flat_attention_output: ', flat_attention_output)\n","\n","        print ('global_attention is ok \\n') \n","        \n","## --------------------------- hidden_layer Sense ------------------------------------------------------------\n","    with tf.variable_scope(\"hidden_layer\", reuse=tf.AUTO_REUSE):\n","        W = tf.get_variable(\"W\", shape=[4*hidden_layer_dim, 2*hidden_layer_dim], initializer = weight_initer)\n","        b = tf.get_variable(\"b\", shape=[2*hidden_layer_dim], initializer = tf.zeros_initializer())\n","        \n","        drop_flat_attention_output = tf.nn.dropout(flat_attention_output, dropout_keep_prob) # shape is (batch*max_len)*(4*hidden_layer_size) -> 2D\n","        print('drop_flat_attention_output: ', drop_flat_attention_output)\n","        \n","        hidden_layer_output = tf.matmul(drop_flat_attention_output, W) + b  # shape is (batch*max_len)*(2*hidden_layer_size) -> 2D\n","        print('hidden_layer_output: ', hidden_layer_output)\n","                \n","        print ('hidden_layer is ok \\n') \n","        \n","## --------------------------- softmax_layer Sense ------------------------------------------------------------\n","    with tf.variable_scope(\"softmax_layer\", reuse=tf.AUTO_REUSE):\n","        W_sense = tf.get_variable(\"W_sense\", shape=[2*hidden_layer_dim, NUM_CLASSES], initializer = weight_initer)\n","        b_sense = tf.get_variable(\"b_sense\", shape=[NUM_CLASSES], initializer=tf.zeros_initializer())\n","        \n","        drop_hidden_layer_output = tf.nn.dropout(hidden_layer_output, dropout_keep_prob)\n","        print('drop_hidden_layer_output: ', drop_hidden_layer_output)\n","        \n","        flat_sense_logits = tf.matmul(drop_hidden_layer_output, W_sense) + b_sense\n","        print('flat_sense_logits: ', flat_sense_logits)\n","        \n","        sense_logits = tf.reshape(flat_sense_logits, [batch_size, max_sent_size, NUM_CLASSES])\n","        print('sense_logits: ', sense_logits)        \n","            \n","        float_mask_RawTosense_batch = tf.cast(mask_RawTosense_batch, dtype=tf.float32)\n","        print('float_mask_RawTosense_batch: ', float_mask_RawTosense_batch)\n","        \n","        final_sense_logits = tf.multiply(sense_logits, float_mask_RawTosense_batch)\n","        print('final_sense_logits: ', final_sense_logits)\n","\n","        predictions = tf.argmax(final_sense_logits , axis=2)\n","        print('predictions: ', predictions)\n","        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= labels, logits= final_sense_logits))\n","        total_loss = total_loss + loss\n","        print ('softmax_layer is ok \\n')                \n","                \n","## --------------------------- train_op Block ----------------------------------------------\n","## -----------------------------------------------------------------------------------------          \n","    with tf.variable_scope('train_op', reuse=tf.AUTO_REUSE):\n","                        \n","        optimizer = tf.train.AdadeltaOptimizer(LEARNING_RATE)\n","        \n","#         optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n","#         optimizer=tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE, use_locking=False, name='GradientDescent')\n","        print ('AdamOptimizer is ok .... \\n')\n","        \n","        tvars=tf.trainable_variables()\n","        print ('tvars is ok ....')\n","    \n","        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tvars if v.get_shape().ndims > 1])\n","        print ('l2_loss is ok ....')\n","        \n","        total_loss = total_loss + L2_REGU_LAMBDA*l2_loss             \n","        print ('total_loss is ok ....')\n","        \n","        summaries.append(tf.summary.scalar(\"loss\", loss))\n","        summaries.append(tf.summary.scalar(\"total_loss\", total_loss))\n","        \n","        grads,_ = tf.clip_by_global_norm(tf.gradients(total_loss,tvars),CLIP)\n","        print ('grads is ok ....')\n","        \n","        train_op = optimizer.apply_gradients(zip(grads,tvars))\n","        print ('train_op apply_gradients is ok ....')\n","                               \n","    return input_, labels, mask_RawTosense_batch, train_op, predictions, dropout_keep_prob, total_loss, seq_length\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjH5I--oEvI0","colab_type":"code","outputId":"407e3847-3b1a-4cbf-ecfd-ea5bcad7bcc9","executionInfo":{"status":"ok","timestamp":1568303465606,"user_tz":-120,"elapsed":3588,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# create tensorflow model without sense embedding and just basic LSTM\n","input_, labels, mask_RawTosense_batch, train_op, predictions, dropout_keep_prob, total_loss, seq_length,\\\n","    = create_tensorflow_model(VOCAB_SIZE, WORD_EMBEDDING_DIM, HIDDEN_LAYER_DIM, sense_embed_bool = False, total_loss = 0)\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Creating TENSORFLOW model\n","seq_length:  Tensor(\"Sum:0\", shape=(?,), dtype=int32)\n","max_sent_size:  Tensor(\"Size:0\", shape=(), dtype=int32)\n","batch_size:  Tensor(\"Size_1:0\", shape=(), dtype=int32)\n","mask_RawTosense_batch:  Tensor(\"mask_Raw2Senses:0\", shape=(?, ?, ?), dtype=bool)\n","x_mask:  Tensor(\"NotEqual_1:0\", shape=(?, ?), dtype=bool)\n","attention_mask:  Tensor(\"mul_2:0\", shape=(?, ?), dtype=float32)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","embeddings:  Tensor(\"embeddings/embedding_lookup/Identity:0\", shape=(?, ?, 100), dtype=float32)\n","embeddings:  Tensor(\"embeddings/Reshape:0\", shape=(?, ?, 100), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-7-3215cdb692d4>:56: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","embeddings is ok \n","\n","lstm_sense is ok\n","WARNING:tensorflow:From <ipython-input-7-3215cdb692d4>:67: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-7-3215cdb692d4>:70: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-7-3215cdb692d4>:81: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","outputBD_Sense:  Tensor(\"rnn_sense/concat:0\", shape=(?, ?, 256), dtype=float32)\n","outputBD_Sense is ok \n","\n","flat_outputBD_Sense:  Tensor(\"attention_layer/Reshape:0\", shape=(?, ?), dtype=float32)\n","u_flat:  Tensor(\"attention_layer/MatMul:0\", shape=(?, 1), dtype=float32)\n","u:  Tensor(\"attention_layer/add:0\", shape=(?, ?), dtype=float32)\n","a:  Tensor(\"attention_layer/ExpandDims:0\", shape=(?, ?, 1), dtype=float32)\n","c:  Tensor(\"attention_layer/Sum:0\", shape=(?, 256), dtype=float32)\n","tiled_c:  Tensor(\"attention_layer/Tile:0\", shape=(?, ?, 256), dtype=float32)\n","attention_output:  Tensor(\"attention_layer/concat:0\", shape=(?, ?, 512), dtype=float32)\n","flat_attention_output:  Tensor(\"attention_layer/Reshape_2:0\", shape=(?, ?), dtype=float32)\n","global_attention is ok \n","\n","drop_flat_attention_output:  Tensor(\"hidden_layer/dropout/mul_1:0\", shape=(?, ?), dtype=float32)\n","hidden_layer_output:  Tensor(\"hidden_layer/add:0\", shape=(?, 256), dtype=float32)\n","hidden_layer is ok \n","\n","drop_hidden_layer_output:  Tensor(\"softmax_layer/dropout/mul_1:0\", shape=(?, 256), dtype=float32)\n","flat_sense_logits:  Tensor(\"softmax_layer/add:0\", shape=(?, 60919), dtype=float32)\n","sense_logits:  Tensor(\"softmax_layer/Reshape:0\", shape=(?, ?, 60919), dtype=float32)\n","float_mask_RawTosense_batch:  Tensor(\"softmax_layer/Cast:0\", shape=(?, ?, ?), dtype=float32)\n","final_sense_logits:  Tensor(\"softmax_layer/Mul:0\", shape=(?, ?, 60919), dtype=float32)\n","predictions:  Tensor(\"softmax_layer/ArgMax:0\", shape=(?, ?), dtype=int64)\n","softmax_layer is ok \n","\n","AdamOptimizer is ok .... \n","\n","tvars is ok ....\n","l2_loss is ok ....\n","total_loss is ok ....\n","grads is ok ....\n","train_op apply_gradients is ok ....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-8O16Z0uYOjR","colab_type":"code","outputId":"7ce04d76-44a8-4a22-f60b-d5bcf2e00107","executionInfo":{"status":"ok","timestamp":1568303476596,"user_tz":-120,"elapsed":8735,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["# Train and Dev split\n","# from sklearn.model_selection import train_test_split\n","# train_x_Raw, dev_x_Raw, train_y_BN, dev_y_BN, train_y_POS, dev_y_POS, train_y_allSenses_Sent, dev_y_allSenses_Sent, train_y_allSenses_Sent_id, dev_y_allSenses_Sent_id =\\\n","#     train_test_split(x_train_Raw_Sent_id, y_train_BNsense_Sent_id, y_train_POS_Sent_id, y_train_allSenses_Sent, y_train_allSenses_Sent_id, test_size=0.01, random_state=42)\n","\n","\n","# subset_train = 50 # Put number of subset data you want for train\n","# subset_dev= 50\n","# subset_test= 50\n","\n","\n","train_path =  root_path + '/semcor/semcor/'\n","dev_path = root_path + '/semcor/semeval2007/'\n","test_path = root_path + '/semcor/semeval2013/'\n","\n","x_train_Raw_Sent_id = pickle.load(open(train_path + 'x_Raw_Sent_id.pkl', \"rb\"))\n","y_train_BNsense_Sent_id = pickle.load(open(train_path + 'y_BNsense_Sent_id.pkl', \"rb\"))\n","y_train_allSenses_Sent = pickle.load(open(train_path + 'y_allSenses_Sent.pkl', \"rb\"))\n","y_train_allSenses_Sent_id = pickle.load(open(train_path + 'y_allSenses_Sent_id.pkl', \"rb\"))\n","\n","# x_train_Raw_Sent_id = x_train_Raw_Sent_id[slice(0, subset_train)]\n","# y_train_BNsense_Sent_id = y_train_BNsense_Sent_id[slice(0, subset_train)]  \n","# y_train_allSenses_Sent = y_train_allSenses_Sent[slice(0, subset_train)]\n","# y_train_allSenses_Sent_id = y_train_allSenses_Sent_id[slice(0, subset_train)]\n","\n","print(len(x_train_Raw_Sent_id))\n","print(len(x_train_Raw_Sent_id[0]))\n","print(len(y_train_BNsense_Sent_id[0]))\n","\n","print('train data is ok ......... ')\n","\n","x_dev_Raw_Sent = pickle.load(open(dev_path + 'x_Raw_Sent.pkl', \"rb\"))\n","x_dev_Raw_Sent_id = pickle.load(open(dev_path + 'x_Raw_Sent_id.pkl', \"rb\"))\n","y_dev_BNsense_Sent_id = pickle.load(open(dev_path + 'y_BNsense_Sent_id.pkl', \"rb\"))\n","y_dev_allSenses_Sent = pickle.load(open(dev_path + 'y_allSenses_Sent.pkl', \"rb\"))\n","y_dev_allSenses_Sent_id = pickle.load(open(dev_path + 'y_allSenses_Sent_id.pkl', \"rb\"))\n","\n","\n","# x_dev_Raw_Sent = x_dev_Raw_Sent[slice(0, subset_dev)] \n","# x_dev_Raw_Sent_id = x_dev_Raw_Sent_id[slice(0, subset_dev)]\n","# y_dev_BNsense_Sent_id = y_dev_BNsense_Sent_id[slice(0, subset_dev)]  \n","# y_dev_allSenses_Sent = y_dev_allSenses_Sent[slice(0, subset_dev)]\n","# y_dev_allSenses_Sent_id = y_dev_allSenses_Sent_id[slice(0, subset_dev)]\n","\n","print(y_dev_BNsense_Sent_id[0])\n","\n","print(y_dev_allSenses_Sent_id[0])\n","\n","print(len(x_dev_Raw_Sent_id))\n","print(len(x_dev_Raw_Sent_id[0]))\n","print(len(y_dev_BNsense_Sent_id[0]))\n","\n","print('dev data is ok ......... ')\n","\n","x_test_Raw_Sent = pickle.load(open(test_path + 'x_Raw_Sent.pkl', \"rb\"))\n","x_test_Raw_Sent_id = pickle.load(open(test_path + 'x_Raw_Sent_id.pkl', \"rb\"))\n","y_test_BNsense_Sent_id = pickle.load(open(test_path + 'y_BNsense_Sent_id.pkl', \"rb\"))\n","y_test_allSenses_Sent = pickle.load(open(test_path + 'y_allSenses_Sent.pkl', \"rb\"))\n","y_test_allSenses_Sent_id = pickle.load(open(test_path + 'y_allSenses_Sent_id.pkl', \"rb\"))\n","\n","\n","# x_test_Raw_Sent = x_test_Raw_Sent[slice(0, subset_test)]\n","# x_test_Raw_Sent_id = x_test_Raw_Sent_id[slice(0, subset_test)]\n","# y_test_BNsense_Sent_id = y_test_BNsense_Sent_id[slice(0, subset_test)]  \n","# y_test_allSenses_Sent = y_test_allSenses_Sent[slice(0, subset_test)]\n","# y_test_allSenses_Sent_id = y_test_allSenses_Sent_id[slice(0, subset_test)]\n","\n","print(len(x_test_Raw_Sent_id))\n","print(len(x_test_Raw_Sent_id[0]))\n","print(len(y_test_BNsense_Sent_id[0]))\n","\n","print('test data is ok ......... ')\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["37176\n","17\n","17\n","train data is ok ......... \n","[43, 8022, 3077, 6346, 29, 1, 10596, 0, 29, 980, 7, 4024, 38, 44, 5, 2464, 4, 44, 13840, 24, 11, 2005, 8, 1, 6554, 2371, 2051, 4, 1, 7675, 4, 1, 580, 2078, 2489, 3]\n","[[43], [8022], [3077], [6346], [29], [1], [10596], [0], [29], [980, 859, 5327, 5720, 3725, 4460, 6154], [7], [4024, 5498], [38], [44], [5], [2464], [4], [44], [13840], [24], [11], [827, 2005, 2645, 2821, 1267, 2904], [8], [1], [6554], [2371], [2051], [4], [1], [7675], [4], [1], [580], [2078], [2489], [3]]\n","135\n","36\n","36\n","dev data is ok ......... \n","306\n","7\n","7\n","test data is ok ......... \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KvFMmMd6mD5N","colab_type":"code","outputId":"17194f6f-d78d-48b6-d0e8-79f0fc6cce49","executionInfo":{"status":"ok","timestamp":1568303476598,"user_tz":-120,"elapsed":5741,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["SUBSET_MODEL_ADD = None #root_path+'/NLPHW3Basic_BDLSTM_models_48.081264108352144/.'\n","print(SUBSET_MODEL_ADD)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CC6suJoEwx_h","colab_type":"code","outputId":"e6f40cef-3229-4c80-bed2-fefaceef64eb","executionInfo":{"status":"ok","timestamp":1568303476967,"user_tz":-120,"elapsed":4920,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["\n","# -----------------------------------------------------------------------------\n","def WN_to_BN_dic(mapping_file):\n","    print('WN_to_BN_dic is started ....')\n","    BN_ID = []\n","    WN_ID = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            WN_ID.append(line_synsets[1])    \n","    WN2BN_map_dic = dict(zip(WN_ID,BN_ID))\n","    print('WN_to_BN_dic is done ....')\n","\n","    return WN2BN_map_dic\n","\n","\n","WN2BN_map_dic = WN_to_BN_dic(MAP_WN2BN_FILE)\n","print(len(WN2BN_map_dic))\n","\n","## ----------------------------------------------------------------------------\n","def getMFS_(word, WN2BN_map_dic):\n","    MFS_wnId = UNK\n","    all_synsets = wn.synsets(word)\n","\n","    # check if the word has synsets or not!\n","    if len(all_synsets) == 0 or all_synsets is None:\n","        return word\n","    \n","    synset = all_synsets[0]\n","    MFS_wnId = \"wn:\" + str(synset.offset()).zfill( 8) + synset.pos()\n","    if WN2BN_map_dic.get(MFS_wnId) is not None:\n","        MFS_bnId = WN2BN_map_dic.get(MFS_wnId)\n","        \n","    return MFS_bnId\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WN_to_BN_dic is started ....\n","WN_to_BN_dic is done ....\n","117659\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J8YND8UAl9Z_","colab_type":"code","outputId":"3b7d8047-e6c2-46a4-af4c-1607bdd0f6aa","executionInfo":{"status":"ok","timestamp":1568282606508,"user_tz":-120,"elapsed":7331906,"user":{"displayName":"manoochehr joodi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCckdAemjbs3FGT0OY3lnnbyrsKn7xrgPzBcrtSWQ=s64","userId":"09853341635658390863"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# Run tensorflow model\n","saver = tf.train.Saver()\n","\n","with tf.Session(config=tf.ConfigProto(log_device_placement = True)) as sess: \n","    # I couln't use tensorboard because it causes my colab session to crash for big datasets.\n","    #train_writer = tf.summary.FileWriter(root_path+'logging/tensorflow_model', sess.graph)\n","    \n","    if SUBSET_MODEL_ADD is not None:\n","        saver = tf.train.Saver()\n","        saver.restore(sess, tf.train.latest_checkpoint(SUBSET_MODEL_ADD))\n","        print('model restored')\n","    else:\n","        sess.run(tf.global_variables_initializer())             \n","        print('global variable initialized')\n","        \n","    print ('start session is ok ....')\n","        \n","    for epoch in range(NUM_EPOCHS):        \n","        #train\n","        train_loss=[]    \n","        for i in range(0, len(x_train_Raw_Sent_id), BATCH_SIZE):\n","            # slice dataset for padding\n","\n","            batch_x_Raw = x_train_Raw_Sent_id[slice(i, i + BATCH_SIZE)]\n","            batch_y_BN = y_train_BNsense_Sent_id[slice(i, i + BATCH_SIZE)]\n","\n","            \n","            y_train_allSenses_Sent_batch = y_train_allSenses_Sent[slice(i, i + BATCH_SIZE)]\n","            y_train_allSenses_Sent_id_batch = y_train_allSenses_Sent_id[slice(i, i + BATCH_SIZE)]\n","            mask_lemma2Sense_b = mask_lemmaToSenses_batch(batch_x_Raw, y_train_allSenses_Sent_batch, y_train_allSenses_Sent_id_batch, SenseWords2id)\n","            \n","#             print(batch_x_Raw)\n","#             print(y_train_allSenses_Sent_batch)\n","#             print(y_train_allSenses_Sent_id_batch)\n","            \n","            batch_x_Raw = padding3(batch_x_Raw, PAD_ID)\n","            batch_y_BN = padding(batch_y_BN, PAD_ID)\n","            \n","            # runnig training session ang getting train loss for every batch and append in epoch loss\n","            _, loss_other = sess.run(\n","                [train_op, total_loss], feed_dict = { input_ : batch_x_Raw, labels : batch_y_BN, mask_RawTosense_batch : mask_lemma2Sense_b, dropout_keep_prob : 0.3 })\n","            \n","            train_loss.append(loss_other)            \n","            \n","            if i % (BATCH_SIZE * 100) == 0:                \n","                print ('Train batch %d loss %f' % (i, loss_other))\n","                \n","        train_loss=np.mean(train_loss,dtype=float)                \n","        print ('Train Epoch %d loss %f' % (epoch+1, train_loss))        \n","        print('--------------------------------') \n","        \n","                                       \n","#         #Dev\n","        dev_loss=[]\n","        dev_pred=[]\n","        dev_hit_all=0\n","        dev_all_char=0        \n","        for i in range(0, len(x_dev_Raw_Sent_id), BATCH_SIZE):            \n","            \n","            batch_x_Lemma = x_dev_Raw_Sent[slice(i, i + BATCH_SIZE)]\n","            batch_x_Raw = x_dev_Raw_Sent_id[slice(i, i + BATCH_SIZE)]\n","            batch_y_BN = y_dev_BNsense_Sent_id[slice(i, i + BATCH_SIZE)]            \n","            \n","            y_dev_allSenses_Sent_batch = y_dev_allSenses_Sent[slice(i, i + BATCH_SIZE)]\n","            y_dev_allSenses_Sent_id_batch = y_dev_allSenses_Sent_id[slice(i, i + BATCH_SIZE)]\n","            \n","            mask_lemma2Sense_b = mask_lemmaToSenses_batch(batch_x_Raw, y_dev_allSenses_Sent_batch, y_dev_allSenses_Sent_id_batch, SenseWords2id)\n","                        \n","            batch_x_Raw = padding3(batch_x_Raw, PAD_ID)\n","            batch_y_BN = padding(batch_y_BN, PAD_ID)\n","            \n","            loss_other, lengths, predict = sess.run(\n","                [total_loss, seq_length, predictions], feed_dict = {input_ : batch_x_Raw, labels : batch_y_BN, mask_RawTosense_batch : mask_lemma2Sense_b, dropout_keep_prob: 1.0 })\n","            \n","            \n","            batch_true = y_dev_BNsense_Sent_id[slice(i, i + BATCH_SIZE)]\n","        \n","            pred_sense = []\n","            true_sense = []            \n","            for p in range(len(predict)):\n","                pr = []\n","                tr = []\n","                for L_ in range(lengths[p]):\n","                    # if our prediction is UNK means that word is not in our dictionary and we can't predict anything for that, so we put BackOff plan and MFS as our prediction.\n","                    if id2Sensewords[predict[p][L_]] != UNK:\n","                        pr.append(id2Sensewords[predict[p][L_]])\n","                    else:\n","                        pr.append(getMFS_(batch_x_Lemma[p][L_], WN2BN_map_dic))\n","#                         print(getMFS_(batch_x_Lemma[p][L_], WN2BN_map_dic))\n","                                                        \n","                    tr.append(id2Sensewords[batch_true[p][L_]])\n","                pred_sense.append(pr)\n","                true_sense.append(tr)\n","                        \n","            batch_pred = []\n","            for p in range(len(predict)):\n","#                 print(pred_sense[p])\n","#                 print(true_sense[p])\n","#                 print('\\n')\n","                dev_pred.append(predict[p][:lengths[p]])\n","                batch_pred.append(predict[p][:lengths[p]])\n","                \n","            dev_batch_hit, dev_batch_char = number_of_batch_sense_hits(batch_pred, batch_true)            \n","            print('dev batch %d loss %f dev_batch_hit:%d dev_batch_char:%d batch_accuracy:%f' % (i, loss_other, dev_batch_hit, dev_batch_char, (dev_batch_hit/dev_batch_char)*100 ))\n","            dev_loss.append(loss_other)            \n","            dev_hit_all+=dev_batch_hit\n","            dev_all_char+=dev_batch_char\n","                \n","        dev_loss=np.mean(dev_loss,dtype=float)\n","        dev_acc = (dev_hit_all/dev_all_char)*100\n","        print('Valid Epoch %d loss %f' % (epoch+1,dev_loss))        \n","        print('dev_hit_all:%d dev_all_char:%d accuracy:%f' % (dev_hit_all,dev_all_char,dev_acc ))\n","        print('--------------------------------')\n","\n","        \n","        save_path = saver.save(sess, root_path + '/Basic_BDLSTM_models_{}/model.ckpt'.format(dev_acc))\n","        print(\"Model saved in path: %s\" % save_path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["global variable initialized\n","start session is ok ....\n","Train batch 0 loss 11.153833\n","Train batch 800 loss 8.789354\n","Train batch 1600 loss 7.571229\n","Train batch 2400 loss 5.720890\n","Train batch 3200 loss 6.992844\n","Train batch 4000 loss 7.378502\n","Train batch 4800 loss 7.912602\n","Train batch 5600 loss 6.039218\n","Train batch 6400 loss 6.208889\n","Train batch 7200 loss 7.438411\n","Train batch 8000 loss 5.043982\n","Train batch 8800 loss 7.247985\n","Train batch 9600 loss 6.095309\n","Train batch 10400 loss 8.241877\n","Train batch 11200 loss 6.322417\n","Train batch 12000 loss 5.225909\n","Train batch 12800 loss 7.420945\n","Train batch 13600 loss 5.867131\n","Train batch 14400 loss 7.032659\n","Train batch 15200 loss 7.474792\n","Train batch 16000 loss 7.620069\n","Train batch 16800 loss 6.310368\n","Train batch 17600 loss 4.876607\n","Train batch 18400 loss 6.274178\n","Train batch 19200 loss 5.689607\n","Train batch 20000 loss 7.205137\n","Train batch 20800 loss 5.584973\n","Train batch 21600 loss 7.201625\n","Train batch 22400 loss 6.219693\n","Train batch 23200 loss 5.178972\n","Train batch 24000 loss 7.198496\n","Train batch 24800 loss 7.551856\n","Train batch 25600 loss 5.581665\n","Train batch 26400 loss 5.096469\n","Train batch 27200 loss 7.462277\n","Train batch 28000 loss 6.397300\n","Train batch 28800 loss 5.271924\n","Train batch 29600 loss 5.521148\n","Train batch 30400 loss 4.827362\n","Train batch 31200 loss 5.682389\n","Train batch 32000 loss 5.160929\n","Train batch 32800 loss 7.100719\n","Train batch 33600 loss 5.879434\n","Train batch 34400 loss 6.007315\n","Train batch 35200 loss 5.593669\n","Train batch 36000 loss 6.176559\n","Train batch 36800 loss 4.939549\n","Train Epoch 1 loss 6.557347\n","--------------------------------\n","dev batch 0 loss 5.708309 dev_batch_hit:13 dev_batch_char:21 batch_accuracy:61.904762\n","dev batch 8 loss 6.126370 dev_batch_hit:14 dev_batch_char:29 batch_accuracy:48.275862\n","dev batch 16 loss 7.831032 dev_batch_hit:9 dev_batch_char:14 batch_accuracy:64.285714\n","dev batch 24 loss 7.511140 dev_batch_hit:17 dev_batch_char:35 batch_accuracy:48.571429\n","dev batch 32 loss 5.740639 dev_batch_hit:12 dev_batch_char:32 batch_accuracy:37.500000\n","dev batch 40 loss 4.116693 dev_batch_hit:10 dev_batch_char:28 batch_accuracy:35.714286\n","dev batch 48 loss 7.422477 dev_batch_hit:14 dev_batch_char:28 batch_accuracy:50.000000\n","dev batch 56 loss 5.386740 dev_batch_hit:13 dev_batch_char:21 batch_accuracy:61.904762\n","dev batch 64 loss 6.610155 dev_batch_hit:19 dev_batch_char:38 batch_accuracy:50.000000\n","dev batch 72 loss 5.477280 dev_batch_hit:11 dev_batch_char:28 batch_accuracy:39.285714\n","dev batch 80 loss 7.573376 dev_batch_hit:6 dev_batch_char:25 batch_accuracy:24.000000\n","dev batch 88 loss 6.223481 dev_batch_hit:4 dev_batch_char:14 batch_accuracy:28.571429\n","dev batch 96 loss 7.196289 dev_batch_hit:11 dev_batch_char:23 batch_accuracy:47.826087\n","dev batch 104 loss 6.290099 dev_batch_hit:11 dev_batch_char:22 batch_accuracy:50.000000\n","dev batch 112 loss 5.788004 dev_batch_hit:11 dev_batch_char:28 batch_accuracy:39.285714\n","dev batch 120 loss 7.563808 dev_batch_hit:24 dev_batch_char:42 batch_accuracy:57.142857\n","dev batch 128 loss 6.223202 dev_batch_hit:9 dev_batch_char:15 batch_accuracy:60.000000\n","Valid Epoch 1 loss 6.399359\n","dev_hit_all:208 dev_all_char:443 accuracy:46.952596\n","--------------------------------\n","Model saved in path: /content/gdrive/My Drive/NLPHW3/Basic_BDLSTM_models_46.95259593679458/model.ckpt\n","Train batch 0 loss 6.695354\n","Train batch 800 loss 6.273889\n","Train batch 1600 loss 6.112489\n","Train batch 2400 loss 4.348802\n","Train batch 3200 loss 5.894414\n","Train batch 4000 loss 6.589446\n","Train batch 4800 loss 7.612557\n","Train batch 5600 loss 5.217189\n","Train batch 6400 loss 5.150040\n","Train batch 7200 loss 7.049816\n","Train batch 8000 loss 4.421553\n","Train batch 8800 loss 6.948839\n","Train batch 9600 loss 5.318984\n","Train batch 10400 loss 8.026742\n","Train batch 11200 loss 5.574924\n","Train batch 12000 loss 5.051558\n","Train batch 12800 loss 7.027009\n","Train batch 13600 loss 5.709673\n","Train batch 14400 loss 6.965310\n","Train batch 15200 loss 7.193721\n","Train batch 16000 loss 7.321107\n","Train batch 16800 loss 6.238534\n","Train batch 17600 loss 4.943977\n","Train batch 18400 loss 6.121453\n","Train batch 19200 loss 5.502461\n","Train batch 20000 loss 7.016821\n","Train batch 20800 loss 4.654404\n","Train batch 21600 loss 5.559393\n","Train batch 22400 loss 5.272605\n","Train batch 23200 loss 4.697045\n","Train batch 24000 loss 6.963417\n","Train batch 24800 loss 6.442816\n","Train batch 25600 loss 4.991508\n","Train batch 26400 loss 4.650098\n","Train batch 27200 loss 7.108886\n","Train batch 28000 loss 5.427224\n","Train batch 28800 loss 4.737040\n","Train batch 29600 loss 5.504343\n","Train batch 30400 loss 4.556818\n","Train batch 31200 loss 5.304347\n","Train batch 32000 loss 4.696807\n","Train batch 32800 loss 6.739042\n","Train batch 33600 loss 5.356456\n","Train batch 34400 loss 5.767500\n","Train batch 35200 loss 5.395826\n","Train batch 36000 loss 5.659734\n","Train batch 36800 loss 4.601408\n","Train Epoch 2 loss 5.912432\n","--------------------------------\n","dev batch 0 loss 5.519048 dev_batch_hit:12 dev_batch_char:21 batch_accuracy:57.142857\n","dev batch 8 loss 5.928327 dev_batch_hit:16 dev_batch_char:29 batch_accuracy:55.172414\n","dev batch 16 loss 7.710286 dev_batch_hit:9 dev_batch_char:14 batch_accuracy:64.285714\n","dev batch 24 loss 7.364464 dev_batch_hit:15 dev_batch_char:35 batch_accuracy:42.857143\n","dev batch 32 loss 5.594869 dev_batch_hit:14 dev_batch_char:32 batch_accuracy:43.750000\n","dev batch 40 loss 3.845687 dev_batch_hit:10 dev_batch_char:28 batch_accuracy:35.714286\n","dev batch 48 loss 7.293076 dev_batch_hit:15 dev_batch_char:28 batch_accuracy:53.571429\n","dev batch 56 loss 5.157700 dev_batch_hit:14 dev_batch_char:21 batch_accuracy:66.666667\n","dev batch 64 loss 6.434434 dev_batch_hit:18 dev_batch_char:38 batch_accuracy:47.368421\n","dev batch 72 loss 5.287992 dev_batch_hit:13 dev_batch_char:28 batch_accuracy:46.428571\n","dev batch 80 loss 7.446763 dev_batch_hit:9 dev_batch_char:25 batch_accuracy:36.000000\n","dev batch 88 loss 6.022820 dev_batch_hit:5 dev_batch_char:14 batch_accuracy:35.714286\n","dev batch 96 loss 6.922169 dev_batch_hit:11 dev_batch_char:23 batch_accuracy:47.826087\n","dev batch 104 loss 6.120896 dev_batch_hit:11 dev_batch_char:22 batch_accuracy:50.000000\n","dev batch 112 loss 5.629791 dev_batch_hit:14 dev_batch_char:28 batch_accuracy:50.000000\n","dev batch 120 loss 7.388516 dev_batch_hit:24 dev_batch_char:42 batch_accuracy:57.142857\n","dev batch 128 loss 6.115687 dev_batch_hit:9 dev_batch_char:15 batch_accuracy:60.000000\n","Valid Epoch 2 loss 6.222501\n","dev_hit_all:219 dev_all_char:443 accuracy:49.435666\n","--------------------------------\n","Model saved in path: /content/gdrive/My Drive/NLPHW3/Basic_BDLSTM_models_49.43566591422122/model.ckpt\n","Train batch 0 loss 6.498445\n","Train batch 800 loss 5.870154\n","Train batch 1600 loss 5.893860\n","Train batch 2400 loss 4.252025\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"isI4WumcU2ME","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}