{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"POS_BDLSTM_Tagger.ipynb","version":"0.3.2","provenance":[{"file_id":"1W6GHhtAv1zZ1Z2D6lLvC-uDSDWJD130O","timestamp":1567765261410}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ueQIwiNm42tG","colab_type":"code","outputId":"303b101a-9a49-4518-d9eb-38e2c617e70b","executionInfo":{"status":"ok","timestamp":1567550423515,"user_tz":-120,"elapsed":3945,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# !pip install tensorflow \n","# TensorFlow and tf.keras\n","# its just working with tensorflow 1.13.1, with others has problem\n","import tensorflow as tf\n","print(tf.__version__)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.14.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z_pjD2e15BFA","colab_type":"text"},"source":["**- goolge Colab Configuration**"]},{"cell_type":"code","metadata":{"id":"nZKvqNpW5HjM","colab_type":"code","outputId":"088abff8-4899-4c79-9d92-0a55da04ca68","executionInfo":{"status":"error","timestamp":1568297605799,"user_tz":-120,"elapsed":1805198,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":507}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f81873e75571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0;31m# Not already authorized, so do the authorization dance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendcontrol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m   \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'Stopped'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"UTR5Csn45f2I","colab_type":"text"},"source":["**- Setting root address to project in Google Drive**"]},{"cell_type":"code","metadata":{"id":"ywAPM0ns5dX-","colab_type":"code","outputId":"e4fc4e7b-59b5-40d2-cc21-ddf02f67173e","executionInfo":{"status":"ok","timestamp":1568275406961,"user_tz":-120,"elapsed":3153,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["!ls \"/content/gdrive/My Drive/NLPHW3\"\n","root_path = '/content/gdrive/My Drive/NLPHW3'"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" allWordsSensesembeddings.vec\t POS_LEX_BDLSTM_Tagger.ipynb\n"," BackUpCodes\t\t\t'POS_LEX_WNDOMAIN BDLSTM '\n","'Basic BDLSTM '\t\t\t POS_LEX_WnDomain_BDLSTM_Tagger.ipynb\n"," BASIC_BDLSTM_Tagger.ipynb\t'POS_WNDOMAIN BDLSTM'\n"," Data_Prepare_PKL_Format.ipynb\t POS_WnDomain_BDLSTM_Tagger.ipynb\n"," Evaluation_Datasets\t\t resources\n","'LEX BDLSTM'\t\t\t semcor\n"," LEX_BDLSTM_Tagger.ipynb\t semcor+omsti\n"," Mask_LSTM_Model1.ipynb\t\t Test_Prediction_Write.ipynb\n"," MFS_Tagger.ipynb\t\t Training_Corpora\n","'POS BDLSTM'\t\t\t'WNDOMAIN BDLSTM'\n"," POS_BDLSTM_Tagger.ipynb\t WnDomain_BDLSTM_Tagger.ipynb\n","'POS_LEX BDLSTM'\t\t WSD_Evaluation_Framework\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"waoW11y6yLmL","colab_type":"code","outputId":"6f4c788c-d8bb-4dbc-fd10-59f2972b559f","executionInfo":{"status":"ok","timestamp":1568275408396,"user_tz":-120,"elapsed":3746,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# Imports\n","from collections import Counter\n","import codecs\n","import xml.etree.cElementTree as etree\n","import pickle\n","import nltk\n","nltk.download('wordnet')\n","import numpy as np\n","import copy\n","from nltk.corpus import wordnet as wn\n","\n","\n","# Added Chars to dictionaries\n","UNK = \"<UNK>\"\n","PAD = \"<PAD>\"\n","\n","\n","RESOURCE_PATH = root_path+'/resources/'\n","\n","MAP_WN2BN_FILE = RESOURCE_PATH + 'babelnet2wordnet.tsv'\n","MAP_BN2WNDOMAIN_FILE = RESOURCE_PATH + 'babelnet2wndomains.tsv'\n","MAP_BN2LEXNAMES_FILE = RESOURCE_PATH + 'babelnet2lexnames.tsv'\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"epYctd54CoTK","colab_type":"code","outputId":"6c53f709-9e16-405f-b7f9-c56900871222","executionInfo":{"status":"ok","timestamp":1568275409304,"user_tz":-120,"elapsed":4202,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["all_dict_data = pickle.load(open(root_path+'/semcor/semcor/all_dict_data.pkl', \"rb\"))\n","\n","RawWords2id = all_dict_data[\"RawWords2id\"]\n","SenseWords2id = all_dict_data[\"SenseWords2id\"] \n","pos2id = all_dict_data[\"pos2id\"] \n","lex2id = all_dict_data[\"lex2id\"] \n","wnDomain2id = all_dict_data[\"wnDomain2id\"]\n","\n","print(len(pos2id))\n","print(len(SenseWords2id))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12\n","60919\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yIYezFrT5PUF","colab_type":"text"},"source":["**- python library imports**"]},{"cell_type":"code","metadata":{"id":"pT5cObkBpThO","colab_type":"code","colab":{}},"source":["# ===-----------------------------------------------------------------------===\n","# Trainig Section\n","# ===-----------------------------------------------------------------------===\n","\n","# function for add padding for train_y in every batch based on maximum length of sentence in that batch\n","# this method boost the performance of running.\n","def padding(X,padding_word):\n","\tmax_len = 0\n","\tfor x in X:\n","\t\tif len(x) > max_len:\n","\t\t\tmax_len = len(x)\n","\tpadded_X = np.ones((len(X), max_len), dtype=np.int32) * padding_word\n","\tfor i in range(len(X)):\n","\t\tfor j in range(len(X[i])):\n","\t\t\tpadded_X[i, j] = X[i][j]\n","\treturn padded_X\n","\n","\n","# padding function for train_x\n","def padding3(X,padding_word):\n","\tmax_len = 0\n","\tfor x in X:\n","\t\tif len(x) > max_len:\n","\t\t\tmax_len = len(x)\n","\tpadded_X = np.ones((len(X), max_len), dtype=np.int32) * padding_word\n","\tfor i in range(len(X)):\n","\t\tfor j in range(len(X[i])):\n","\t\t\tpadded_X[i, j] = X[i][j]\n","\treturn padded_X\n","\n","# function for calculating umber of hits and number of all predictions.\n","def number_of_batch_hits(y_batch_pred,y_batch_true):\n","    num_hits = 0\n","    num_all_chars = 0    \n","    for i in range(len(y_batch_true)):\n","        for j in range(len(y_batch_true[i])):\n","            num_all_chars = num_all_chars+1\n","            if y_batch_pred[i][j] == y_batch_true[i][j]:\n","                num_hits = num_hits+1    \n","    return num_hits, num_all_chars\n","\n","# function for calculating umber of hits and number of all predictions.\n","def number_of_batch_sense_hits(y_batch_pred,y_batch_true):\n","    num_hits = 0\n","    num_all_chars = 0    \n","    for i in range(len(y_batch_true)):\n","        for j in range(len(y_batch_true[i])):\n","            if id2Sensewords[y_batch_true[i][j]].startswith('bn'):\n","                num_all_chars = num_all_chars+1\n","                if y_batch_pred[i][j] == y_batch_true[i][j]:\n","                    num_hits = num_hits+1    \n","    return num_hits, num_all_chars\n","\n","# ----------------- Add Summary Function ------------------------------------------\n","def add_summary(writer, name, value, global_step):\n","    summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n","    writer.add_summary(summary, global_step=global_step)                                 \n","    \n","\n","## ----------------------------------------------------------------------------\n","def mask_lemmaToSenses_batch(x_Raw_Sent_batch, y_allSenses_Sent_batch, y_allSenses_Sent_id_batch, SenseWords2id):\n","     \n","    max_len = 0    \n","    for inp in x_Raw_Sent_batch:\n","        if len(inp) > max_len:\n","            max_len = len(inp)\n","            \n","    mask_lemma2sense_batch = [0]*np.ones(shape = (len(x_Raw_Sent_batch), max_len, len(SenseWords2id)))\n","            \n","    for i in range(len(x_Raw_Sent_batch)):\n","        for j in range(len(x_Raw_Sent_batch[i])):\n","            for k in range(len(y_allSenses_Sent_batch[i][j])):            \n","#                 if SenseWords2id.get(y_allSenses_Sent_batch[i][j][k]) is not None:\n","                mask_lemma2sense_batch[i][j][y_allSenses_Sent_id_batch[i][j][k]] = 1    \n","#                    mask_lemma2sense_batch[rawWords_batch2id[x_Raw_Sent_batch[i][j]]][y_allSenses_Sent_id_batch[i][j][k]] = 1    \n","#                 else:\n","#                     print('not in dictionary sense is : ', y_allSenses_Sent_batch[i][j][k])\n","            \n","    return mask_lemma2sense_batch    \n","\n","def reading_pretrained_sense_embeddings(filename, RawWords2id):\n","    # Reading Pretrained Embeddings from file\n","    pretrain_embeddigs = {}\n","    with codecs.open(filename, \"r\", \"utf-8\") as f:\n","        for line in f:\n","            pre_train = line.split()\n","            if len(pre_train) > 2:\n","                word = pre_train[0]\n","                if word in RawWords2id:\n","                    vec = pre_train[1:]\n","                    pretrain_embeddigs[word] = vec                \n","    \n","    print(\"pretrained embeddings files reading finished ...\")\n","    # making embeddings for all RawWords2id.\n","    embedding_dim = len(next(iter(pretrain_embeddigs.values())))\n","    out_of_vocab = 0\n","    out = np.ones((len(RawWords2id), embedding_dim))*0.001\n","    for word in RawWords2id.keys():\n","        if len(word) == 1:\n","            if word in pretrain_embeddigs.keys():        \n","                out[RawWords2id[word]]=np.array(pretrain_embeddigs[word])\n","            else:                \n","                out_of_vocab+=1\n","                np.random.uniform(-1.0, 1.0, embedding_dim)\n","        \n","    return out,out_of_vocab\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifW2h9YyD7oi","colab_type":"code","outputId":"b013a01b-c49e-430f-f9cb-e9481f5cd934","executionInfo":{"status":"ok","timestamp":1568275427610,"user_tz":-120,"elapsed":10092,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["\n","PAD_ID = RawWords2id[PAD]\n","id2Sensewords = dict(zip(SenseWords2id.values(), SenseWords2id.keys()))\n","\n","sense_embeddings, out_of_vocab = reading_pretrained_sense_embeddings(root_path + '/allWordsSensesembeddings.vec', RawWords2id)\n","print('sense_embeddings: ', len(sense_embeddings))\n","print('out_of_vocab: ', out_of_vocab)\n","\n","# we are using pretrained embedding - len(words2id)\n","VOCAB_SIZE =  len(RawWords2id)\n","WORD_EMBEDDING_DIM = 100\n","print('WORD_EMBEDDING_DIM: ', WORD_EMBEDDING_DIM)\n","print('VOCAB_SIZE: ', VOCAB_SIZE)\n","\n","# some Basic Hyper parameters\n","NUM_EPOCHS = 5\n","BATCH_SIZE = 4\n","HIDDEN_LAYER_DIM = 256\n","LEARNING_RATE = 1\n","NUM_CLASSES = len(SenseWords2id)\n","print('num sense classes: ', NUM_CLASSES)\n","L2_REGU_LAMBDA=0.0001\n","NUM_LAYERS = 2\n","CLIP=10\n","crf_lambda = 0.05\n","num_sense_pos = len(pos2id)\n","print('num_sense_pos: ', num_sense_pos)\n","num_sense_lex = len(lex2id)\n","print('num_sense_lex: ', num_sense_lex)\n","num_sense_wnDomain = len(wnDomain2id)\n","print('num_sense_wnDomain: ', num_sense_wnDomain)\n","\n","summaries = []\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["pretrained embeddings files reading finished ...\n","sense_embeddings:  32893\n","out_of_vocab:  5\n","WORD_EMBEDDING_DIM:  100\n","VOCAB_SIZE:  32893\n","num sense classes:  60919\n","num_sense_pos:  12\n","num_sense_lex:  46\n","num_sense_wnDomain:  160\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c7VncpqopT0M","colab_type":"code","colab":{}},"source":["\n","# --------------------- Tensorflow part ---------------------------------------------------\n","import tensorflow as tf\n","from tensorflow.contrib import layers\n","from tensorflow.contrib import crf\n","\n","def create_tensorflow_model(vocab_size, embedding_dim, hidden_layer_dim, sense_embed_bool = False, total_loss = 0):\n","    print(\"Creating TENSORFLOW model\")\n","     \n","    # Inputs have (batch_size, timesteps) shape.\n","    input_ = tf.placeholder(dtype = tf.int32, shape=[None, None], name='input_x')   \n","    # Labels have (batch_size,) shape.\n","    labels = tf.placeholder(dtype = tf.int32, shape=[None, None], name='labels_BN')\n","    \n","    y_pos = tf.placeholder(dtype = tf.int32, shape=[None, None], name='pos_tag')          \n","    \n","    # dropout_keep_prob is a scalar.\n","    dropout_keep_prob = tf.placeholder(dtype = tf.float32, name='dropout_keep_prob')\n","    \n","    seq_length = tf.reduce_sum(tf.cast( tf.not_equal(input_[:,:], tf.ones_like(input_[:,:] ) * PAD_ID ), tf.int32), 1)    \n","    print('seq_length: ', seq_length)\n","    \n","    max_sent_size = tf.size(input_[0,:])    \n","    print('max_sent_size: ', max_sent_size)\n","    \n","    batch_size = tf.size(input_[:,0])    \n","    print('batch_size: ', batch_size)\n","            \n","    mask_RawTosense_batch = tf.placeholder(dtype = tf.bool, shape=[None, None, None], name='mask_Raw2Senses')    \n","    print('mask_RawTosense_batch: ', mask_RawTosense_batch)\n","    \n","    mask_pos2sense = tf.placeholder(dtype = tf.bool, shape=[None, None], name='mask_Raw2Senses')    \n","    print('mask_pos2sense: ', mask_pos2sense)\n","    mask_matrix_pos = tf.tile(tf.expand_dims(mask_pos2sense, 0), [batch_size, 1, 1])     \n","    print('mask_matrix_pos: ', mask_matrix_pos)\n","            \n","    x_mask = tf.not_equal(input_[:,:], tf.ones_like(input_[:,:] ) * PAD_ID )    \n","    print('x_mask: ', x_mask)\n","    \n","    attention_mask = (tf.cast(x_mask, 'float') -1) * VERY_BIG_NUMBER     \n","    print('attention_mask: ', attention_mask)\n","\n","\n","\n","    # initialize weights randomly from a Gaussian distribution\n","    # step 1: create the initializer for weights\n","    weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n","        \n","\n","## ---------------------------- embedding Block --------------------------------------------\n","## -----------------------------------------------------------------------------------------  \n","    with tf.variable_scope('embeddings', reuse=tf.AUTO_REUSE):\n","        if sense_embed_bool:\n","            embedding_matrix = tf.Variable(sense_embeddings, dtype=tf.float32, name='embedding')\n","        else:\n","            embedding_matrix = tf.get_variable(\"embeddings\", shape=[vocab_size, embedding_dim])\n","        \n","        embeddings = tf.nn.embedding_lookup(embedding_matrix, input_)        \n","        print('embeddings: ', embeddings)\n","        \n","        embeddings = tf.reshape(embeddings,[batch_size, max_sent_size, embedding_dim]) #         \n","        print('embeddings: ', embeddings)\n","\n","        embeddings=tf.nn.dropout(tf.cast(embeddings, tf.float32), dropout_keep_prob) #     embeddings shape (batch size, sentence length with padding, 100) \n","\n","        print ('embeddings is ok \\n')\n","                \n","    \n","## --------------------------- POS Block ---------------------------------------------------\n","## -----------------------------------------------------------------------------------------  \n","    with tf.variable_scope('rnn_cell_pos', reuse=tf.AUTO_REUSE):\n","            print ('lstm_cell_pos is is ok ... ')\n","            def lstm_cell1():\n","                return tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_layer_dim), output_keep_prob=dropout_keep_prob)\n","\n","            stacked_fw_lstm_pos = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell1() for _ in range(NUM_LAYERS)])\n","\n","            stacked_bw_lstm_pos = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell1() for _ in range(NUM_LAYERS)])\n","\n","    with tf.variable_scope('rnn_pos', reuse=tf.AUTO_REUSE):                        \n","        (forward_output_pos, backword_output_pos), _ = tf.nn.bidirectional_dynamic_rnn(\n","            cell_fw = stacked_fw_lstm_pos,\n","            cell_bw = stacked_bw_lstm_pos,\n","            inputs = embeddings,\n","            sequence_length = seq_length,\n","            dtype=tf.float32\n","        )                        \n","        outputBD_pos = tf.concat([forward_output_pos, backword_output_pos], axis=2)\n","        print('outputBD_pos: ', outputBD_pos) # shape is batch*max_len*(2*hidden_layer_size)                         \n","\n","        print ('outputBD_pos is ok \\n')\n","\n","    with tf.variable_scope(\"softmax_layer_pos\", reuse=tf.AUTO_REUSE):\n","        W_pos = tf.get_variable(\"W_pos\", shape=[2*hidden_layer_dim, num_sense_pos], initializer = weight_initer)\n","        b_pos = tf.get_variable(\"b_pos\", shape=[num_sense_pos], initializer=tf.zeros_initializer())\n","\n","        flat_softmax_pos = tf.reshape(outputBD_pos, [-1, tf.shape(outputBD_pos)[2]]) # shape is (batch*max_len)*(2*hidden_layer_size)\n","        print('flat_softmax_pos: ', flat_softmax_pos)\n","\n","        drop_flat_softmax_pos = tf.nn.dropout(flat_softmax_pos, dropout_keep_prob) # shape is (batch*max_len)*(2*hidden_layer_size)\n","        print('drop_flat_softmax_pos: ', drop_flat_softmax_pos)\n","\n","        flat_pos_logits = tf.matmul(drop_flat_softmax_pos, W_pos) + b_pos # shape is (batch*max_len)*num_sense_pos\n","        print('flat_pos_logits: ', flat_pos_logits)\n","\n","        pos_logits = tf.reshape(flat_pos_logits, [batch_size, max_sent_size, num_sense_pos]) # shape is batch*max_len*num_sense_pos ->3D\n","        print('pos_logits: ', pos_logits)\n","\n","        predict_pos = tf.argmax(pos_logits , axis=2)\n","\n","        loss_pos = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= y_pos, logits= pos_logits))\n","\n","        total_loss = total_loss + loss_pos\n","\n","        float_mask_matrix_pos = tf.cast(mask_matrix_pos, dtype=tf.float32)\n","        print('float_mask_matrix_pos: ', float_mask_matrix_pos)\n","\n","        pos_masked_senses = tf.matmul(pos_logits, float_mask_matrix_pos) #mask_mat = 2*11*56 or batch*pos_num*NUM_CLASSES\n","        print('pos_masked_senses: ', pos_masked_senses)  # size is: batch*max_len*NUM_CLASSES\n","\n","        print ('softmax_layer_pos is ok \\n')\n","\n","        input_rnn_sense = outputBD_pos\n","            \n","            \n","## --------------------------- SENSE Block ------------------------------------------------   \n","## ----------------------------------------------------------------------------------------   \n","    with tf.variable_scope('rnn_cell_sense', reuse=tf.AUTO_REUSE):\n","            print ('lstm_sense is ok')\n","            def lstm_cell():\n","                return tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_layer_dim), output_keep_prob=dropout_keep_prob)\n","        \n","            stacked_fw_lstm_Sense = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell() for _ in range(NUM_LAYERS)])\n","            \n","            stacked_bw_lstm_Sense = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell() for _ in range(NUM_LAYERS)])\n","            \n","    with tf.variable_scope('rnn_sense', reuse=tf.AUTO_REUSE):                        \n","            (forward_output_Sense, backword_output_Sense), _ = tf.nn.bidirectional_dynamic_rnn(\n","                cell_fw = stacked_fw_lstm_Sense,\n","                cell_bw = stacked_bw_lstm_Sense,\n","                inputs = input_rnn_sense,\n","                sequence_length = seq_length,\n","                dtype=tf.float32\n","            )\n","            \n","            outputBD_Sense = tf.concat([forward_output_Sense, backword_output_Sense], axis=2)            \n","            print('outputBD_Sense: ', outputBD_Sense) # shape is batch*max_len*(2*hidden_layer_size) \n","            \n","            print ('outputBD_Sense is ok \\n')   \n","\n","            \n","## --------------------------- attention_layer --------------------------------------------------\n","    with tf.variable_scope(\"attention_layer\", reuse=tf.AUTO_REUSE):\n","        W_attention_L = tf.get_variable(\"W_attention_L\", shape=[2*hidden_layer_dim, 1], initializer = weight_initer )\n","        flat_outputBD_Sense = tf.reshape(outputBD_Sense, [batch_size*max_sent_size, tf.shape(outputBD_Sense)[2]])        \n","        print('flat_outputBD_Sense: ', flat_outputBD_Sense) # shape is  shape is (batch*max_length*(2*hidden_layer_size)).(51(2*hidden_layer_size)2*1) = (batch*max_length)*1 ->2D\n","        \n","        flat_outputBD_Sense_tanh = tf.tanh(flat_outputBD_Sense)\n","        \n","        u_flat_outputBD_Sense_tanh = tf.matmul(flat_outputBD_Sense_tanh, W_attention_L) # shape is batch*max_length -> 2D\n","        print('u_flat: ', u_flat_outputBD_Sense_tanh)\n","        \n","        u = tf.reshape(u_flat_outputBD_Sense_tanh, [batch_size, max_sent_size]) + attention_mask #\n","        print('u: ', u)\n","        \n","        u_softmax = tf.nn.softmax(u, 1)\n","        \n","        a = tf.expand_dims(u_softmax, 2) # shape is expand to batch*max_len*1 -> 3D\n","        print('a: ', a)\n","        \n","        c = tf.reduce_sum(tf.multiply(outputBD_Sense, a), axis=1) # shape is batch*max_len*(2*hidden_layer_size) and then reduce_sum with axis 1 to: batch*(2*hidden_layer_size)\n","        print('c: ', c)\n","        \n","        tiled_c = tf.tile(tf.expand_dims(c, 1), [1, max_sent_size, 1]) # shape is batch*max_len*(2*hidden_layer_size)\n","        print('tiled_c: ', tiled_c)\n","        \n","        attention_output = tf.concat([tiled_c, outputBD_Sense], 2) # batch*max_len*(2*hidden_layer_size) \"concat with\" batch*max_len*(2*hidden_layer_size) -> batch*max_len*(4*hidden_layer_size)\n","        print('attention_output: ', attention_output)\n","        \n","        flat_attention_output = tf.reshape(attention_output, [batch_size*max_sent_size, tf.shape(attention_output)[2]]) # reshape to (batch*max_len)*(4*hidden_layer_size) -> 2D\n","        print('flat_attention_output: ', flat_attention_output)\n","\n","        print ('global_attention is ok \\n') \n","        \n","## --------------------------- hidden_layer Sense ------------------------------------------------------------\n","    with tf.variable_scope(\"hidden_layer\", reuse=tf.AUTO_REUSE):\n","        W = tf.get_variable(\"W\", shape=[4*hidden_layer_dim, 2*hidden_layer_dim], initializer = weight_initer)\n","        b = tf.get_variable(\"b\", shape=[2*hidden_layer_dim], initializer = tf.zeros_initializer())\n","        \n","        drop_flat_attention_output = tf.nn.dropout(flat_attention_output, dropout_keep_prob) # shape is (batch*max_len)*(4*hidden_layer_size) -> 2D\n","        print('drop_flat_attention_output: ', drop_flat_attention_output)\n","        \n","        hidden_layer_output = tf.matmul(drop_flat_attention_output, W) + b  # shape is (batch*max_len)*(2*hidden_layer_size) -> 2D\n","        print('hidden_layer_output: ', hidden_layer_output)\n","                \n","        print ('hidden_layer is ok \\n') \n","        \n","## --------------------------- softmax_layer Sense ------------------------------------------------------------\n","    with tf.variable_scope(\"softmax_layer\", reuse=tf.AUTO_REUSE):\n","        W_sense = tf.get_variable(\"W_sense\", shape=[2*hidden_layer_dim, NUM_CLASSES], initializer = weight_initer)\n","        b_sense = tf.get_variable(\"b_sense\", shape=[NUM_CLASSES], initializer=tf.zeros_initializer())\n","        \n","        drop_hidden_layer_output = tf.nn.dropout(hidden_layer_output, dropout_keep_prob)\n","        print('drop_hidden_layer_output: ', drop_hidden_layer_output)\n","        \n","        flat_sense_logits = tf.matmul(drop_hidden_layer_output, W_sense) + b_sense\n","        print('flat_sense_logits: ', flat_sense_logits)\n","        \n","        sense_logits = tf.reshape(flat_sense_logits, [batch_size, max_sent_size, NUM_CLASSES])\n","        print('sense_logits: ', sense_logits)\n","        \n","        masked_sense_logits = tf.multiply(sense_logits, pos_masked_senses) # multiply is element wise multiplicatin, matmul is matrix multiplication.\n","        print('masked_sense_logits: ', masked_sense_logits)  \n","            \n","        float_mask_RawTosense_batch = tf.cast(mask_RawTosense_batch, dtype=tf.float32)\n","        print('float_mask_RawTosense_batch: ', float_mask_RawTosense_batch)\n","        \n","        final_sense_logits = tf.multiply(masked_sense_logits, float_mask_RawTosense_batch)\n","        print('final_sense_logits: ', final_sense_logits)\n","\n","        predictions = tf.argmax(final_sense_logits , axis=2)\n","        print('predictions: ', predictions)\n","        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= labels, logits= final_sense_logits))\n","        total_loss = total_loss + loss\n","        print ('softmax_layer is ok \\n')\n","                \n","                \n","## --------------------------- train_op Block ----------------------------------------------\n","## -----------------------------------------------------------------------------------------          \n","    with tf.variable_scope('train_op', reuse=tf.AUTO_REUSE):\n","                        \n","        optimizer = tf.train.AdadeltaOptimizer(LEARNING_RATE)\n","        \n","#         optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n","#         optimizer=tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE, use_locking=False, name='GradientDescent')\n","        print ('AdamOptimizer is ok .... \\n')\n","        \n","        tvars=tf.trainable_variables()\n","        print ('tvars is ok ....')\n","    \n","        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tvars if v.get_shape().ndims > 1])\n","        print ('l2_loss is ok ....')\n","        \n","        total_loss = total_loss + L2_REGU_LAMBDA*l2_loss             \n","        print ('total_loss is ok ....')\n","        \n","        summaries.append(tf.summary.scalar(\"loss\", loss))\n","        summaries.append(tf.summary.scalar(\"total_loss\", total_loss))\n","        \n","        grads,_ = tf.clip_by_global_norm(tf.gradients(total_loss,tvars),CLIP)\n","        print ('grads is ok ....')\n","        \n","        train_op = optimizer.apply_gradients(zip(grads,tvars))\n","        print ('train_op apply_gradients is ok ....')\n","                               \n","    return input_, labels, y_pos, mask_RawTosense_batch, train_op, predictions, dropout_keep_prob, total_loss, seq_length, mask_pos2sense\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjH5I--oEvI0","colab_type":"code","outputId":"f7b6ec7a-74cc-490a-bf85-c21891dc2cc4","executionInfo":{"status":"ok","timestamp":1568275450324,"user_tz":-120,"elapsed":5705,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# create tensorflow model without sense embedding and just basic LSTM\n","input_, labels, y_pos, mask_RawTosense_batch, train_op, predictions, dropout_keep_prob, total_loss, seq_length, mask_pos2sense \\\n","    = create_tensorflow_model(VOCAB_SIZE, WORD_EMBEDDING_DIM, HIDDEN_LAYER_DIM, sense_embed_bool = True, total_loss = 0)\n","\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Creating TENSORFLOW model\n","seq_length:  Tensor(\"Sum:0\", shape=(?,), dtype=int32)\n","max_sent_size:  Tensor(\"Size:0\", shape=(), dtype=int32)\n","batch_size:  Tensor(\"Size_1:0\", shape=(), dtype=int32)\n","mask_RawTosense_batch:  Tensor(\"mask_Raw2Senses:0\", shape=(?, ?, ?), dtype=bool)\n","mask_pos2sense:  Tensor(\"mask_Raw2Senses_1:0\", shape=(?, ?), dtype=bool)\n","mask_matrix_pos:  Tensor(\"Tile:0\", shape=(?, ?, ?), dtype=bool)\n","x_mask:  Tensor(\"NotEqual_1:0\", shape=(?, ?), dtype=bool)\n","attention_mask:  Tensor(\"mul_2:0\", shape=(?, ?), dtype=float32)\n","embeddings:  Tensor(\"embeddings/embedding_lookup/Identity:0\", shape=(?, ?, 100), dtype=float32)\n","embeddings:  Tensor(\"embeddings/Reshape:0\", shape=(?, ?, 100), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-7-9cd0815ce66d>:62: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","embeddings is ok \n","\n","lstm_cell_pos is is ok ... \n","WARNING:tensorflow:From <ipython-input-7-9cd0815ce66d>:72: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-7-9cd0815ce66d>:75: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-7-9cd0815ce66d>:86: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","outputBD_pos:  Tensor(\"rnn_pos/concat:0\", shape=(?, ?, 512), dtype=float32)\n","outputBD_pos is ok \n","\n","flat_softmax_pos:  Tensor(\"softmax_layer_pos/Reshape:0\", shape=(?, ?), dtype=float32)\n","drop_flat_softmax_pos:  Tensor(\"softmax_layer_pos/dropout/mul_1:0\", shape=(?, ?), dtype=float32)\n","flat_pos_logits:  Tensor(\"softmax_layer_pos/add:0\", shape=(?, 12), dtype=float32)\n","pos_logits:  Tensor(\"softmax_layer_pos/Reshape_1:0\", shape=(?, ?, 12), dtype=float32)\n","float_mask_matrix_pos:  Tensor(\"softmax_layer_pos/Cast:0\", shape=(?, ?, ?), dtype=float32)\n","pos_masked_senses:  Tensor(\"softmax_layer_pos/MatMul_1:0\", shape=(?, ?, ?), dtype=float32)\n","softmax_layer_pos is ok \n","\n","lstm_sense is ok\n","outputBD_Sense:  Tensor(\"rnn_sense/concat:0\", shape=(?, ?, 512), dtype=float32)\n","outputBD_Sense is ok \n","\n","flat_outputBD_Sense:  Tensor(\"attention_layer/Reshape:0\", shape=(?, ?), dtype=float32)\n","u_flat:  Tensor(\"attention_layer/MatMul:0\", shape=(?, 1), dtype=float32)\n","u:  Tensor(\"attention_layer/add:0\", shape=(?, ?), dtype=float32)\n","a:  Tensor(\"attention_layer/ExpandDims:0\", shape=(?, ?, 1), dtype=float32)\n","c:  Tensor(\"attention_layer/Sum:0\", shape=(?, 512), dtype=float32)\n","tiled_c:  Tensor(\"attention_layer/Tile:0\", shape=(?, ?, 512), dtype=float32)\n","attention_output:  Tensor(\"attention_layer/concat:0\", shape=(?, ?, 1024), dtype=float32)\n","flat_attention_output:  Tensor(\"attention_layer/Reshape_2:0\", shape=(?, ?), dtype=float32)\n","global_attention is ok \n","\n","drop_flat_attention_output:  Tensor(\"hidden_layer/dropout/mul_1:0\", shape=(?, ?), dtype=float32)\n","hidden_layer_output:  Tensor(\"hidden_layer/add:0\", shape=(?, 512), dtype=float32)\n","hidden_layer is ok \n","\n","drop_hidden_layer_output:  Tensor(\"softmax_layer/dropout/mul_1:0\", shape=(?, 512), dtype=float32)\n","flat_sense_logits:  Tensor(\"softmax_layer/add:0\", shape=(?, 60919), dtype=float32)\n","sense_logits:  Tensor(\"softmax_layer/Reshape:0\", shape=(?, ?, 60919), dtype=float32)\n","masked_sense_logits:  Tensor(\"softmax_layer/Mul:0\", shape=(?, ?, 60919), dtype=float32)\n","float_mask_RawTosense_batch:  Tensor(\"softmax_layer/Cast:0\", shape=(?, ?, ?), dtype=float32)\n","final_sense_logits:  Tensor(\"softmax_layer/Mul_1:0\", shape=(?, ?, 60919), dtype=float32)\n","predictions:  Tensor(\"softmax_layer/ArgMax:0\", shape=(?, ?), dtype=int64)\n","softmax_layer is ok \n","\n","AdamOptimizer is ok .... \n","\n","tvars is ok ....\n","l2_loss is ok ....\n","total_loss is ok ....\n","grads is ok ....\n","train_op apply_gradients is ok ....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-8O16Z0uYOjR","colab_type":"code","outputId":"ef6efe66-2f60-47ca-a18f-e0188da6c0dc","executionInfo":{"status":"ok","timestamp":1568275464724,"user_tz":-120,"elapsed":15227,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["# Train and Dev split\n","# from sklearn.model_selection import train_test_split\n","# train_x_Raw, dev_x_Raw, train_y_BN, dev_y_BN, train_y_POS, dev_y_POS, train_y_allSenses_Sent, dev_y_allSenses_Sent, train_y_allSenses_Sent_id, dev_y_allSenses_Sent_id =\\\n","#     train_test_split(x_train_Raw_Sent_id, y_train_BNsense_Sent_id, y_train_POS_Sent_id, y_train_allSenses_Sent, y_train_allSenses_Sent_id, test_size=0.01, random_state=42)\n","\n","\n","# subset_train = 100 # Put number of subset data you want for train\n","# subset_dev= 50\n","# subset_test= 50\n","\n","\n","train_path =  root_path + '/semcor/semcor/'\n","dev_path = root_path + '/semcor/semeval2007/'\n","test_path = root_path + '/semcor/semeval2013/'\n","\n","\n","x_train_Raw_Sent_id = pickle.load(open(train_path + 'x_Raw_Sent_id.pkl', \"rb\"))\n","y_train_BNsense_Sent_id = pickle.load(open(train_path + 'y_BNsense_Sent_id.pkl', \"rb\"))\n","y_train_POS_Sent_id = pickle.load(open(train_path + 'y_POS_Sent_id.pkl', \"rb\"))\n","y_train_allSenses_Sent = pickle.load(open(train_path + 'y_allSenses_Sent.pkl', \"rb\"))\n","y_train_allSenses_Sent_id = pickle.load(open(train_path + 'y_allSenses_Sent_id.pkl', \"rb\"))\n","\n","mask_train_pos2sense = pickle.load(open(train_path + 'mask_pos2sense.pkl', \"rb\"))\n","\n","# x_train_Raw_Sent_id = x_train_Raw_Sent_id[slice(0, subset_train)]\n","# y_train_BNsense_Sent_id = y_train_BNsense_Sent_id[slice(0, subset_train)]  \n","# y_train_POS_Sent_id = y_train_POS_Sent_id[slice(0, subset_train)]\n","# y_train_allSenses_Sent = y_train_allSenses_Sent[slice(0, subset_train)]\n","# y_train_allSenses_Sent_id = y_train_allSenses_Sent_id[slice(0, subset_train)]\n","\n","print(len(x_train_Raw_Sent_id))\n","print(len(x_train_Raw_Sent_id[0]))\n","print(len(y_train_BNsense_Sent_id[0]))\n","\n","print('train data is ok ......... ')\n","\n","x_dev_IdInstance_Sent = pickle.load(open(dev_path + 'x_IdInstance_Sent.pkl', \"rb\"))\n","x_dev_Raw_Sent = pickle.load(open(dev_path + 'x_Raw_Sent.pkl', \"rb\"))\n","x_dev_Raw_Sent_id = pickle.load(open(dev_path + 'x_Raw_Sent_id.pkl', \"rb\"))\n","y_dev_BNsense_Sent_id = pickle.load(open(dev_path + 'y_BNsense_Sent_id.pkl', \"rb\"))\n","y_dev_POS_Sent_id = pickle.load(open(dev_path + 'y_POS_Sent_id.pkl', \"rb\"))\n","y_dev_allSenses_Sent = pickle.load(open(dev_path + 'y_allSenses_Sent.pkl', \"rb\"))\n","y_dev_allSenses_Sent_id = pickle.load(open(dev_path + 'y_allSenses_Sent_id.pkl', \"rb\"))\n","\n","mask_dev_pos2sense = pickle.load(open(dev_path + 'mask_pos2sense.pkl', \"rb\"))\n"," \n","# x_dev_Raw_Sent_id = x_dev_Raw_Sent_id[slice(0, subset_dev)]\n","# y_dev_BNsense_Sent_id = y_dev_BNsense_Sent_id[slice(0, subset_dev)]  \n","# y_dev_POS_Sent_id = y_dev_POS_Sent_id[slice(0, subset_dev)]\n","# y_dev_allSenses_Sent = y_dev_allSenses_Sent[slice(0, subset_dev)]\n","# y_dev_allSenses_Sent_id = y_dev_allSenses_Sent_id[slice(0, subset_dev)]\n","\n","print(len(x_dev_Raw_Sent_id))\n","print(len(x_dev_Raw_Sent_id[0]))\n","print(len(y_dev_BNsense_Sent_id[0]))\n","\n","print('dev data is ok ......... ')\n","\n","x_test_Raw_Sent = pickle.load(open(test_path + 'x_Raw_Sent.pkl', \"rb\"))\n","x_test_Raw_Sent_id = pickle.load(open(test_path + 'x_Raw_Sent_id.pkl', \"rb\"))\n","y_test_BNsense_Sent_id = pickle.load(open(test_path + 'y_BNsense_Sent_id.pkl', \"rb\"))\n","y_test_POS_Sent_id = pickle.load(open(test_path + 'y_POS_Sent_id.pkl', \"rb\"))\n","y_test_allSenses_Sent = pickle.load(open(test_path + 'y_allSenses_Sent.pkl', \"rb\"))\n","y_test_allSenses_Sent_id = pickle.load(open(test_path + 'y_allSenses_Sent_id.pkl', \"rb\"))\n","\n","mask_test_lex2sense = pickle.load(open(test_path + 'mask_lex2sense.pkl', \"rb\"))\n","mask_test_wnDomain2sense = pickle.load(open(test_path + 'mask_wnDomain2sense.pkl', \"rb\"))\n","mask_test_pos2sense = pickle.load(open(test_path + 'mask_pos2sense.pkl', \"rb\"))\n"," \n","    \n","# x_test_Raw_Sent_id = x_test_Raw_Sent_id[slice(0, subset_test)]\n","# y_test_BNsense_Sent_id = y_test_BNsense_Sent_id[slice(0, subset_test)]  \n","# y_test_POS_Sent_id = y_test_POS_Sent_id[slice(0, subset_test)]\n","# y_test_allSenses_Sent = y_test_allSenses_Sent[slice(0, subset_test)]\n","# y_test_allSenses_Sent_id = y_test_allSenses_Sent_id[slice(0, subset_test)]\n","\n","print(len(x_test_Raw_Sent_id))\n","print(len(x_test_Raw_Sent_id[0]))\n","print(len(y_test_BNsense_Sent_id[0]))\n","\n","print('test data is ok ......... ')\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["37176\n","17\n","17\n","train data is ok ......... \n","135\n","36\n","36\n","dev data is ok ......... \n","306\n","7\n","7\n","test data is ok ......... \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n5Cy1h_gs4Mu","colab_type":"code","outputId":"e25086aa-beeb-4b8e-e51b-b835a0ed5c6f","executionInfo":{"status":"ok","timestamp":1568275465397,"user_tz":-120,"elapsed":13157,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["\n","# -----------------------------------------------------------------------------\n","def WN_to_BN_dic(mapping_file):\n","    print('WN_to_BN_dic is started ....')\n","    BN_ID = []\n","    WN_ID = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            WN_ID.append(line_synsets[1])    \n","    WN2BN_map_dic = dict(zip(WN_ID,BN_ID))\n","    print('WN_to_BN_dic is done ....')\n","\n","    return WN2BN_map_dic\n","\n","\n","WN2BN_map_dic = WN_to_BN_dic(MAP_WN2BN_FILE)\n","print(len(WN2BN_map_dic))\n","\n","## ----------------------------------------------------------------------------\n","def getMFS_(word, WN2BN_map_dic):\n","#     print('no prediction for: ', word)\n","    all_synsets = wn.synsets(word)\n","\n","    # check if the word has synsets or not!\n","    if len(all_synsets) == 0 or all_synsets is None:\n","        return word\n","    \n","    synset = all_synsets[0]\n","    MFS_wnId = \"wn:\" + str(synset.offset()).zfill( 8) + synset.pos()\n","    if WN2BN_map_dic.get(MFS_wnId) is not None:\n","        MFS_bnId = WN2BN_map_dic.get(MFS_wnId)\n","        \n","    return MFS_bnId\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WN_to_BN_dic is started ....\n","WN_to_BN_dic is done ....\n","117659\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KvFMmMd6mD5N","colab_type":"code","outputId":"1824438d-5424-47dc-a44e-5c47f5029e23","executionInfo":{"status":"ok","timestamp":1568275465400,"user_tz":-120,"elapsed":11380,"user":{"displayName":"Manoochehr Joodi","photoUrl":"","userId":"13338330927752009719"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["SUBSET_MODEL_ADD = None #root_path+'best_model_path98.12481353909577/.'\n","print(SUBSET_MODEL_ADD)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J8YND8UAl9Z_","colab_type":"code","outputId":"48db425f-62ca-4ebf-ec17-1cef27280083","executionInfo":{"status":"ok","timestamp":1568250224071,"user_tz":-120,"elapsed":6934073,"user":{"displayName":"amir sarrafzadeh arasi","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCoJfa2IRcvc03rDx8Vu9xlX7ULGKLnJTtvLJKm=s64","userId":"05903807322000797312"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["# Run tensorflow model\n","saver = tf.train.Saver()\n","\n","with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: \n","    # I couln't use tensorboard because it causes my colab session to crash for big datasets.\n","    #train_writer = tf.summary.FileWriter(root_path+'logging/tensorflow_model', sess.graph)\n","    \n","    if SUBSET_MODEL_ADD is not None:\n","        saver = tf.train.Saver()\n","        saver.restore(sess, tf.train.latest_checkpoint(SUBSET_MODEL_ADD))\n","        print('model restored')\n","    else:\n","        sess.run(tf.global_variables_initializer())             \n","        print('global variable initialized')\n","        \n","    print ('start session is ok ....')\n","        \n","    for epoch in range(NUM_EPOCHS):        \n","        #train\n","        train_loss=[]    \n","        for i in range(0, len(x_train_Raw_Sent_id), BATCH_SIZE):\n","            # slice dataset for padding\n","\n","            batch_x_Raw = x_train_Raw_Sent_id[slice(i, i + BATCH_SIZE)]\n","            batch_y_BN = y_train_BNsense_Sent_id[slice(i, i + BATCH_SIZE)]\n","            batch_y_POS = y_train_POS_Sent_id[slice(i, i + BATCH_SIZE)]\n","            \n","            \n","            y_train_allSenses_Sent_batch = y_train_allSenses_Sent[slice(i, i + BATCH_SIZE)]\n","            y_train_allSenses_Sent_id_batch = y_train_allSenses_Sent_id[slice(i, i + BATCH_SIZE)]\n","            mask_lemma2Sense_b = mask_lemmaToSenses_batch(batch_x_Raw, y_train_allSenses_Sent_batch, y_train_allSenses_Sent_id_batch, SenseWords2id)\n","            \n","            \n","            batch_x_Raw = padding3(batch_x_Raw, PAD_ID)\n","            batch_y_BN = padding(batch_y_BN, PAD_ID)\n","            batch_y_POS = padding(batch_y_POS, PAD_ID)\n","            \n","            # runnig training session ang getting train loss for every batch and append in epoch loss\n","            _, loss_other = sess.run(\n","                [train_op, total_loss], feed_dict = { input_ : batch_x_Raw, labels : batch_y_BN, y_pos : batch_y_POS, mask_RawTosense_batch : mask_lemma2Sense_b,\\\n","                                                     mask_pos2sense: mask_train_pos2sense , dropout_keep_prob : 0.3 })\n","            \n","            train_loss.append(loss_other)            \n","            \n","            if i % (BATCH_SIZE * 100) == 0:                \n","                print ('Train batch %d loss %f' % (i, loss_other))\n","                \n","        train_loss=np.mean(train_loss,dtype=float)                \n","        print ('Train Epoch %d loss %f' % (epoch+1, train_loss))        \n","        print('--------------------------------') \n","        \n","                                       \n","#         #Dev\n","        dev_loss=[]\n","        dev_pred=[]\n","        dev_hit_all=0\n","        dev_all_char=0        \n","        for i in range(0, len(x_dev_Raw_Sent_id), BATCH_SIZE):            \n","            \n","            batch_x_Lemma = x_dev_Raw_Sent[slice(i, i + BATCH_SIZE)]\n","            batch_x_Raw = x_dev_Raw_Sent_id[slice(i, i + BATCH_SIZE)]\n","            batch_y_BN = y_dev_BNsense_Sent_id[slice(i, i + BATCH_SIZE)]\n","            batch_y_POS = y_dev_POS_Sent_id[slice(i, i + BATCH_SIZE)]\n","            \n","            y_dev_allSenses_Sent_batch = y_dev_allSenses_Sent[slice(i, i + BATCH_SIZE)]\n","            y_dev_allSenses_Sent_id_batch = y_dev_allSenses_Sent_id[slice(i, i + BATCH_SIZE)]\n","            \n","            mask_lemma2Sense_b = mask_lemmaToSenses_batch(batch_x_Raw, y_dev_allSenses_Sent_batch, y_dev_allSenses_Sent_id_batch, SenseWords2id)\n","            \n","            \n","            batch_x_Raw = padding3(batch_x_Raw, PAD_ID)\n","            batch_y_BN = padding(batch_y_BN, PAD_ID)\n","            batch_y_POS = padding(batch_y_POS, PAD_ID)            \n","            \n","            loss_other, lengths, predict = sess.run(\n","                [total_loss, seq_length, predictions], feed_dict = {input_ : batch_x_Raw, labels : batch_y_BN, y_pos : batch_y_POS, mask_RawTosense_batch : mask_lemma2Sense_b,\\\n","                                                                    mask_pos2sense: mask_dev_pos2sense , dropout_keep_prob: 1.0 })\n","            \n","            batch_true = y_dev_BNsense_Sent_id[slice(i, i + BATCH_SIZE)]\n","        \n","            pred_sense = []\n","            true_sense = []            \n","            for p in range(len(predict)):\n","                pr = []\n","                tr = []\n","                for L_ in range(lengths[p]):\n","                    # if our prediction is UNK means that word is not in our dictionary and we can't predict anything for that, so we put BackOff plan and MFS as our prediction.\n","                    if id2Sensewords[predict[p][L_]] != UNK:\n","                        pr.append(id2Sensewords[predict[p][L_]])\n","                    else:\n","                        pr.append(getMFS_(batch_x_Lemma[p][L_], WN2BN_map_dic))\n","#                         print(getMFS_(batch_x_Lemma[p][L_], WN2BN_map_dic))                        \n","                                                        \n","                    tr.append(id2Sensewords[batch_true[p][L_]])\n","#                 print(pr)\n","#                 print(tr)\n","#                 print('\\n')\n","                pred_sense.append(pr)\n","                true_sense.append(tr)\n","                        \n","            batch_pred = []\n","            for p in range(len(predict)):                \n","                dev_pred.append(predict[p][:lengths[p]])\n","                batch_pred.append(predict[p][:lengths[p]])\n","                \n","#             print('\\n')\n","            dev_batch_hit, dev_batch_char = number_of_batch_sense_hits(batch_pred, batch_true)            \n","            print('dev batch %d loss %f dev_batch_hit:%d dev_batch_char:%d batch_accuracy:%f' % (i, loss_other, dev_batch_hit, dev_batch_char, (dev_batch_hit/dev_batch_char)*100 ))\n","            dev_loss.append(loss_other)            \n","            dev_hit_all+=dev_batch_hit\n","            dev_all_char+=dev_batch_char\n","                \n","        dev_loss=np.mean(dev_loss,dtype=float)\n","        dev_acc = (dev_hit_all/dev_all_char)*100\n","        print('Valid Epoch %d loss %f' % (epoch+1,dev_loss))        \n","        print('dev_hit_all:%d dev_all_char:%d accuracy:%f' % (dev_hit_all,dev_all_char,dev_acc ))\n","        print('--------------------------------')\n","\n","        \n","        save_path = saver.save(sess, root_path + '/POS_BDLSTM_SenseEmbd_models_{}/model.ckpt'.format(dev_acc))\n","        print(\"Model saved in path: %s\" % save_path)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["global variable initialized\n","start session is ok ....\n","Train batch 0 loss 13.904799\n","Train batch 400 loss 12.158331\n","Train batch 800 loss 10.300423\n","Train batch 1200 loss 8.887848\n","Train batch 1600 loss 8.882174\n","Train batch 2000 loss 9.790975\n","Train batch 2400 loss 7.841256\n","Train batch 2800 loss 9.010290\n","Train batch 3200 loss 9.447711\n","Train batch 3600 loss 7.401810\n","Train batch 4000 loss 8.213692\n","Train batch 4400 loss 8.916322\n","Train batch 4800 loss 8.950150\n","Train batch 5200 loss 7.094263\n","Train batch 5600 loss 8.133968\n","Train batch 6000 loss 7.217041\n","Train batch 6400 loss 7.067302\n","Train batch 6800 loss 8.803652\n","Train batch 7200 loss 8.247992\n","Train batch 7600 loss 7.888868\n","Train batch 8000 loss 6.996228\n","Train batch 8400 loss 5.984664\n","Train batch 8800 loss 8.310463\n","Train batch 9200 loss 7.575582\n","Train batch 9600 loss 7.615728\n","Train batch 10000 loss 6.885161\n","Train batch 10400 loss 8.443302\n","Train batch 10800 loss 8.312398\n","Train batch 11200 loss 5.515939\n","Train batch 11600 loss 6.421906\n","Train batch 12000 loss 7.579618\n","Train batch 12400 loss 7.953328\n","Train batch 12800 loss 8.576845\n","Train batch 13200 loss 7.050471\n","Train batch 13600 loss 8.152922\n","Train batch 14000 loss 6.232923\n","Train batch 14400 loss 8.317991\n","Train batch 14800 loss 6.703600\n","Train batch 15200 loss 9.171314\n","Train batch 15600 loss 8.735638\n","Train batch 16000 loss 7.512558\n","Train batch 16400 loss 7.437321\n","Train batch 16800 loss 7.323681\n","Train batch 17200 loss 8.546795\n","Train batch 17600 loss 6.212642\n","Train batch 18000 loss 6.483882\n","Train batch 18400 loss 8.321738\n","Train batch 18800 loss 6.291648\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qsbNoEvJ4ULm","colab_type":"code","outputId":"d50e81f4-e42e-41b6-fd22-e14e78ffd244","executionInfo":{"status":"ok","timestamp":1568215021666,"user_tz":-120,"elapsed":770,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":[""],"execution_count":0,"outputs":[{"output_type":"stream","text":["<UNK>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3cquAV-s5jTZ","colab_type":"code","outputId":"5a9e1a12-5b49-439d-d843-de4ecb660016","executionInfo":{"status":"ok","timestamp":1568228177675,"user_tz":-120,"elapsed":1411,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["x_dev_IdInstance_Sent = pickle.load(open(dev_path + 'x_IdInstance_Sent.pkl', \"rb\"))\n","# bnId_file = open(SENT_FILE, \"w\")\n","# lex_file = open(SENT_FILE, \"w\")\n","# wDomain_file = open(SENT_FILE, \"w\")\n","for i in range(len(x_dev_IdInstance_Sent)):\n","    for j in range(len(x_dev_IdInstance_Sent[i])):\n","        if x_dev_IdInstance_Sent[i][j] != 'None':\n","            print(x_dev_IdInstance_Sent[i][j],' ',id2Sensewords[dev_pred[i][j]])\n","            print(x_dev_IdInstance_Sent[i][j],' ',id2Sensewords[dev_pred[i][j]])\n","            print(x_dev_IdInstance_Sent[i][j],' ',id2Sensewords[dev_pred[i][j]])\n","#             bnId_file.write(x_dev_IdInstance_Sent[i][j] + ' ' + id2Sensewords[dev_pred[i][j]])                \n","            \n","            # conver dev pred to blex and wordnet to write."],"execution_count":0,"outputs":[{"output_type":"stream","text":["d000.s000.t000   bn:00083240v\n","d000.s000.t001   bn:00067280n\n","d000.s000.t002   bn:00082233v\n","d000.s001.t000   bn:00020977n\n","d000.s001.t001   bn:00085636v\n","d000.s001.t002   bn:00086933v\n","d000.s001.t003   bn:00016850n\n","d000.s001.t004   bn:00088095v\n","d000.s002.t000   bn:00087106v\n","d000.s002.t001   bn:00074033n\n","d000.s003.t000   bn:00083293v\n","d000.s004.t000   bn:00016850n\n","d000.s004.t001   bn:00095375v\n","d000.s005.t000   bn:00085007v\n","d000.s005.t001   bn:00082195v\n","d000.s006.t000   bn:00090740v\n","d000.s006.t001   bn:00061450n\n","d000.s006.t002   bn:00082596v\n","d000.s006.t003   bn:00089240v\n","d000.s006.t004   bn:00009055n\n","d000.s007.t000   bn:00094573v\n","d000.s008.t000   bn:00061450n\n","d000.s008.t001   bn:00090161v\n","d000.s008.t002   bn:00071019n\n","d000.s008.t003   bn:00090161v\n","d000.s008.t004   bn:00057131n\n","d000.s009.t000   bn:00085415v\n","d000.s009.t001   bn:00048242n\n","d000.s009.t002   <UNK>\n","d000.s009.t003   bn:00046516n\n","d000.s009.t004   bn:00016733n\n","d000.s009.t005   bn:00085489v\n","d000.s009.t006   bn:00090161v\n","d000.s009.t007   bn:00071019n\n","d000.s010.t000   bn:00047052n\n","d000.s010.t001   bn:00086491v\n","d000.s010.t002   bn:00000498n\n","d000.s011.t000   bn:00082669v\n","d000.s011.t001   bn:00088454v\n","d000.s012.t000   bn:00082822v\n","d000.s012.t001   bn:00086733v\n","d000.s012.t002   bn:00005134n\n","d000.s012.t003   bn:00091873v\n","d000.s012.t004   bn:00086733v\n","d000.s014.t000   bn:00074790n\n","d000.s014.t001   bn:00093287v\n","d000.s015.t000   bn:00074790n\n","d000.s015.t001   bn:00086557v\n","d000.s015.t002   bn:00083187v\n","d000.s015.t003   bn:00087013v\n","d000.s015.t004   bn:00020872n\n","d000.s016.t000   bn:00048242n\n","d000.s016.t001   bn:00086008v\n","d000.s016.t002   bn:00087671v\n","d000.s016.t003   bn:00082868v\n","d000.s016.t004   bn:00046516n\n","d000.s016.t005   bn:00087441v\n","d000.s018.t000   bn:00085007v\n","d000.s018.t001   bn:00027366n\n","d000.s018.t002   bn:00017120n\n","d000.s018.t003   bn:00084684v\n","d000.s018.t004   bn:00093070v\n","d000.s019.t000   <UNK>\n","d000.s020.t000   bn:00089240v\n","d000.s020.t001   <UNK>\n","d000.s021.t000   bn:00087976v\n","d000.s024.t000   bn:00084160v\n","d000.s024.t001   bn:00061746n\n","d000.s024.t002   bn:00025780n\n","d000.s024.t003   bn:00091708v\n","d000.s024.t004   bn:00017749n\n","d000.s024.t005   bn:00058285n\n","d000.s024.t006   bn:00001530n\n","d000.s025.t000   bn:00082641v\n","d000.s025.t001   bn:00083370v\n","d000.s026.t000   bn:00067280n\n","d000.s026.t001   bn:00084710v\n","d000.s026.t002   <UNK>\n","d000.s026.t003   bn:00087106v\n","d000.s028.t000   bn:00053318n\n","d000.s028.t001   bn:00084932v\n","d000.s028.t002   bn:00085007v\n","d000.s028.t003   bn:00041942n\n","d000.s028.t004   bn:00082393v\n","d000.s028.t005   bn:00082276v\n","d000.s028.t006   bn:00075330n\n","d000.s028.t007   bn:00085261v\n","d000.s028.t008   bn:00076921n\n","d000.s028.t009   bn:00093430v\n","d000.s028.t010   bn:00024139n\n","d000.s028.t011   bn:00083141v\n","d000.s029.t000   bn:00035768n\n","d000.s029.t001   bn:00093731v\n","d000.s029.t002   bn:00082413v\n","d000.s030.t000   bn:00090740v\n","d000.s030.t001   bn:00059480n\n","d000.s030.t002   bn:00091451v\n","d000.s031.t000   <UNK>\n","d000.s031.t001   <UNK>\n","d000.s031.t002   bn:00032093n\n","d000.s031.t003   bn:00082417v\n","d000.s031.t004   bn:00084593v\n","d000.s031.t005   bn:00074790n\n","d000.s032.t000   bn:00084932v\n","d000.s032.t001   bn:00032180n\n","d000.s032.t002   bn:00089660v\n","d000.s032.t003   bn:00092204v\n","d000.s032.t004   bn:00086636v\n","d000.s033.t000   bn:00082668v\n","d000.s033.t001   bn:00088095v\n","d000.s033.t002   bn:00089667v\n","d001.s000.t000   bn:00000741n\n","d001.s000.t001   bn:00083341v\n","d001.s001.t000   bn:00082442v\n","d001.s001.t001   bn:00016401n\n","d001.s002.t000   bn:00091135v\n","d001.s002.t001   bn:00078318n\n","d001.s002.t002   bn:00093287v\n","d001.s002.t003   bn:00066365n\n","d001.s002.t004   bn:00082197v\n","d001.s002.t005   bn:00095851v\n","d001.s002.t006   bn:00086391v\n","d001.s003.t000   bn:00030951n\n","d001.s003.t001   bn:00007287n\n","d001.s003.t002   bn:00083369v\n","d001.s003.t003   bn:00082646v\n","d001.s003.t004   bn:00092424v\n","d001.s003.t005   bn:00012059n\n","d001.s004.t000   bn:00085567v\n","d001.s004.t001   bn:00061006n\n","d001.s004.t002   bn:00088912v\n","d001.s004.t003   bn:00021286n\n","d001.s004.t004   bn:00022240n\n","d001.s004.t005   <UNK>\n","d001.s004.t006   bn:00084069v\n","d001.s005.t000   bn:00012059n\n","d001.s005.t001   bn:00084725v\n","d001.s005.t002   bn:00083293v\n","d001.s005.t003   bn:00010161n\n","d001.s006.t000   bn:00083341v\n","d001.s006.t001   bn:00082276v\n","d001.s006.t002   bn:00093076v\n","d001.s006.t003   bn:00086933v\n","d001.s006.t004   bn:00062759n\n","d001.s007.t000   bn:00022991n\n","d001.s007.t001   <UNK>\n","d001.s007.t002   bn:00022227n\n","d001.s007.t003   bn:00082966v\n","d001.s008.t000   bn:00082472v\n","d001.s008.t001   bn:00090717v\n","d001.s009.t000   bn:00010161n\n","d001.s009.t001   bn:00089240v\n","d001.s009.t002   bn:00087056v\n","d001.s009.t003   bn:00059957n\n","d001.s010.t000   bn:00083293v\n","d001.s011.t000   bn:00090424v\n","d001.s011.t001   bn:00005513n\n","d001.s011.t002   bn:00087106v\n","d001.s012.t000   bn:00021286n\n","d001.s012.t001   bn:00092471v\n","d001.s012.t002   bn:00089384v\n","d001.s012.t003   bn:00082705v\n","d001.s012.t004   bn:00004117n\n","d001.s013.t000   bn:00083341v\n","d001.s013.t001   bn:00089309v\n","d001.s013.t002   bn:00070528n\n","d001.s013.t003   bn:00021286n\n","d001.s013.t004   bn:00083341v\n","d001.s013.t005   bn:00014839n\n","d001.s013.t006   bn:00084040v\n","d001.s013.t007   bn:00036837n\n","d001.s013.t008   bn:00089660v\n","d001.s014.t000   bn:00082705v\n","d001.s015.t000   bn:00087106v\n","d001.s015.t001   bn:00030627n\n","d001.s015.t002   bn:00092928v\n","d001.s015.t003   bn:00089660v\n","d001.s016.t000   bn:00082844v\n","d001.s016.t001   bn:00087863v\n","d001.s016.t002   bn:00002155n\n","d001.s017.t000   bn:00083293v\n","d001.s017.t001   bn:00010161n\n","d001.s018.t000   bn:00053116n\n","d001.s018.t001   bn:00082705v\n","d001.s018.t002   bn:00075760n\n","d001.s019.t000   bn:00087863v\n","d001.s019.t001   <UNK>\n","d001.s020.t000   bn:00088095v\n","d001.s020.t001   bn:00026603n\n","d001.s020.t002   bn:00083285v\n","d001.s020.t003   bn:00046516n\n","d001.s021.t000   <UNK>\n","d001.s022.t000   bn:00086557v\n","d001.s022.t001   <UNK>\n","d001.s022.t002   bn:00088095v\n","d001.s023.t000   bn:00084040v\n","d001.s023.t001   bn:00089538v\n","d001.s024.t000   bn:00086682v\n","d001.s024.t001   bn:00005007n\n","d001.s025.t000   bn:00094234v\n","d001.s026.t000   bn:00084769v\n","d001.s026.t001   bn:00056758n\n","d001.s026.t002   bn:00083293v\n","d001.s026.t003   bn:00007287n\n","d001.s027.t000   bn:00085363v\n","d001.s027.t001   bn:00082276v\n","d001.s027.t002   bn:00082728v\n","d001.s028.t000   bn:00082170v\n","d001.s028.t001   bn:00007287n\n","d001.s028.t002   bn:00088901v\n","d001.s028.t003   bn:00062759n\n","d001.s028.t004   bn:00059799n\n","d001.s028.t005   bn:00085337v\n","d001.s029.t000   bn:00085344v\n","d001.s029.t001   bn:00093287v\n","d001.s029.t002   bn:00023301n\n","d001.s029.t003   bn:00086682v\n","d001.s029.t004   bn:00084711v\n","d001.s029.t005   bn:00062759n\n","d001.s030.t000   bn:00088939v\n","d001.s030.t001   bn:00040921n\n","d001.s030.t002   bn:00087894v\n","d001.s030.t003   bn:00075412n\n","d001.s030.t004   bn:00095597v\n","d001.s030.t005   bn:00083441v\n","d001.s031.t000   bn:00000005n\n","d001.s031.t001   bn:00089240v\n","d001.s031.t002   bn:00017761n\n","d001.s032.t000   bn:00085337v\n","d001.s032.t001   bn:00062759n\n","d001.s032.t002   bn:00082690v\n","d001.s033.t000   bn:00062759n\n","d001.s033.t001   bn:00087115v\n","d001.s034.t000   bn:00088643v\n","d001.s034.t001   bn:00020058n\n","d001.s035.t000   bn:00041942n\n","d001.s035.t001   bn:00061450n\n","d001.s035.t002   bn:00086612v\n","d001.s035.t003   bn:00083391v\n","d001.s036.t000   bn:00085261v\n","d001.s036.t001   bn:00007299n\n","d001.s036.t002   bn:00092580v\n","d001.s036.t003   bn:00080747n\n","d001.s036.t004   bn:00086713v\n","d001.s036.t005   bn:00020982n\n","d001.s036.t006   bn:00089384v\n","d001.s036.t007   bn:00031641n\n","d001.s036.t008   bn:00055644n\n","d001.s036.t009   bn:00087106v\n","d001.s036.t010   bn:00086713v\n","d001.s036.t011   <UNK>\n","d001.s036.t012   bn:00021590n\n","d001.s037.t000   bn:00089240v\n","d001.s037.t001   bn:00080435n\n","d001.s037.t002   bn:00093430v\n","d001.s037.t003   bn:00084769v\n","d001.s038.t000   bn:00083184v\n","d001.s038.t001   bn:00014138n\n","d001.s038.t002   bn:00063155n\n","d001.s039.t000   bn:00047688n\n","d001.s039.t001   bn:00084110v\n","d002.s000.t000   bn:00082226v\n","d002.s000.t001   bn:00082613v\n","d002.s000.t002   bn:00082817v\n","d002.s000.t003   bn:00086682v\n","d002.s000.t004   bn:00086607v\n","d002.s001.t000   bn:00092424v\n","d002.s001.t001   bn:00094792v\n","d002.s002.t000   bn:00001533n\n","d002.s002.t001   bn:00092837v\n","d002.s002.t002   bn:00007011n\n","d002.s002.t003   bn:00082273v\n","d002.s002.t004   bn:00010728n\n","d002.s003.t000   bn:00087556v\n","d002.s003.t001   bn:00074009n\n","d002.s003.t002   bn:00083240v\n","d002.s003.t003   bn:00082844v\n","d002.s003.t004   bn:00088095v\n","d002.s003.t005   bn:00052556n\n","d002.s003.t006   bn:00089240v\n","d002.s004.t000   <UNK>\n","d002.s004.t001   bn:00089584v\n","d002.s004.t002   bn:00083133v\n","d002.s004.t003   bn:00083995v\n","d002.s004.t004   bn:00035851n\n","d002.s005.t000   bn:00095777v\n","d002.s005.t001   bn:00004451n\n","d002.s006.t000   <UNK>\n","d002.s008.t000   bn:00082690v\n","d002.s008.t001   bn:00083188v\n","d002.s008.t002   bn:00050340n\n","d002.s008.t003   bn:00088035v\n","d002.s009.t000   bn:00058285n\n","d002.s009.t001   bn:00091458v\n","d002.s009.t002   bn:00076647n\n","d002.s009.t003   bn:00094669v\n","d002.s009.t004   bn:00005164n\n","d002.s009.t005   bn:00093170v\n","d002.s010.t000   bn:00089384v\n","d002.s010.t001   bn:00089660v\n","d002.s010.t002   bn:00082920v\n","d002.s010.t003   bn:00008188n\n","d002.s010.t004   bn:00086713v\n","d002.s010.t005   bn:00092876v\n","d002.s011.t000   bn:00008188n\n","d002.s011.t001   bn:00086591v\n","d002.s011.t002   bn:00021644n\n","d002.s013.t000   bn:00089384v\n","d002.s013.t001   bn:00007062n\n","d002.s014.t000   bn:00088095v\n","d002.s014.t001   bn:00093738v\n","d002.s015.t000   bn:00076921n\n","d002.s015.t001   bn:00093287v\n","d002.s015.t002   bn:00082822v\n","d002.s015.t003   bn:00007146n\n","d002.s016.t000   bn:00090448v\n","d002.s017.t000   bn:00093287v\n","d002.s017.t001   bn:00086717v\n","d002.s017.t002   <UNK>\n","d002.s018.t000   bn:00093287v\n","d002.s018.t001   bn:00090448v\n","d002.s019.t000   bn:00082808v\n","d002.s020.t000   bn:00093287v\n","d002.s020.t001   bn:00088912v\n","d002.s020.t002   bn:00086682v\n","d002.s020.t003   bn:00082276v\n","d002.s021.t000   bn:00083240v\n","d002.s022.t000   bn:00090943v\n","d002.s024.t000   bn:00002306n\n","d002.s024.t001   bn:00090943v\n","d002.s025.t000   bn:00083240v\n","d002.s025.t001   bn:00083341v\n","d002.s025.t002   bn:00084711v\n","d002.s026.t000   bn:00031545n\n","d002.s026.t001   bn:00085337v\n","d002.s027.t000   bn:00082788v\n","d002.s027.t001   bn:00008985n\n","d002.s027.t002   bn:00095665v\n","d002.s027.t003   bn:00008188n\n","d002.s027.t004   bn:00082588v\n","d002.s027.t005   bn:00094275v\n","d002.s027.t006   bn:00086370v\n","d002.s027.t007   bn:00088378v\n","d002.s027.t008   bn:00008872n\n","d002.s027.t009   bn:00089384v\n","d002.s028.t000   bn:00083188v\n","d002.s028.t001   bn:00001557n\n","d002.s028.t002   bn:00090400v\n","d002.s028.t003   bn:00028795n\n","d002.s028.t004   bn:00084388v\n","d002.s029.t000   bn:00082788v\n","d002.s029.t001   bn:00055094n\n","d002.s029.t002   bn:00083195v\n","d002.s029.t003   bn:00095665v\n","d002.s029.t004   bn:00082761v\n","d002.s029.t005   bn:00023381n\n","d002.s029.t006   bn:00082569v\n","d002.s030.t000   bn:00088095v\n","d002.s030.t001   bn:00089384v\n","d002.s030.t002   bn:00085415v\n","d002.s031.t000   bn:00082788v\n","d002.s031.t001   bn:00092204v\n","d002.s031.t002   bn:00042759n\n","d002.s031.t003   bn:00094230v\n","d002.s033.t000   bn:00082788v\n","d002.s034.t000   bn:00093430v\n","d002.s034.t001   bn:00089112v\n","d002.s036.t000   bn:00088912v\n","d002.s036.t001   bn:00089282v\n","d002.s036.t002   bn:00002185n\n","d002.s036.t003   bn:00014018n\n","d002.s036.t004   bn:00087106v\n","d002.s036.t005   bn:00077607n\n","d002.s036.t006   bn:00088095v\n","d002.s036.t007   bn:00086607v\n","d002.s036.t008   bn:00024510n\n","d002.s037.t000   bn:00087106v\n","d002.s037.t001   bn:00007309n\n","d002.s038.t000   bn:00088912v\n","d002.s038.t001   bn:00082951v\n","d002.s038.t002   bn:00000127n\n","d002.s039.t000   bn:00008188n\n","d002.s039.t001   bn:00086100v\n","d002.s040.t000   bn:00082808v\n","d002.s040.t001   bn:00085356v\n","d002.s041.t000   bn:00087106v\n","d002.s041.t001   <UNK>\n","d002.s041.t002   bn:00006532n\n","d002.s042.t000   bn:00084388v\n","d002.s042.t001   bn:00002306n\n","d002.s042.t002   bn:00008872n\n","d002.s042.t003   bn:00087087v\n","d002.s043.t000   bn:00084388v\n","d002.s043.t001   bn:00083833v\n","d002.s043.t002   bn:00089252v\n","d002.s043.t003   bn:00013853n\n","d002.s044.t000   bn:00002306n\n","d002.s044.t001   bn:00087901v\n","d002.s044.t002   bn:00014018n\n","d002.s044.t003   bn:00082761v\n","d002.s045.t000   bn:00093316v\n","d002.s045.t001   bn:00087087v\n","d002.s046.t000   bn:00082788v\n","d002.s046.t001   bn:00092903v\n","d002.s046.t002   bn:00089240v\n","d002.s046.t003   bn:00062972n\n","d002.s046.t004   bn:00093364v\n","d002.s046.t005   bn:00042601n\n","d002.s046.t006   bn:00093364v\n","d002.s047.t000   bn:00090448v\n","d002.s049.t000   bn:00087327v\n","d002.s049.t001   bn:00082276v\n","d002.s049.t002   bn:00083731v\n","d002.s049.t003   bn:00087652v\n","d002.s049.t004   bn:00002155n\n","d002.s049.t005   bn:00033687n\n","d002.s049.t006   bn:00087845v\n","d002.s049.t007   bn:00044283n\n","d002.s049.t008   bn:00088779v\n","d002.s050.t000   <UNK>\n","d002.s050.t001   bn:00008188n\n","d002.s050.t002   bn:00082788v\n","d002.s050.t003   bn:00092903v\n","d002.s050.t004   bn:00095665v\n","d002.s050.t005   bn:00083956v\n","d002.s050.t006   bn:00023496n\n","d002.s050.t007   bn:00001128n\n","d002.s050.t008   bn:00089660v\n","d002.s050.t009   bn:00055504n\n","d002.s050.t010   bn:00089958v\n","d002.s050.t011   bn:00091762v\n","d002.s050.t012   bn:00002185n\n","d002.s050.t013   bn:00084241v\n","d002.s050.t014   bn:00084916v\n","d002.s050.t015   bn:00008872n\n","d002.s051.t000   bn:00032203n\n","d002.s051.t001   bn:00089240v\n","d002.s051.t002   bn:00083188v\n","d002.s051.t003   bn:00087327v\n","d002.s052.t000   bn:00087672v\n","d002.s052.t001   bn:00092969v\n","d002.s052.t002   bn:00095665v\n","d002.s052.t003   bn:00029065n\n","d002.s052.t004   <UNK>\n","d002.s052.t005   bn:00041024n\n","d002.s052.t006   bn:00093776v\n","d002.s052.t007   bn:00029935n\n","d002.s054.t000   bn:00084373v\n","d002.s055.t000   bn:00085021v\n","d002.s056.t000   bn:00088939v\n","d002.s056.t001   bn:00086607v\n","d002.s057.t000   bn:00094432v\n","d002.s057.t001   bn:00089411v\n","d002.s057.t002   bn:00087714v\n","d002.s057.t003   bn:00093244v\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F3gN7ts_5vWV","colab_type":"code","outputId":"7d53bfab-c2ea-4dfe-ad22-0f621ee63474","executionInfo":{"status":"ok","timestamp":1568230104524,"user_tz":-120,"elapsed":560,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["SenseWords2id['the']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"KDn6vcOYzFr6","colab_type":"code","outputId":"3993b64c-558e-4f9d-d459-2cb424a135b1","executionInfo":{"status":"ok","timestamp":1568232420187,"user_tz":-120,"elapsed":424,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["RawWords2id['the']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"CRBdH3P577EL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}