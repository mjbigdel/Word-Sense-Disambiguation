{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"POS_WnDomain_Tagger_Prediction.ipynb","version":"0.3.2","provenance":[{"file_id":"1Wy6S3SpIJb6bHMlcCWvR7Da59ke3wXax","timestamp":1568343113863},{"file_id":"1wdytMuns6MewTEy192MP7w4taz3ciG9u","timestamp":1568340054758},{"file_id":"133-cht5tKzdKATkeqa86Dc2atf6HRNFJ","timestamp":1568336387096},{"file_id":"1KUy4mTQz-CIbM2zvPX5z9cQWeKOJD2HT","timestamp":1568334133194},{"file_id":"1W6GHhtAv1zZ1Z2D6lLvC-uDSDWJD130O","timestamp":1567765261410}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ueQIwiNm42tG","colab_type":"code","outputId":"9ce3c8e0-7192-4416-b460-6cf9090fe27e","executionInfo":{"status":"ok","timestamp":1568323637835,"user_tz":-120,"elapsed":1927,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# !pip install tensorflow==1.12.0\n","# !pip install tensorflow \n","# TensorFlow and tf.keras\n","# its just working with tensorflow 1.13.1, with others has problem\n","import tensorflow as tf\n","print(tf.__version__)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.14.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z_pjD2e15BFA","colab_type":"text"},"source":["**- goolge Colab Configuration**"]},{"cell_type":"code","metadata":{"id":"nZKvqNpW5HjM","colab_type":"code","outputId":"adb5b8ff-b3ab-4834-df00-97024a301ae2","executionInfo":{"status":"ok","timestamp":1568369014457,"user_tz":-120,"elapsed":18392,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UTR5Csn45f2I","colab_type":"text"},"source":["**- Setting root address to project in Google Drive**"]},{"cell_type":"code","metadata":{"id":"ywAPM0ns5dX-","colab_type":"code","colab":{}},"source":["!ls \"/content/gdrive/My Drive/NLPHW3\"\n","root_path = '/content/gdrive/My Drive/NLPHW3'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"waoW11y6yLmL","colab_type":"code","outputId":"3a2b3707-e162-4b7f-e5c0-bba2f712ece2","executionInfo":{"status":"ok","timestamp":1568369018141,"user_tz":-120,"elapsed":4239,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["# Imports\n","from collections import Counter\n","import codecs\n","import xml.etree.cElementTree as etree\n","import pickle\n","import nltk\n","nltk.download('wordnet')\n","import numpy as np\n","import copy\n","from nltk.corpus import wordnet as wn\n","\n","\n","# Added Chars to dictionaries\n","UNK = \"<UNK>\"\n","PAD = \"<PAD>\"\n","\n","\n","RESOURCE_PATH = root_path+'/resources/'\n","\n","MAP_WN2BN_FILE = RESOURCE_PATH + 'babelnet2wordnet.tsv'\n","MAP_BN2WNDOMAIN_FILE = RESOURCE_PATH + 'babelnet2wndomains.tsv'\n","MAP_BN2LEXNAMES_FILE = RESOURCE_PATH + 'babelnet2lexnames.tsv'\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"epYctd54CoTK","colab_type":"code","outputId":"b6959f2d-d95c-4faa-c38a-5772570c669f","executionInfo":{"status":"ok","timestamp":1568369030292,"user_tz":-120,"elapsed":1161,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["all_dict_data = pickle.load(open(root_path+'/semcor/semcor/all_dict_data.pkl', \"rb\"))\n","\n","RawWords2id = all_dict_data[\"RawWords2id\"]\n","SenseWords2id = all_dict_data[\"SenseWords2id\"] \n","pos2id = all_dict_data[\"pos2id\"] \n","lex2id = all_dict_data[\"lex2id\"] \n","wnDomain2id = all_dict_data[\"wnDomain2id\"]\n","\n","print(len(pos2id))\n","print(len(SenseWords2id))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12\n","60919\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yIYezFrT5PUF","colab_type":"text"},"source":["**- python library imports**"]},{"cell_type":"code","metadata":{"id":"pT5cObkBpThO","colab_type":"code","colab":{}},"source":["# ===-----------------------------------------------------------------------===\n","# Trainig Section\n","# ===-----------------------------------------------------------------------===\n","\n","# function for add padding for train_y in every batch based on maximum length of sentence in that batch\n","# this method boost the performance of running.\n","def padding(X,padding_word):\n","\tmax_len = 0\n","\tfor x in X:\n","\t\tif len(x) > max_len:\n","\t\t\tmax_len = len(x)\n","\tpadded_X = np.ones((len(X), max_len), dtype=np.int32) * padding_word\n","\tfor i in range(len(X)):\n","\t\tfor j in range(len(X[i])):\n","\t\t\tpadded_X[i, j] = X[i][j]\n","\treturn padded_X\n","\n","\n","# padding function for train_x\n","def padding3(X,padding_word):\n","\tmax_len = 0\n","\tfor x in X:\n","\t\tif len(x) > max_len:\n","\t\t\tmax_len = len(x)\n","\tpadded_X = np.ones((len(X), max_len), dtype=np.int32) * padding_word\n","\tfor i in range(len(X)):\n","\t\tfor j in range(len(X[i])):\n","\t\t\tpadded_X[i, j] = X[i][j]\n","\treturn padded_X\n","\n","# function for calculating umber of hits and number of all predictions.\n","def number_of_batch_hits(y_batch_pred,y_batch_true):\n","    num_hits = 0\n","    num_all_chars = 0    \n","    for i in range(len(y_batch_true)):\n","        for j in range(len(y_batch_true[i])):\n","            num_all_chars = num_all_chars+1\n","            if y_batch_pred[i][j] == y_batch_true[i][j]:\n","                num_hits = num_hits+1    \n","    return num_hits, num_all_chars\n","\n","# function for calculating umber of hits and number of all predictions.\n","def number_of_batch_sense_hits(y_batch_pred,y_batch_true):\n","    num_hits = 0\n","    num_all_chars = 0    \n","    for i in range(len(y_batch_true)):\n","        for j in range(len(y_batch_true[i])):\n","            if id2Sensewords[y_batch_true[i][j]].startswith('bn'):\n","                num_all_chars = num_all_chars+1\n","                if y_batch_pred[i][j] == y_batch_true[i][j]:\n","                    num_hits = num_hits+1    \n","    return num_hits, num_all_chars\n","\n","# ----------------- Add Summary Function ------------------------------------------\n","def add_summary(writer, name, value, global_step):\n","    summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n","    writer.add_summary(summary, global_step=global_step)                                 \n","    \n","\n","## ----------------------------------------------------------------------------\n","def mask_lemmaToSenses_batch(x_Raw_Sent_batch, y_allSenses_Sent_batch, y_allSenses_Sent_id_batch, SenseWords2id):\n","     \n","    max_len = 0    \n","    for inp in x_Raw_Sent_batch:\n","        if len(inp) > max_len:\n","            max_len = len(inp)\n","            \n","    mask_lemma2sense_batch = [0]*np.ones(shape = (len(x_Raw_Sent_batch), max_len, len(SenseWords2id)))\n","            \n","    for i in range(len(x_Raw_Sent_batch)):\n","        for j in range(len(x_Raw_Sent_batch[i])):\n","            for k in range(len(y_allSenses_Sent_batch[i][j])):            \n","#                 if SenseWords2id.get(y_allSenses_Sent_batch[i][j][k]) is not None:\n","                mask_lemma2sense_batch[i][j][y_allSenses_Sent_id_batch[i][j][k]] = 1    \n","#                    mask_lemma2sense_batch[rawWords_batch2id[x_Raw_Sent_batch[i][j]]][y_allSenses_Sent_id_batch[i][j][k]] = 1    \n","#                 else:\n","#                     print('not in dictionary sense is : ', y_allSenses_Sent_batch[i][j][k])\n","            \n","    return mask_lemma2sense_batch    \n","\n","#x_Raw_Sent_batch = x_Raw_Sent[:12]\n","#y_allSenses_Sent_batch = y_allSenses_Sent[:12]\n","#y_allSenses_Sent_id_batch = y_allSenses_Sent_id[:12]\n","#\n","#mask_lemma2sense_batch = mask_lemmaToSenses_batch(x_Raw_Sent_batch, y_allSenses_Sent_batch, y_allSenses_Sent_id_batch, SenseWords2id)    \n","\n","# use pretrained embeddings function, for chars we are using pretrain file and for bigram, trigram \n","# and fourgram we are using mean of embeddings of all unigrams of them\n","def reading_pretrained_sense_embeddings(filename, RawWords2id):\n","    # Reading Pretrained Embeddings from file\n","    pretrain_embeddigs = {}\n","    with codecs.open(filename, \"r\", \"utf-8\") as f:\n","        for line in f:\n","            pre_train = line.split()\n","            if len(pre_train) > 2:\n","                word = pre_train[0]\n","                if word in RawWords2id:\n","                    vec = pre_train[1:]\n","                    pretrain_embeddigs[word] = vec                \n","    \n","    print(\"pretrained embeddings files reading finished ...\")\n","    # making embeddings for all RawWords2id.\n","    embedding_dim = len(next(iter(pretrain_embeddigs.values())))\n","    out_of_vocab = 0\n","    out = np.ones((len(RawWords2id), embedding_dim))*0.001\n","    for word in RawWords2id.keys():\n","        if len(word) == 1:\n","            if word in pretrain_embeddigs.keys():        \n","                out[RawWords2id[word]]=np.array(pretrain_embeddigs[word])\n","            else:                \n","                out_of_vocab+=1\n","                np.random.uniform(-1.0, 1.0, embedding_dim)\n","        \n","    return out,out_of_vocab\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ifW2h9YyD7oi","colab_type":"code","outputId":"c79b2453-7f36-45a3-8250-aa7e40806213","executionInfo":{"status":"ok","timestamp":1568369065260,"user_tz":-120,"elapsed":8016,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["\n","PAD_ID = RawWords2id[PAD]\n","id2Sensewords = dict(zip(SenseWords2id.values(), SenseWords2id.keys()))\n","\n","sense_embeddings, out_of_vocab = reading_pretrained_sense_embeddings(root_path + '/allWordsSensesembeddings.vec', RawWords2id)\n","print('sense_embeddings: ', len(sense_embeddings))\n","print('out_of_vocab: ', out_of_vocab)\n","\n","# we are using pretrained embedding - len(words2id)\n","VOCAB_SIZE =  len(RawWords2id)\n","WORD_EMBEDDING_DIM = 100\n","print('WORD_EMBEDDING_DIM: ', WORD_EMBEDDING_DIM)\n","print('VOCAB_SIZE: ', VOCAB_SIZE)\n","\n","# some Basic Hyper parameters\n","NUM_EPOCHS = 5\n","BATCH_SIZE = 4\n","HIDDEN_LAYER_DIM = 256\n","LEARNING_RATE = 1\n","NUM_CLASSES = len(SenseWords2id)\n","print('num sense classes: ', NUM_CLASSES)\n","L2_REGU_LAMBDA=0.0001\n","NUM_LAYERS = 2\n","CLIP=10\n","crf_lambda = 0.05\n","num_sense_pos = len(pos2id)\n","print('num_sense_pos: ', num_sense_pos)\n","num_sense_lex = len(lex2id)\n","print('num_sense_lex: ', num_sense_lex)\n","num_sense_wnDomain = len(wnDomain2id)\n","print('num_sense_wnDomain: ', num_sense_wnDomain)\n","VERY_BIG_NUMBER = 1e30\n","summaries = []\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["pretrained embeddings files reading finished ...\n","sense_embeddings:  32893\n","out_of_vocab:  5\n","WORD_EMBEDDING_DIM:  100\n","VOCAB_SIZE:  32893\n","num sense classes:  60919\n","num_sense_pos:  12\n","num_sense_lex:  46\n","num_sense_wnDomain:  160\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c7VncpqopT0M","colab_type":"code","colab":{}},"source":["\n","# --------------------- Tensorflow part ---------------------------------------------------\n","import tensorflow as tf\n","from tensorflow.contrib import layers\n","from tensorflow.contrib import crf\n","\n","def create_tensorflow_model(vocab_size, embedding_dim, hidden_layer_dim, sense_embed_bool = False, total_loss = 0):\n","    print(\"Creating TENSORFLOW model\")\n","     \n","    # Inputs have (batch_size, timesteps) shape.\n","    input_ = tf.placeholder(dtype = tf.int32, shape=[None, None], name='input_x')   \n","    # Labels have (batch_size,) shape.\n","    labels = tf.placeholder(dtype = tf.int32, shape=[None, None], name='labels_BN')\n","    \n","    y_pos = tf.placeholder(dtype = tf.int32, shape=[None, None], name='pos_tag')\n","        \n","    y_wnDomain = tf.placeholder(dtype = tf.int32, shape=[None, None],name='wnDomain_tag')        \n","    \n","    # dropout_keep_prob is a scalar.\n","    dropout_keep_prob = tf.placeholder(dtype = tf.float32, name='dropout_keep_prob')\n","    \n","    seq_length = tf.reduce_sum(tf.cast( tf.not_equal(input_[:,:], tf.ones_like(input_[:,:] ) * PAD_ID ), tf.int32), 1)    \n","    print('seq_length: ', seq_length)\n","    \n","    max_sent_size = tf.size(input_[0,:])    \n","    print('max_sent_size: ', max_sent_size)\n","    \n","    batch_size = tf.size(input_[:,0])    \n","    print('batch_size: ', batch_size)\n","            \n","    mask_RawTosense_batch = tf.placeholder(dtype = tf.bool, shape=[None, None, None], name='mask_Raw2Senses')    \n","    print('mask_RawTosense_batch: ', mask_RawTosense_batch)\n","    \n","    mask_pos2sense = tf.placeholder(dtype = tf.bool, shape=[None, None], name='mask_Raw2Senses')    \n","    print('mask_pos2sense: ', mask_pos2sense)\n","    mask_matrix_pos = tf.tile(tf.expand_dims(mask_pos2sense, 0), [batch_size, 1, 1])     \n","    print('mask_matrix_pos: ', mask_matrix_pos)\n","    \n","    mask_wnDomain2sense = tf.placeholder(dtype = tf.bool, shape=[None, None], name='mask_Raw2Senses')    \n","    print('mask_wnDomain2sense: ', mask_wnDomain2sense)\n","    mask_matrix_wnDomain = tf.tile(tf.expand_dims(mask_wnDomain2sense, 0), [batch_size, 1, 1])     \n","    print('mask_matrix_wnDomain: ', mask_matrix_wnDomain)\n","    \n","    x_mask = tf.not_equal(input_[:,:], tf.ones_like(input_[:,:] ) * PAD_ID )    \n","    print('x_mask: ', x_mask)\n","    \n","    attention_mask = (tf.cast(x_mask, 'float') -1) * VERY_BIG_NUMBER     \n","    print('attention_mask: ', attention_mask)\n","\n","    # initialize weights randomly from a Gaussian distribution\n","    weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n","            \n","## ---------------------------- embedding Block --------------------------------------------\n","## -----------------------------------------------------------------------------------------  \n","    with tf.variable_scope('embeddings', reuse=tf.AUTO_REUSE):\n","        if sense_embed_bool:\n","            embedding_matrix = tf.Variable(sense_embeddings, dtype=tf.float32, name='embedding')\n","        else:\n","            embedding_matrix = tf.get_variable(\"embeddings\", shape=[vocab_size, embedding_dim])\n","        \n","        embeddings = tf.nn.embedding_lookup(embedding_matrix, input_)        \n","        print('embeddings: ', embeddings)\n","        \n","        embeddings = tf.reshape(embeddings,[batch_size, max_sent_size, embedding_dim]) #         \n","        print('embeddings: ', embeddings)\n","\n","        embeddings=tf.nn.dropout(tf.cast(embeddings, tf.float32), dropout_keep_prob) #     embeddings shape (batch size, sentence length with padding, 100) \n","\n","        print ('embeddings is ok \\n')\n","                \n","    \n","## --------------------------- POS Block ---------------------------------------------------\n","## -----------------------------------------------------------------------------------------  \n","    with tf.variable_scope('rnn_cell_pos', reuse=tf.AUTO_REUSE):\n","            print ('lstm_cell_pos is is ok ... ')\n","            def lstm_cell1():\n","                return tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_layer_dim), output_keep_prob=dropout_keep_prob)\n","\n","            stacked_fw_lstm_pos = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell1() for _ in range(NUM_LAYERS)])\n","\n","            stacked_bw_lstm_pos = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell1() for _ in range(NUM_LAYERS)])\n","\n","    with tf.variable_scope('rnn_pos', reuse=tf.AUTO_REUSE):                        \n","        (forward_output_pos, backword_output_pos), _ = tf.nn.bidirectional_dynamic_rnn(\n","            cell_fw = stacked_fw_lstm_pos,\n","            cell_bw = stacked_bw_lstm_pos,\n","            inputs = embeddings,\n","            sequence_length = seq_length,\n","            dtype=tf.float32\n","        )                        \n","        outputBD_pos = tf.concat([forward_output_pos, backword_output_pos], axis=2)\n","        print('outputBD_pos: ', outputBD_pos) # shape is batch*max_len*(2*hidden_layer_size)                         \n","\n","        print ('outputBD_pos is ok \\n')\n","\n","    with tf.variable_scope(\"softmax_layer_pos\", reuse=tf.AUTO_REUSE):\n","        W_pos = tf.get_variable(\"W_pos\", shape=[2*hidden_layer_dim, num_sense_pos], initializer = weight_initer)\n","        b_pos = tf.get_variable(\"b_pos\", shape=[num_sense_pos], initializer=tf.zeros_initializer())\n","\n","        flat_softmax_pos = tf.reshape(outputBD_pos, [-1, tf.shape(outputBD_pos)[2]]) # shape is (batch*max_len)*(2*hidden_layer_size)\n","        print('flat_softmax_pos: ', flat_softmax_pos)\n","\n","        drop_flat_softmax_pos = tf.nn.dropout(flat_softmax_pos, dropout_keep_prob) # shape is (batch*max_len)*(2*hidden_layer_size)\n","        print('drop_flat_softmax_pos: ', drop_flat_softmax_pos)\n","\n","        flat_pos_logits = tf.matmul(drop_flat_softmax_pos, W_pos) + b_pos # shape is (batch*max_len)*num_sense_pos\n","        print('flat_pos_logits: ', flat_pos_logits)\n","\n","        pos_logits = tf.reshape(flat_pos_logits, [batch_size, max_sent_size, num_sense_pos]) # shape is batch*max_len*num_sense_pos ->3D\n","        print('pos_logits: ', pos_logits)\n","\n","        predict_pos = tf.argmax(pos_logits , axis=2)\n","\n","        loss_pos = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= y_pos, logits= pos_logits))\n","\n","        total_loss = total_loss + loss_pos\n","\n","        float_mask_matrix_pos = tf.cast(mask_matrix_pos, dtype=tf.float32)\n","        print('float_mask_matrix_pos: ', float_mask_matrix_pos)\n","\n","        pos_masked_senses = tf.matmul(pos_logits, float_mask_matrix_pos) #mask_mat = 2*11*56 or batch*pos_num*NUM_CLASSES\n","        print('pos_masked_senses: ', pos_masked_senses)  # size is: batch*max_len*NUM_CLASSES\n","\n","        print ('softmax_layer_pos is ok \\n')\n","\n","\n","## --------------------------- wnDomain Block ------------------------------------------------\n","## -------------------------------------------------------------------------------------------\n","    with tf.variable_scope('rnn_cell_wnDomain', reuse=tf.AUTO_REUSE):\n","        print ('lstm_cell_wnDomain is is ok ... ')\n","        def lstm_cell1():\n","            return tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_layer_dim), output_keep_prob=dropout_keep_prob)\n","\n","        stacked_fw_lstm_wnDomain = tf.nn.rnn_cell.MultiRNNCell(\n","            [lstm_cell1() for _ in range(NUM_LAYERS)])\n","\n","        stacked_bw_lstm_wnDomain = tf.nn.rnn_cell.MultiRNNCell(\n","        [lstm_cell1() for _ in range(NUM_LAYERS)])\n","\n","    with tf.variable_scope('rnn_wnDomain', reuse=tf.AUTO_REUSE):                        \n","            (forward_output_wnDomain, backword_output_wnDomain), _ = tf.nn.bidirectional_dynamic_rnn(\n","                cell_fw = stacked_fw_lstm_wnDomain,\n","                cell_bw = stacked_bw_lstm_wnDomain,\n","                inputs = outputBD_pos,\n","                sequence_length = seq_length,\n","                dtype=tf.float32\n","            )\n","\n","            outputBD_wnDomain = tf.concat([forward_output_wnDomain, backword_output_wnDomain], axis=2)\n","            print('outputBD_wnDomain: ', outputBD_wnDomain) # shape is batch*max_len*(2*hidden_layer_size) \n","\n","            print ('outputBD_wnDomain is ok \\n')         \n","\n","    with tf.variable_scope(\"softmax_layer_wnDomain\", reuse=tf.AUTO_REUSE):\n","        W_wnDomain = tf.get_variable(\"W_wnDomain\", shape=[2*hidden_layer_dim, num_sense_wnDomain], initializer = weight_initer)\n","        b_wnDomain = tf.get_variable(\"b_wnDomain\", shape=[num_sense_wnDomain], initializer=tf.zeros_initializer())\n","\n","        flat_softmax_wnDomain = tf.reshape(outputBD_wnDomain, [-1, tf.shape(outputBD_wnDomain)[2]]) # shape is (batch*max_len)*(2*hidden_layer_size)\n","        print('flat_softmax_wnDomain: ', flat_softmax_wnDomain)\n","\n","        drop_flat_softmax_wnDomain = tf.nn.dropout(flat_softmax_wnDomain, dropout_keep_prob) # shape is (batch*max_len)*(2*hidden_layer_size)\n","        print('drop_flat_softmax_wnDomain: ', drop_flat_softmax_wnDomain)\n","\n","        flat_wnDomain_logits = tf.matmul(drop_flat_softmax_wnDomain, W_wnDomain) + b_wnDomain # shape is (batch*max_len)*num_sense_lex\n","        print('flat_wnDomain_logits: ', flat_wnDomain_logits)\n","\n","        wnDomain_logits = tf.reshape(flat_wnDomain_logits, [batch_size, max_sent_size, num_sense_wnDomain]) # shape is batch*max_len*num_sense_lex ->3D\n","        print('wnDomain_logits: ', wnDomain_logits)\n","\n","        predict_wnDomain = tf.argmax(wnDomain_logits , axis=2)\n","\n","        loss_wnDomain = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= y_wnDomain, logits= wnDomain_logits))\n","        total_loss = total_loss + loss_wnDomain\n","\n","        float_mask_matrix_wnDomain = tf.cast(mask_matrix_wnDomain, dtype=tf.float32)\n","        print('float_mask_matrix_wnDomain: ', float_mask_matrix_wnDomain)\n","\n","        wnDomain_masked_senses = tf.matmul(wnDomain_logits, float_mask_matrix_wnDomain) #mask_mat = 2*11*56 or batch*lex_num*NUM_CLASSES\n","        print('wnDomain_masked_senses: ', wnDomain_masked_senses)  # size is: batch*max_len*NUM_CLASSES\n","\n","        print ('softmax_layer_wnDomain is ok \\n')                         \n","\n","    input_rnn_sense = outputBD_wnDomain\n","\n","        \n","## --------------------------- SENSE Block ------------------------------------------------   \n","## ----------------------------------------------------------------------------------------   \n","    with tf.variable_scope('rnn_cell_sense', reuse=tf.AUTO_REUSE):\n","            print ('lstm_sense is ok')\n","            def lstm_cell():\n","                return tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_layer_dim), output_keep_prob=dropout_keep_prob)\n","        \n","            stacked_fw_lstm_Sense = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell() for _ in range(NUM_LAYERS)])\n","            \n","            stacked_bw_lstm_Sense = tf.nn.rnn_cell.MultiRNNCell(\n","                [lstm_cell() for _ in range(NUM_LAYERS)])\n","            \n","    with tf.variable_scope('rnn_sense', reuse=tf.AUTO_REUSE):                        \n","            (forward_output_Sense, backword_output_Sense), _ = tf.nn.bidirectional_dynamic_rnn(\n","                cell_fw = stacked_fw_lstm_Sense,\n","                cell_bw = stacked_bw_lstm_Sense,\n","                inputs = input_rnn_sense,\n","                sequence_length = seq_length,\n","                dtype=tf.float32\n","            )\n","            \n","            outputBD_Sense = tf.concat([forward_output_Sense, backword_output_Sense], axis=2)            \n","            print('outputBD_Sense: ', outputBD_Sense) # shape is batch*max_len*(2*hidden_layer_size) \n","            \n","            print ('outputBD_Sense is ok \\n')   \n","\n","            \n","## --------------------------- attention_layer --------------------------------------------------\n","    with tf.variable_scope(\"attention_layer\", reuse=tf.AUTO_REUSE):\n","        W_attention_L = tf.get_variable(\"W_attention_L\", shape=[2*hidden_layer_dim, 1], initializer = weight_initer )\n","        flat_outputBD_Sense = tf.reshape(outputBD_Sense, [batch_size*max_sent_size, tf.shape(outputBD_Sense)[2]])        \n","        print('flat_outputBD_Sense: ', flat_outputBD_Sense) # shape is  shape is (batch*max_length*(2*hidden_layer_size)).(51(2*hidden_layer_size)2*1) = (batch*max_length)*1 ->2D\n","        \n","        flat_outputBD_Sense_tanh = tf.tanh(flat_outputBD_Sense)\n","        \n","        u_flat_outputBD_Sense_tanh = tf.matmul(flat_outputBD_Sense_tanh, W_attention_L) # shape is batch*max_length -> 2D\n","        print('u_flat: ', u_flat_outputBD_Sense_tanh)\n","        \n","        u = tf.reshape(u_flat_outputBD_Sense_tanh, [batch_size, max_sent_size]) + attention_mask #\n","        print('u: ', u)\n","        \n","        u_softmax = tf.nn.softmax(u, 1)\n","        \n","        a = tf.expand_dims(u_softmax, 2) # shape is expand to batch*max_len*1 -> 3D\n","        print('a: ', a)\n","        \n","        c = tf.reduce_sum(tf.multiply(outputBD_Sense, a), axis=1) # shape is batch*max_len*(2*hidden_layer_size) and then reduce_sum with axis 1 to: batch*(2*hidden_layer_size)\n","        print('c: ', c)\n","        \n","        tiled_c = tf.tile(tf.expand_dims(c, 1), [1, max_sent_size, 1]) # shape is batch*max_len*(2*hidden_layer_size)\n","        print('tiled_c: ', tiled_c)\n","        \n","        attention_output = tf.concat([tiled_c, outputBD_Sense], 2) # batch*max_len*(2*hidden_layer_size) \"concat with\" batch*max_len*(2*hidden_layer_size) -> batch*max_len*(4*hidden_layer_size)\n","        print('attention_output: ', attention_output)\n","        \n","        flat_attention_output = tf.reshape(attention_output, [batch_size*max_sent_size, tf.shape(attention_output)[2]]) # reshape to (batch*max_len)*(4*hidden_layer_size) -> 2D\n","        print('flat_attention_output: ', flat_attention_output)\n","\n","        print ('global_attention is ok \\n') \n","        \n","## --------------------------- hidden_layer Sense ------------------------------------------------------------\n","    with tf.variable_scope(\"hidden_layer\", reuse=tf.AUTO_REUSE):\n","        W = tf.get_variable(\"W\", shape=[4*hidden_layer_dim, 2*hidden_layer_dim], initializer = weight_initer)\n","        b = tf.get_variable(\"b\", shape=[2*hidden_layer_dim], initializer = tf.zeros_initializer())\n","        \n","        drop_flat_attention_output = tf.nn.dropout(flat_attention_output, dropout_keep_prob) # shape is (batch*max_len)*(4*hidden_layer_size) -> 2D\n","        print('drop_flat_attention_output: ', drop_flat_attention_output)\n","        \n","        hidden_layer_output = tf.matmul(drop_flat_attention_output, W) + b  # shape is (batch*max_len)*(2*hidden_layer_size) -> 2D\n","        print('hidden_layer_output: ', hidden_layer_output)\n","                \n","        print ('hidden_layer is ok \\n') \n","        \n","## --------------------------- softmax_layer Sense ------------------------------------------------------------\n","    with tf.variable_scope(\"softmax_layer\", reuse=tf.AUTO_REUSE):\n","        W_sense = tf.get_variable(\"W_sense\", shape=[2*hidden_layer_dim, NUM_CLASSES], initializer = weight_initer)\n","        b_sense = tf.get_variable(\"b_sense\", shape=[NUM_CLASSES], initializer=tf.zeros_initializer())\n","        \n","        drop_hidden_layer_output = tf.nn.dropout(hidden_layer_output, dropout_keep_prob)\n","        print('drop_hidden_layer_output: ', drop_hidden_layer_output)\n","        \n","        flat_sense_logits = tf.matmul(drop_hidden_layer_output, W_sense) + b_sense\n","        print('flat_sense_logits: ', flat_sense_logits)\n","        \n","        sense_logits = tf.reshape(flat_sense_logits, [batch_size, max_sent_size, NUM_CLASSES])\n","        print('sense_logits: ', sense_logits)\n","\n","        masked_sense_logits = tf.multiply(sense_logits, tf.multiply(pos_masked_senses, wnDomain_masked_senses))\n","        print('masked_sense_logits: ', masked_sense_logits)  \n","\n","            \n","        float_mask_RawTosense_batch = tf.cast(mask_RawTosense_batch, dtype=tf.float32)\n","        print('float_mask_RawTosense_batch: ', float_mask_RawTosense_batch)\n","        \n","        final_sense_logits = tf.multiply(masked_sense_logits, float_mask_RawTosense_batch)\n","        print('final_sense_logits: ', final_sense_logits)\n","\n","        predictions = tf.argmax(final_sense_logits , axis=2)\n","        print('predictions: ', predictions)\n","        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels= labels, logits= final_sense_logits))\n","        total_loss = total_loss + loss\n","        print ('softmax_layer is ok \\n')\n","        \n","              \n","## --------------------------- train_op Block ----------------------------------------------\n","## -----------------------------------------------------------------------------------------          \n","    with tf.variable_scope('train_op', reuse=tf.AUTO_REUSE):\n","                        \n","        optimizer = tf.train.AdadeltaOptimizer(LEARNING_RATE)\n","        \n","#         optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n","#         optimizer=tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE, use_locking=False, name='GradientDescent')\n","        print ('AdamOptimizer is ok .... \\n')\n","        \n","        tvars=tf.trainable_variables()\n","        print ('tvars is ok ....')\n","    \n","        l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tvars if v.get_shape().ndims > 1])\n","        print ('l2_loss is ok ....')\n","        \n","        total_loss = total_loss + L2_REGU_LAMBDA*l2_loss             \n","        print ('total_loss is ok ....')\n","        \n","        summaries.append(tf.summary.scalar(\"loss\", loss))\n","        summaries.append(tf.summary.scalar(\"total_loss\", total_loss))\n","        \n","        grads,_ = tf.clip_by_global_norm(tf.gradients(total_loss,tvars),CLIP)\n","        print ('grads is ok ....')\n","        \n","        train_op = optimizer.apply_gradients(zip(grads,tvars))\n","        print ('train_op apply_gradients is ok ....')\n","                               \n","    return input_, labels, y_pos, y_wnDomain, mask_RawTosense_batch, train_op, predictions, dropout_keep_prob, total_loss, seq_length, mask_pos2sense, mask_wnDomain2sense\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_HasXVV9NLf","colab_type":"code","outputId":"d5cff53d-0750-4789-9d8d-f52821676c93","executionInfo":{"status":"ok","timestamp":1568369084481,"user_tz":-120,"elapsed":7939,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# create tensorflow model without sense embedding and just basic LSTM\n","input_, labels, y_pos, y_wnDomain, mask_RawTosense_batch, train_op, predictions, dropout_keep_prob, total_loss, seq_length,\\\n","    mask_pos2sense, mask_wnDomain2sense = create_tensorflow_model(VOCAB_SIZE, WORD_EMBEDDING_DIM, HIDDEN_LAYER_DIM, sense_embed_bool = True, total_loss = 0)\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Creating TENSORFLOW model\n","seq_length:  Tensor(\"Sum:0\", shape=(?,), dtype=int32)\n","max_sent_size:  Tensor(\"Size:0\", shape=(), dtype=int32)\n","batch_size:  Tensor(\"Size_1:0\", shape=(), dtype=int32)\n","mask_RawTosense_batch:  Tensor(\"mask_Raw2Senses:0\", shape=(?, ?, ?), dtype=bool)\n","mask_pos2sense:  Tensor(\"mask_Raw2Senses_1:0\", shape=(?, ?), dtype=bool)\n","mask_matrix_pos:  Tensor(\"Tile:0\", shape=(?, ?, ?), dtype=bool)\n","mask_wnDomain2sense:  Tensor(\"mask_Raw2Senses_2:0\", shape=(?, ?), dtype=bool)\n","mask_matrix_wnDomain:  Tensor(\"Tile_1:0\", shape=(?, ?, ?), dtype=bool)\n","x_mask:  Tensor(\"NotEqual_1:0\", shape=(?, ?), dtype=bool)\n","attention_mask:  Tensor(\"mul_2:0\", shape=(?, ?), dtype=float32)\n","embeddings:  Tensor(\"embeddings/embedding_lookup/Identity:0\", shape=(?, ?, 100), dtype=float32)\n","embeddings:  Tensor(\"embeddings/Reshape:0\", shape=(?, ?, 100), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-8-dc51ab3270a8>:65: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","embeddings is ok \n","\n","lstm_cell_pos is is ok ... \n","WARNING:tensorflow:From <ipython-input-8-dc51ab3270a8>:75: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-8-dc51ab3270a8>:78: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From <ipython-input-8-dc51ab3270a8>:89: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","outputBD_pos:  Tensor(\"rnn_pos/concat:0\", shape=(?, ?, 512), dtype=float32)\n","outputBD_pos is ok \n","\n","flat_softmax_pos:  Tensor(\"softmax_layer_pos/Reshape:0\", shape=(?, ?), dtype=float32)\n","drop_flat_softmax_pos:  Tensor(\"softmax_layer_pos/dropout/mul_1:0\", shape=(?, ?), dtype=float32)\n","flat_pos_logits:  Tensor(\"softmax_layer_pos/add:0\", shape=(?, 12), dtype=float32)\n","pos_logits:  Tensor(\"softmax_layer_pos/Reshape_1:0\", shape=(?, ?, 12), dtype=float32)\n","float_mask_matrix_pos:  Tensor(\"softmax_layer_pos/Cast:0\", shape=(?, ?, ?), dtype=float32)\n","pos_masked_senses:  Tensor(\"softmax_layer_pos/MatMul_1:0\", shape=(?, ?, ?), dtype=float32)\n","softmax_layer_pos is ok \n","\n","lstm_cell_wnDomain is is ok ... \n","outputBD_wnDomain:  Tensor(\"rnn_wnDomain/concat:0\", shape=(?, ?, 512), dtype=float32)\n","outputBD_wnDomain is ok \n","\n","flat_softmax_wnDomain:  Tensor(\"softmax_layer_wnDomain/Reshape:0\", shape=(?, ?), dtype=float32)\n","drop_flat_softmax_wnDomain:  Tensor(\"softmax_layer_wnDomain/dropout/mul_1:0\", shape=(?, ?), dtype=float32)\n","flat_wnDomain_logits:  Tensor(\"softmax_layer_wnDomain/add:0\", shape=(?, 160), dtype=float32)\n","wnDomain_logits:  Tensor(\"softmax_layer_wnDomain/Reshape_1:0\", shape=(?, ?, 160), dtype=float32)\n","float_mask_matrix_wnDomain:  Tensor(\"softmax_layer_wnDomain/Cast:0\", shape=(?, ?, ?), dtype=float32)\n","wnDomain_masked_senses:  Tensor(\"softmax_layer_wnDomain/MatMul_1:0\", shape=(?, ?, ?), dtype=float32)\n","softmax_layer_wnDomain is ok \n","\n","lstm_sense is ok\n","outputBD_Sense:  Tensor(\"rnn_sense/concat:0\", shape=(?, ?, 512), dtype=float32)\n","outputBD_Sense is ok \n","\n","flat_outputBD_Sense:  Tensor(\"attention_layer/Reshape:0\", shape=(?, ?), dtype=float32)\n","u_flat:  Tensor(\"attention_layer/MatMul:0\", shape=(?, 1), dtype=float32)\n","u:  Tensor(\"attention_layer/add:0\", shape=(?, ?), dtype=float32)\n","a:  Tensor(\"attention_layer/ExpandDims:0\", shape=(?, ?, 1), dtype=float32)\n","c:  Tensor(\"attention_layer/Sum:0\", shape=(?, 512), dtype=float32)\n","tiled_c:  Tensor(\"attention_layer/Tile:0\", shape=(?, ?, 512), dtype=float32)\n","attention_output:  Tensor(\"attention_layer/concat:0\", shape=(?, ?, 1024), dtype=float32)\n","flat_attention_output:  Tensor(\"attention_layer/Reshape_2:0\", shape=(?, ?), dtype=float32)\n","global_attention is ok \n","\n","drop_flat_attention_output:  Tensor(\"hidden_layer/dropout/mul_1:0\", shape=(?, ?), dtype=float32)\n","hidden_layer_output:  Tensor(\"hidden_layer/add:0\", shape=(?, 512), dtype=float32)\n","hidden_layer is ok \n","\n","drop_hidden_layer_output:  Tensor(\"softmax_layer/dropout/mul_1:0\", shape=(?, 512), dtype=float32)\n","flat_sense_logits:  Tensor(\"softmax_layer/add:0\", shape=(?, 60919), dtype=float32)\n","sense_logits:  Tensor(\"softmax_layer/Reshape:0\", shape=(?, ?, 60919), dtype=float32)\n","masked_sense_logits:  Tensor(\"softmax_layer/Mul_1:0\", shape=(?, ?, 60919), dtype=float32)\n","float_mask_RawTosense_batch:  Tensor(\"softmax_layer/Cast:0\", shape=(?, ?, ?), dtype=float32)\n","final_sense_logits:  Tensor(\"softmax_layer/Mul_2:0\", shape=(?, ?, 60919), dtype=float32)\n","predictions:  Tensor(\"softmax_layer/ArgMax:0\", shape=(?, ?), dtype=int64)\n","softmax_layer is ok \n","\n","AdamOptimizer is ok .... \n","\n","tvars is ok ....\n","l2_loss is ok ....\n","total_loss is ok ....\n","grads is ok ....\n","train_op apply_gradients is ok ....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sjH5I--oEvI0","colab_type":"code","outputId":"dcc83da4-d956-4cdc-a69c-1ef5cebcdd95","executionInfo":{"status":"ok","timestamp":1568369087007,"user_tz":-120,"elapsed":1621,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["# -----------------------------------------------------------------------------\n","def BN_to_WNDOMAIN_dic(mapping_file):\n","    print('WN_to_BN_dic is started ....')\n","    BN_ID = []\n","    WN_DOMAIN = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            WN_DOMAIN.append(line_synsets[1])    \n","    BN2WnDomain_dic = dict(zip(BN_ID, WN_DOMAIN))\n","    print('BN_to_WNDOMAIN_dic is done ....')\n","    return BN2WnDomain_dic\n","\n","\n","# -----------------------------------------------------------------------------\n","def BN_to_LexNames_dic(mapping_file):\n","    print('BN_to_LexNames is started ....')\n","    BN_ID = []\n","    LEXNAMES = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            LEXNAMES.append(line_synsets[1])    \n","    BN2LexNames_dic = dict(zip(BN_ID, LEXNAMES))\n","    print('BN_to_LexNames_dic is done ....')\n","\n","    return BN2LexNames_dic\n","\n","# -----------------------------------------------------------------------------\n","def WN_to_BN_dic(mapping_file):\n","    print('WN_to_BN_dic is started ....')\n","    BN_ID = []\n","    WN_ID = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            WN_ID.append(line_synsets[1])    \n","    WN2BN_map_dic = dict(zip(WN_ID,BN_ID))\n","    print('WN_to_BN_dic is done ....')\n","\n","    return WN2BN_map_dic\n","\n","\n","# -----------------------------------------------------------------------------\n","def WN_to_BN_dic(mapping_file):\n","    print('WN_to_BN_dic is started ....')\n","    BN_ID = []\n","    WN_ID = []\n","    with codecs.open(mapping_file,'rb') as f:        \n","        for line in f:            \n","            line_synsets = line.decode().strip().split('\\t')\n","            BN_ID.append(line_synsets[0])\n","            WN_ID.append(line_synsets[1])    \n","    WN2BN_map_dic = dict(zip(WN_ID,BN_ID))\n","    print('WN_to_BN_dic is done ....')\n","\n","    return WN2BN_map_dic\n","\n","\n","WN2BN_map_dic = WN_to_BN_dic(MAP_WN2BN_FILE)\n","print(len(WN2BN_map_dic))\n","\n","BN2WnDomain_dic = BN_to_WNDOMAIN_dic(MAP_BN2WNDOMAIN_FILE)\n","print(len(BN2WnDomain_dic))\n","\n","BN2LexNames_dic = BN_to_LexNames_dic(MAP_BN2LEXNAMES_FILE)\n","print(len(BN2LexNames_dic))\n","\n","## ----------------------------------------------------------------------------\n","def getMFS_(word, WN2BN_map_dic):\n","    MFS_wnId = UNK\n","    all_synsets = wn.synsets(word)\n","\n","    # check if the word has synsets or not!\n","    if len(all_synsets) == 0 or all_synsets is None:\n","        return word\n","    \n","    synset = all_synsets[0]\n","    MFS_wnId = \"wn:\" + str(synset.offset()).zfill( 8) + synset.pos()\n","    if WN2BN_map_dic.get(MFS_wnId) is not None:\n","        MFS_bnId = WN2BN_map_dic.get(MFS_wnId)\n","        \n","    return MFS_bnId\n","\n","def Tagger_acc(Id_Instance, pred_sent, true_sent):    \n","    T = 0\n","    F = 0\n","    f1Score = 0\n","    for i in range(len(pred_sent)):\n","        for j in range(len(pred_sent[i])):\n","            if Id_Instance[i][j] != 'None':\n","                if pred_sent[i][j] == true_sent[i][j]:\n","                    T = T + 1\n","                else:\n","                    F = F + 1\n","                                \n","    f1Score = T/(T+F)\n","    \n","    return f1Score\n","\n","   \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WN_to_BN_dic is started ....\n","WN_to_BN_dic is done ....\n","117659\n","WN_to_BN_dic is started ....\n","BN_to_WNDOMAIN_dic is done ....\n","92601\n","BN_to_LexNames is started ....\n","BN_to_LexNames_dic is done ....\n","117653\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-8O16Z0uYOjR","colab_type":"code","outputId":"8f2f82dd-83f9-4e02-b185-05c80204c17d","executionInfo":{"status":"ok","timestamp":1568369296032,"user_tz":-120,"elapsed":208761,"user":{"displayName":"manoochehr joodi bigdello","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClFT0ApRPzN-A5mrBkVzOsZ-tFqq5y2LMjI6Cz=s64","userId":"07183045139148849434"}},"colab":{"base_uri":"https://localhost:8080/","height":606}},"source":["# model loading, prediction and write part #Restore Saved Tensorflow model.\n","import tensorflow as tf\n","saver = tf.train.Saver()\n","\n","with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:    \n","    \n","    saver.restore(sess, root_path+'/POS_WnDomain_SenseEmbed_models_49.43566591422122/model.ckpt') # POS_WnDomain_SenseEmbed_models_49.43566591422122\n","    \n","    print(\"Model restored.\")         \n","    \n","    file_names = ['senseval2', 'senseval3', 'semeval2007', 'semeval2013', 'semeval2015', 'ALL']\n","\n","    for file_name in file_names:\n","        test_path = root_path + '/semcor/{}/'.format(file_name)\n","        x_test_IdInstance_Sent = pickle.load(open(test_path + 'x_IdInstance_Sent.pkl', \"rb\"))\n","        x_test_Raw_Sent = pickle.load(open(test_path + 'x_Raw_Sent.pkl', \"rb\"))\n","        x_test_Raw_Sent_id = pickle.load(open(test_path + 'x_Raw_Sent_id.pkl', \"rb\"))\n","        y_test_BNsense_Sent = pickle.load(open(test_path + 'y_BNsense_Sent.pkl', \"rb\"))\n","        y_test_POS_Sent_id = pickle.load(open(test_path + 'y_POS_Sent_id.pkl', \"rb\"))\n","        y_test_allSenses_Sent = pickle.load(open(test_path + 'y_allSenses_Sent.pkl', \"rb\"))\n","        y_test_allSenses_Sent_id = pickle.load(open(test_path + 'y_allSenses_Sent_id.pkl', \"rb\"))    \n","        y_test_Lex_Sent = pickle.load(open(test_path + 'y_Lex_Sent.pkl', \"rb\"))\n","        y_test_WnDomain_Sent = pickle.load(open(test_path + 'y_WnDomain_Sent.pkl', \"rb\"))\n","        mask_test_wnDomain2sense = pickle.load(open(test_path + 'mask_wnDomain2sense.pkl', \"rb\"))\n","        mask_test_pos2sense = pickle.load(open(test_path + 'mask_pos2sense.pkl', \"rb\"))\n","        \n","        \n","        test_pred_BN = []\n","        test_pred_Lex = []\n","        test_pred_WnDomain = []\n","\n","        for i in range(0, len(x_test_Raw_Sent_id), BATCH_SIZE):                            \n","            batch_x_Lemma = x_test_Raw_Sent[slice(i, i + BATCH_SIZE)]\n","            batch_x_Raw = x_test_Raw_Sent_id[slice(i, i + BATCH_SIZE)]\n","            \n","            \n","            y_test_allSenses_Sent_batch = y_test_allSenses_Sent[slice(i, i + BATCH_SIZE)]\n","            y_test_allSenses_Sent_id_batch = y_test_allSenses_Sent_id[slice(i, i + BATCH_SIZE)]\n","\n","            mask_lemma2Sense_b = mask_lemmaToSenses_batch(batch_x_Raw, y_test_allSenses_Sent_batch, y_test_allSenses_Sent_id_batch, SenseWords2id)\n","\n","            batch_x_Raw = padding3(batch_x_Raw, PAD_ID)\n","            \n","\n","            lengths, predict = sess.run(\n","                [seq_length, predictions], feed_dict = {input_ : batch_x_Raw, mask_RawTosense_batch : mask_lemma2Sense_b, mask_pos2sense: mask_test_pos2sense,\\\n","                                                                    mask_wnDomain2sense: mask_test_wnDomain2sense , dropout_keep_prob: 1.0 })            \n","\n","            pred_sense = []\n","            for sent_num in range(len(predict)):\n","                pr_BN, pr_Lex, pr_WnDomain = [], [], []\n","                for L_ in range(lengths[sent_num]):\n","                    # if our prediction is UNK means that word is not in our dictionary and we can't predict anything for that, so we put BackOff plan and MFS as our prediction.\n","                    if id2Sensewords[predict[sent_num][L_]] != UNK:\n","                        pr_BN.append(id2Sensewords[predict[sent_num][L_]])\n","                        pr_Lex.append(BN2LexNames_dic.get(id2Sensewords[predict[sent_num][L_]])) # BN2LexNames_dic\n","                        pr_WnDomain.append(BN2WnDomain_dic.get(id2Sensewords[predict[sent_num][L_]])) # BN2WnDomain_dic\n","                    else:\n","                        pr_BN.append(getMFS_(batch_x_Lemma[sent_num][L_], WN2BN_map_dic))\n","                        pr_Lex.append(BN2LexNames_dic.get(getMFS_(batch_x_Lemma[sent_num][L_], WN2BN_map_dic)))\n","                        pr_WnDomain.append(BN2WnDomain_dic.get(getMFS_(batch_x_Lemma[sent_num][L_], WN2BN_map_dic)))\n","\n","                test_pred_BN.append(pr_BN)\n","                test_pred_Lex.append(pr_Lex)\n","                test_pred_WnDomain.append(pr_WnDomain)\n","\n","\n","        f1Score_Bn = Tagger_acc(x_test_IdInstance_Sent, test_pred_BN, y_test_BNsense_Sent)\n","        print('f1Score_Bn = ',f1Score_Bn) \n","\n","        f1Score_Lex = Tagger_acc(x_test_IdInstance_Sent, test_pred_Lex, y_test_Lex_Sent)\n","        print('f1Score_Lex = ',f1Score_Lex) \n","\n","        f1Score_WnDomain = Tagger_acc(x_test_IdInstance_Sent, test_pred_WnDomain, y_test_WnDomain_Sent)\n","        print('f1Score_WnDomain = ',f1Score_WnDomain) \n","\n","        print('Tagger Results is ok ......... ', file_name)\n","        print('-------------------------------------------------------')         \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/NLPHW3/POS_WnDomain_SenseEmbed_models_49.43566591422122/model.ckpt\n","Model restored.\n","f1Score_Bn =  0.6358457493426819\n","f1Score_Lex =  0.8172655565293602\n","f1Score_WnDomain =  0.5863277826468011\n","Tagger Results is ok .........  senseval2\n","-------------------------------------------------------\n","f1Score_Bn =  0.6043243243243244\n","f1Score_Lex =  0.7632432432432432\n","f1Score_WnDomain =  0.6556756756756756\n","Tagger Results is ok .........  senseval3\n","-------------------------------------------------------\n","f1Score_Bn =  0.4967032967032967\n","f1Score_Lex =  0.6725274725274726\n","f1Score_WnDomain =  0.8373626373626374\n","Tagger Results is ok .........  semeval2007\n","-------------------------------------------------------\n","f1Score_Bn =  0.5869829683698297\n","f1Score_Lex =  0.7055961070559611\n","f1Score_WnDomain =  0.7110705596107056\n","Tagger Results is ok .........  semeval2013\n","-------------------------------------------------------\n","f1Score_Bn =  0.5870841487279843\n","f1Score_Lex =  0.7788649706457925\n","f1Score_WnDomain =  0.5665362035225049\n","Tagger Results is ok .........  semeval2015\n","-------------------------------------------------------\n","f1Score_Bn =  0.6011305666620709\n","f1Score_Lex =  0.7636839928305529\n","f1Score_WnDomain =  0.6452502412794706\n","Tagger Results is ok .........  ALL\n","-------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KvFMmMd6mD5N","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}